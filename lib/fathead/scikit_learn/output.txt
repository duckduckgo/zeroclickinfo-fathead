Biclustering documents with the Spectral Co-clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import print_function\n\nprint(__doc__)\n\nfrom collections import defaultdict\nimport operator\nimport re\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.cluster.bicluster import SpectralCoclustering\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.externals.six import iteritems\nfrom sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.cluster import v_measure_score\n\n\ndef number_aware_tokenizer(doc):\n    """ Tokenizer that maps all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    """\n    token_pattern = re.compile(u'(?u)\\b\\w\\w+\\b')\n    tokens = token_pattern.findall(doc)\n    tokens = ["#NUMBER" if token[0] in "0123456789_" else token\n              for token in tokens]\n    return tokens\n\n# exclude 'comp.os.ms-windows.misc'\ncategories = ['alt.atheism', 'comp.graphics',\n              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n              'comp.windows.x', 'misc.forsale', 'rec.autos',\n              'rec.motorcycles', 'rec.sport.baseball',\n              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n              'sci.med', 'sci.space', 'soc.religion.christian',\n              'talk.politics.guns', 'talk.politics.mideast',\n              'talk.politics.misc', 'talk.religion.misc']\nnewsgroups = fetch_20newsgroups(categories=categories)\ny_true = newsgroups.target\n\nvectorizer = TfidfVectorizer(stop_words='english', min_df=5,\n                             tokenizer=number_aware_tokenizer)\ncocluster = SpectralCoclustering(n_clusters=len(categories),\n                                 svd_method='arpack', random_state=0)\nkmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n                         random_state=0)\n\nprint("Vectorizing...")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint("Coclustering...")\nstart_time = time()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint("Done in {:.2f}s. V-measure: {:.4f}".format(\n    time() - start_time,\n    v_measure_score(y_cocluster, y_true)))\n\nprint("MiniBatchKMeans...")\nstart_time = time()\ny_kmeans = kmeans.fit_predict(X)\nprint("Done in {:.2f}s. V-measure: {:.4f}".format(\n    time() - start_time,\n    v_measure_score(y_kmeans, y_true)))\n\nfeature_names = vectorizer.get_feature_names()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not (np.any(rows) and np.any(cols)):\n        import sys\n        return sys.float_info.max\n    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis], cols].sum() but\n    # much faster in scipy <= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = (X[row_complement][:, cols].sum() +\n           X[rows][:, col_complement].sum())\n    return cut / weight\n\n\ndef most_common(d):\n    """Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python >=2.7.\n    """\n    return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i)\n                       for i in range(len(newsgroups.target_names)))\nbest_idx = np.argsort(bicluster_ncuts)[:5]\n\nprint()\nprint("Best biclusters:")\nprint("----------------")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = defaultdict(int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = ", ".join("{:.0f}% {}".format(float(c) / n_rows * 100, name)\n                           for name, c in most_common(counter)[:3])\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n                           word_col[out_of_cluster_docs, :].sum(axis=0))\n    word_scores = word_scores.ravel()\n    important_words = list(feature_names[cluster_words[i]]\n                           for i in word_scores.argsort()[:-11:-1])\n\n    print("bicluster {} : {} documents, {} words".format(\n        idx, n_rows, n_cols))\n    print("categories   : {}".format(cat_string))\n    print("words        : {}\n".format(', '.join(important_words)))</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/bicluster_newsgroups.html
Digits Classification Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nfrom sklearn import datasets, neighbors, linear_model\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\nn_samples = len(X_digits)\n\nX_train = X_digits[:.9 * n_samples]\ny_train = y_digits[:.9 * n_samples]\nX_test = X_digits[.9 * n_samples:]\ny_test = y_digits[.9 * n_samples:]\n\nknn = neighbors.KNeighborsClassifier()\nlogistic = linear_model.LogisticRegression()\n\nprint('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\nprint('LogisticRegression score: %f'\n      % logistic.fit(X_train, y_train).score(X_test, y_test))</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html
Classification of text documents using sparse features	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nfrom optparse import OptionParser\nimport sys\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.extmath import density\nfrom sklearn import metrics\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option("--report",\n              action="store_true", dest="print_report",\n              help="Print a detailed classification report.")\nop.add_option("--chi2_select",\n              action="store", type="int", dest="select_chi2",\n              help="Select some number of features using a chi-squared test")\nop.add_option("--confusion_matrix",\n              action="store_true", dest="print_cm",\n              help="Print the confusion matrix.")\nop.add_option("--top10",\n              action="store_true", dest="print_top10",\n              help="Print ten most discriminative terms per class"\n                   " for every classifier.")\nop.add_option("--all_categories",\n              action="store_true", dest="all_categories",\n              help="Whether to use all categories or not.")\nop.add_option("--use_hashing",\n              action="store_true",\n              help="Use a hashing vectorizer.")\nop.add_option("--n_features",\n              action="store", type=int, default=2 ** 16,\n              help="n_features when using the hashing vectorizer.")\nop.add_option("--filtered",\n              action="store_true",\n              help="Remove newsgroup information that is easily overfit: "\n                   "headers, signatures, and quoting.")\n\n(opts, args) = op.parse_args()\nif len(args) > 0:\n    op.error("this script takes no arguments.")\n    sys.exit(1)\n\nprint(__doc__)\nop.print_help()\nprint()\n\n\n###############################################################################\n# Load some categories from the training set\nif opts.all_categories:\n    categories = None\nelse:\n    categories = [\n        'alt.atheism',\n        'talk.religion.misc',\n        'comp.graphics',\n        'sci.space',\n    ]\n\nif opts.filtered:\n    remove = ('headers', 'footers', 'quotes')\nelse:\n    remove = ()\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories if categories else "all")\n\ndata_train = fetch_20newsgroups(subset='train', categories=categories,\n                                shuffle=True, random_state=42,\n                                remove=remove)\n\ndata_test = fetch_20newsgroups(subset='test', categories=categories,\n                               shuffle=True, random_state=42,\n                               remove=remove)\nprint('data loaded')\n\ncategories = data_train.target_names    # for case categories == None\n\n\ndef size_mb(docs):\n    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n\ndata_train_size_mb = size_mb(data_train.data)\ndata_test_size_mb = size_mb(data_test.data)\n\nprint("%d documents - %0.3fMB (training set)" % (\n    len(data_train.data), data_train_size_mb))\nprint("%d documents - %0.3fMB (test set)" % (\n    len(data_test.data), data_test_size_mb))\nprint("%d categories" % len(categories))\nprint()\n\n# split a training set and a test set\ny_train, y_test = data_train.target, data_test.target\n\nprint("Extracting features from the training data using a sparse vectorizer")\nt0 = time()\nif opts.use_hashing:\n    vectorizer = HashingVectorizer(stop_words='english', non_negative=True,\n                                   n_features=opts.n_features)\n    X_train = vectorizer.transform(data_train.data)\nelse:\n    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words='english')\n    X_train = vectorizer.fit_transform(data_train.data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))\nprint("n_samples: %d, n_features: %d" % X_train.shape)\nprint()\n\nprint("Extracting features from the test data using the same vectorizer")\nt0 = time()\nX_test = vectorizer.transform(data_test.data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))\nprint("n_samples: %d, n_features: %d" % X_test.shape)\nprint()\n\n# mapping from integer feature name to original token string\nif opts.use_hashing:\n    feature_names = None\nelse:\n    feature_names = vectorizer.get_feature_names()\n\nif opts.select_chi2:\n    print("Extracting %d best features by a chi-squared test" %\n          opts.select_chi2)\n    t0 = time()\n    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n    X_train = ch2.fit_transform(X_train, y_train)\n    X_test = ch2.transform(X_test)\n    if feature_names:\n        # keep selected feature names\n        feature_names = [feature_names[i] for i\n                         in ch2.get_support(indices=True)]\n    print("done in %fs" % (time() - t0))\n    print()\n\nif feature_names:\n    feature_names = np.asarray(feature_names)\n\n\ndef trim(s):\n    """Trim string to fit on terminal (assuming 80-column display)"""\n    return s if len(s) <= 80 else s[:77] + "..."\n\n\n###############################################################################\n# Benchmark classifiers\ndef benchmark(clf):\n    print('_' * 80)\n    print("Training: ")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, y_train)\n    train_time = time() - t0\n    print("train time: %0.3fs" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print("test time:  %0.3fs" % test_time)\n\n    score = metrics.accuracy_score(y_test, pred)\n    print("accuracy:   %0.3f" % score)\n\n    if hasattr(clf, 'coef_'):\n        print("dimensionality: %d" % clf.coef_.shape[1])\n        print("density: %f" % density(clf.coef_))\n\n        if opts.print_top10 and feature_names is not None:\n            print("top 10 keywords per class:")\n            for i, category in enumerate(categories):\n                top10 = np.argsort(clf.coef_[i])[-10:]\n                print(trim("%s: %s"\n                      % (category, " ".join(feature_names[top10]))))\n        print()\n\n    if opts.print_report:\n        print("classification report:")\n        print(metrics.classification_report(y_test, pred,\n                                            target_names=categories))\n\n    if opts.print_cm:\n        print("confusion matrix:")\n        print(metrics.confusion_matrix(y_test, pred))\n\n    print()\n    clf_descr = str(clf).split('(')[0]\n    return clf_descr, score, train_time, test_time\n\n\nresults = []\nfor clf, name in (\n        (RidgeClassifier(tol=1e-2, solver="lsqr"), "Ridge Classifier"),\n        (Perceptron(n_iter=50), "Perceptron"),\n        (PassiveAggressiveClassifier(n_iter=50), "Passive-Aggressive"),\n        (KNeighborsClassifier(n_neighbors=10), "kNN"),\n        (RandomForestClassifier(n_estimators=100), "Random forest")):\n    print('=' * 80)\n    print(name)\n    results.append(benchmark(clf))\n\nfor penalty in ["l2", "l1"]:\n    print('=' * 80)\n    print("%s penalty" % penalty.upper())\n    # Train Liblinear model\n    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n                                            dual=False, tol=1e-3)))\n\n    # Train SGD model\n    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n                                           penalty=penalty)))\n\n# Train SGD with Elastic Net penalty\nprint('=' * 80)\nprint("Elastic-Net penalty")\nresults.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n                                       penalty="elasticnet")))\n\n# Train NearestCentroid without threshold\nprint('=' * 80)\nprint("NearestCentroid (aka Rocchio classifier)")\nresults.append(benchmark(NearestCentroid()))\n\n# Train sparse Naive Bayes classifiers\nprint('=' * 80)\nprint("Naive Bayes")\nresults.append(benchmark(MultinomialNB(alpha=.01)))\nresults.append(benchmark(BernoulliNB(alpha=.01)))\n\nprint('=' * 80)\nprint("LinearSVC with L1-based feature selection")\n# The smaller C, the stronger the regularization.\n# The more regularization, the more sparsity.\nresults.append(benchmark(Pipeline([\n  ('feature_selection', LinearSVC(penalty="l1", dual=False, tol=1e-3)),\n  ('classification', LinearSVC())\n])))\n\n# make some plots\n\nindices = np.arange(len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = np.array(training_time) / np.max(training_time)\ntest_time = np.array(test_time) / np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title("Score")\nplt.barh(indices, score, .2, label="score", color='r')\nplt.barh(indices + .3, training_time, .2, label="training time", color='g')\nplt.barh(indices + .6, test_time, .2, label="test time", color='b')\nplt.yticks(())\nplt.legend(loc='best')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html
Clustering text documents using k-means	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option("--lsa",\n              dest="n_components", type="int",\n              help="Preprocess documents with latent semantic analysis.")\nop.add_option("--no-minibatch",\n              action="store_false", dest="minibatch", default=True,\n              help="Use ordinary k-means algorithm (in batch mode).")\nop.add_option("--no-idf",\n              action="store_false", dest="use_idf", default=True,\n              help="Disable Inverse Document Frequency feature weighting.")\nop.add_option("--use-hashing",\n              action="store_true", default=False,\n              help="Use a hashing feature vectorizer")\nop.add_option("--n-features", type=int, default=10000,\n              help="Maximum number of features (dimensions)"\n                   " to extract from text.")\nop.add_option("--verbose",\n              action="store_true", dest="verbose", default=False,\n              help="Print progress reports inside k-means algorithm.")\n\nprint(__doc__)\nop.print_help()\n\n(opts, args) = op.parse_args()\nif len(args) > 0:\n    op.error("this script takes no arguments.")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n#categories = None\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint("%d documents" % len(dataset.data))\nprint("%d categories" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint("Extracting features from the training dataset using a sparse vectorizer")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X.shape)\nprint()\n\nif opts.n_components:\n    print("Performing dimensionality reduction using LSA")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print("done in %fs" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print("Explained variance of the SVD step: {}%".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint("Clustering sparse data with %s" % km)\nt0 = time()\nkm.fit(X)\nprint("done in %0.3fs" % (time() - t0))\nprint()\n\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels, km.labels_))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_))\nprint("Adjusted Rand-Index: %.3f"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print("Top terms per cluster:")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print("Cluster %d:" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/document_clustering.html
Faces recognition example using eigenfaces and SVMs	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import print_function\n\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n\n\n###############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint("Total dataset size:")\nprint("n_samples: %d" % n_samples)\nprint("n_features: %d" % n_features)\nprint("n_classes: %d" % n_classes)\n\n\n###############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n\n###############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint("Extracting the top %d eigenfaces from %d faces"\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint("done in %0.3fs" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint("Projecting the input data on the eigenfaces orthonormal basis")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint("done in %0.3fs" % (time() - t0))\n\n\n###############################################################################\n# Train a SVM classification model\n\nprint("Fitting the classifier to the training set")\nt0 = time()\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint("done in %0.3fs" % (time() - t0))\nprint("Best estimator found by grid search:")\nprint(clf.best_estimator_)\n\n\n###############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint("Predicting people's names on the test set")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint("done in %0.3fs" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n###############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    """Helper function to plot a gallery of portraits"""\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html
Pipeline Anova SVM	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nfrom sklearn import svm\nfrom sklearn.datasets import samples_generator\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import make_pipeline\n\n# import some data to play with\nX, y = samples_generator.make_classification(\n    n_features=20, n_informative=3, n_redundant=0, n_classes=4,\n    n_clusters_per_class=2)\n\n# ANOVA SVM-C\n# 1) anova filter, take 3 best ranked features\nanova_filter = SelectKBest(f_regression, k=3)\n# 2) svm\nclf = svm.SVC(kernel='linear')\n\nanova_svm = make_pipeline(anova_filter, clf)\nanova_svm.fit(X, y)\nanova_svm.predict(X)</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html
Concatenating multiple feature extraction methods	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 clause\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way to high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([("pca", pca), ("univ_select", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\n\nsvm = SVC(kernel="linear")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([("features", combined_features), ("svm", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_stacker.html
Gaussian Processes regression: goodness-of-fit on the ‘diabetes’ dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n# Licence: BSD 3 clause\n\nfrom sklearn import datasets\nfrom sklearn.gaussian_process import GaussianProcess\nfrom sklearn.cross_validation import cross_val_score, KFold\n\n# Load the dataset from scikit's data sets\ndiabetes = datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Instanciate a GP model\ngp = GaussianProcess(regr='constant', corr='absolute_exponential',\n                     theta0=[1e-4] * 10, thetaL=[1e-12] * 10,\n                     thetaU=[1e-2] * 10, nugget=1e-2, optimizer='Welch')\n\n# Fit the GP model to the data performing maximum likelihood estimation\ngp.fit(X, y)\n\n# Deactivate maximum likelihood estimation for the cross-validation loop\ngp.theta0 = gp.theta_  # Given correlation parameter = MLE\ngp.thetaL, gp.thetaU = None, None  # None bounds deactivate MLE\n\n# Perform a cross-validation estimate of the coefficient of determination using\n# the cross_validation module using all CPUs available on the machine\nK = 20  # folds\nR2 = cross_val_score(gp, X, y=y, cv=KFold(y.size, K), n_jobs=1).mean()\nprint("The %d-Folds estimate of the coefficient of determination is R2 = %s"\n      % (K, R2))</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/gp_diabetes_dataset.html
Parameter estimation using grid search with cross-validation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import print_function\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\nprint(__doc__)\n\n# Loading the Digits dataset\ndigits = datasets.load_digits()\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target\n\n# Split the dataset in two equal parts\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print("# Tuning hyper-parameters for %s" % score)\n    print()\n\n    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n                       scoring='%s_weighted' % score)\n    clf.fit(X_train, y_train)\n\n    print("Best parameters set found on development set:")\n    print()\n    print(clf.best_params_)\n    print()\n    print("Grid scores on development set:")\n    print()\n    for params, mean_score, scores in clf.grid_scores_:\n        print("%0.3f (+/-%0.03f) for %r"\n              % (mean_score, scores.std() * 2, params))\n    print()\n\n    print("Detailed classification report:")\n    print()\n    print("The model is trained on the full development set.")\n    print("The scores are computed on the full evaluation set.")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n\n# Note the problem is too easy: the hyperparameter plateau is too flat and the\n# output model is the same for precision and recall with ties in quality.</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html
Sample pipeline for text feature extraction and evaluation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Mathieu Blondel <mathieu@mblondel.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom pprint import pprint\nfrom time import time\nimport logging\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n]\n# Uncomment the following to do the analysis on all the categories\n#categories = None\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories)\n\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint("%d documents" % len(data.filenames))\nprint("%d categories" % len(data.target_names))\nprint()\n\n###############################################################################\n# define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\n\n# uncommenting more parameters will give better exploring power but will\n# increase processing time in a combinatorial way\nparameters = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    #'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    #'tfidf__use_idf': (True, False),\n    #'tfidf__norm': ('l1', 'l2'),\n    'clf__alpha': (0.00001, 0.000001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    #'clf__n_iter': (10, 50, 80),\n}\n\nif __name__ == "__main__":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print("Performing grid search...")\n    print("pipeline:", [name for name, _ in pipeline.steps])\n    print("parameters:")\n    pprint(parameters)\n    t0 = time()\n    grid_search.fit(data.data, data.target)\n    print("done in %0.3fs" % (time() - t0))\n    print()\n\n    print("Best score: %0.3f" % grid_search.best_score_)\n    print("Best parameters set:")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print("\t%s: %r" % (param_name, best_parameters[param_name]))</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html
FeatureHasher and DictVectorizer Comparison	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom collections import defaultdict\nimport re\nimport sys\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\n\n\ndef n_nonzero_columns(X):\n    """Returns the number of non-zero columns in a CSR matrix X."""\n    return len(np.unique(X.nonzero()[1]))\n\n\ndef tokens(doc):\n    """Extract tokens from doc.\n\n    This uses a simple regex to break strings into tokens. For a more\n    principled approach, see CountVectorizer or TfidfVectorizer.\n    """\n    return (tok.lower() for tok in re.findall(r"\w+", doc))\n\n\ndef token_freqs(doc):\n    """Extract a dict mapping tokens from doc to their frequencies."""\n    freq = defaultdict(int)\n    for tok in tokens(doc):\n        freq[tok] += 1\n    return freq\n\n\ncategories = [\n    'alt.atheism',\n    'comp.graphics',\n    'comp.sys.ibm.pc.hardware',\n    'misc.forsale',\n    'rec.autos',\n    'sci.space',\n    'talk.religion.misc',\n]\n# Uncomment the following line to use a larger set (11k+ documents)\n#categories = None\n\nprint(__doc__)\nprint("Usage: %s [n_features_for_hashing]" % sys.argv[0])\nprint("    The default number of features is 2**18.")\nprint()\n\ntry:\n    n_features = int(sys.argv[1])\nexcept IndexError:\n    n_features = 2 ** 18\nexcept ValueError:\n    print("not a valid number of features: %r" % sys.argv[1])\n    sys.exit(1)\n\n\nprint("Loading 20 newsgroups training data")\nraw_data = fetch_20newsgroups(subset='train', categories=categories).data\ndata_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6\nprint("%d documents - %0.3fMB" % (len(raw_data), data_size_mb))\nprint()\n\nprint("DictVectorizer")\nt0 = time()\nvectorizer = DictVectorizer()\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % len(vectorizer.get_feature_names()))\nprint()\n\nprint("FeatureHasher on frequency dicts")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % n_nonzero_columns(X))\nprint()\n\nprint("FeatureHasher on raw tokens")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features, input_type="string")\nX = hasher.transform(tokens(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % n_nonzero_columns(X))</code></pre>	http://scikit-learn.org/stable/auto_examples/text/hashing_vs_dict_vectorizer.html
Feature Union with Heterogeneous Data Sources	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Matt Terry <matt.terry@gmail.com>\n#\n# License: BSD 3 clause\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to sklearn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {'a': [1, 5, 2, 5, 2, 8],\n               'b': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key='a')\n    >> data['a'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]\n\n\nclass TextStats(BaseEstimator, TransformerMixin):\n    """Extract features from each document for DictVectorizer"""\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        return [{'length': len(text),\n                 'num_sentences': text.count('.')}\n                for text in posts]\n\n\nclass SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n    """Extract the subject & body from a usenet post in a single pass.\n\n    Takes a sequence of strings and produces a dict of sequences.  Keys are\n    `subject` and `body`.\n    """\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        features = np.recarray(shape=(len(posts),),\n                               dtype=[('subject', object), ('body', object)])\n        for i, text in enumerate(posts):\n            headers, _, bod = text.partition('\n\n')\n            bod = strip_newsgroup_footer(bod)\n            bod = strip_newsgroup_quoting(bod)\n            features['body'][i] = bod\n\n            prefix = 'Subject:'\n            sub = ''\n            for line in headers.split('\n'):\n                if line.startswith(prefix):\n                    sub = line[len(prefix):]\n                    break\n            features['subject'][i] = sub\n\n        return features\n\n\npipeline = Pipeline([\n    # Extract the subject & body\n    ('subjectbody', SubjectBodyExtractor()),\n\n    # Use FeatureUnion to combine the features from subject and body\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from the post's subject line\n            ('subject', Pipeline([\n                ('selector', ItemSelector(key='subject')),\n                ('tfidf', TfidfVectorizer(min_df=50)),\n            ])),\n\n            # Pipeline for standard bag-of-words model for body\n            ('body_bow', Pipeline([\n                ('selector', ItemSelector(key='body')),\n                ('tfidf', TfidfVectorizer()),\n                ('best', TruncatedSVD(n_components=50)),\n            ])),\n\n            # Pipeline for pulling ad hoc features from post's body\n            ('body_stats', Pipeline([\n                ('selector', ItemSelector(key='body')),\n                ('stats', TextStats()),  # returns a list of dicts\n                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n            ])),\n\n        ],\n\n        # weight components in FeatureUnion\n        transformer_weights={\n            'subject': 0.8,\n            'body_bow': 0.5,\n            'body_stats': 1.0,\n        },\n    )),\n\n    # Use a SVC classifier on the combined features\n    ('svc', SVC(kernel='linear')),\n])\n\n# limit the list of categories to make running this exmaple faster.\ncategories = ['alt.atheism', 'talk.religion.misc']\ntrain = fetch_20newsgroups(random_state=1,\n                           subset='train',\n                           categories=categories,\n                           )\ntest = fetch_20newsgroups(random_state=1,\n                          subset='test',\n                          categories=categories,\n                          )\n\npipeline.fit(train.data, train.target)\ny = pipeline.predict(test.data)\nprint(classification_report(y, test.target))</code></pre>	http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html
Examples	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code></code></pre>	http://scikit-learn.org/stable/auto_examples/index.html
Lasso on dense and sparse data	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nfrom time import time\nfrom scipy import sparse\nfrom scipy import linalg\n\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.linear_model import Lasso\n\n\n###############################################################################\n# The two Lasso implementations on Dense data\nprint("--- Dense matrices")\n\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\nX_sp = sparse.coo_matrix(X)\n\nalpha = 1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = time()\nsparse_lasso.fit(X_sp, y)\nprint("Sparse Lasso done in %fs" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(X, y)\nprint("Dense Lasso done in %fs" % (time() - t0))\n\nprint("Distance between coefficients : %s"\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))\n\n###############################################################################\n# The two Lasso implementations on Sparse data\nprint("--- Sparse matrices")\n\nXs = X.copy()\nXs[Xs < 2.5] = 0.0\nXs = sparse.coo_matrix(Xs)\nXs = Xs.tocsc()\n\nprint("Matrix density : %s %%" % (Xs.nnz / float(X.size) * 100))\n\nalpha = 0.1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = time()\nsparse_lasso.fit(Xs, y)\nprint("Sparse Lasso done in %fs" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(Xs.toarray(), y)\nprint("Dense Lasso done in %fs" % (time() - t0))\n\nprint("Distance between coefficients : %s"\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/lasso_dense_vs_sparse_data.html
Imputing missing values before building an estimator	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>import numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.cross_validation import cross_val_score\n\nrng = np.random.RandomState(0)\n\ndataset = load_boston()\nX_full, y_full = dataset.data, dataset.target\nn_samples = X_full.shape[0]\nn_features = X_full.shape[1]\n\n# Estimate the score on the entire dataset, with no missing values\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_full, y_full).mean()\nprint("Score with the entire dataset = %.2f" % score)\n\n# Add missing values in 75% of the lines\nmissing_rate = 0.75\nn_missing_samples = np.floor(n_samples * missing_rate)\nmissing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n                                      dtype=np.bool),\n                             np.ones(n_missing_samples,\n                                     dtype=np.bool)))\nrng.shuffle(missing_samples)\nmissing_features = rng.randint(0, n_features, n_missing_samples)\n\n# Estimate the score without the lines containing missing values\nX_filtered = X_full[~missing_samples, :]\ny_filtered = y_full[~missing_samples]\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_filtered, y_filtered).mean()\nprint("Score without the samples containing missing values = %.2f" % score)\n\n# Estimate the score after imputation of the missing values\nX_missing = X_full.copy()\nX_missing[np.where(missing_samples)[0], missing_features] = 0\ny_missing = y_full.copy()\nestimator = Pipeline([("imputer", Imputer(missing_values=0,\n                                          strategy="mean",\n                                          axis=0)),\n                      ("forest", RandomForestRegressor(random_state=0,\n                                                       n_estimators=100))])\nscore = cross_val_score(estimator, X_missing, y_missing).mean()\nprint("Score after imputation of the missing values = %.2f" % score)</code></pre>	http://scikit-learn.org/stable/auto_examples/missing_values.html
Classification of text documents: using a MLComp dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom time import time\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse as sp\nimport pylab as pl\n\nfrom sklearn.datasets import load_mlcomp\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nprint(__doc__)\n\nif 'MLCOMP_DATASETS_HOME' not in os.environ:\n    print("MLCOMP_DATASETS_HOME not set; please follow the above instructions")\n    sys.exit(0)\n\n# Load the training set\nprint("Loading 20 newsgroups training set... ")\nnews_train = load_mlcomp('20news-18828', 'train')\nprint(news_train.DESCR)\nprint("%d documents" % len(news_train.filenames))\nprint("%d categories" % len(news_train.target_names))\n\nprint("Extracting features from the dataset using a sparse vectorizer")\nt0 = time()\nvectorizer = TfidfVectorizer(encoding='latin1')\nX_train = vectorizer.fit_transform((open(f).read()\n                                    for f in news_train.filenames))\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X_train.shape)\nassert sp.issparse(X_train)\ny_train = news_train.target\n\nprint("Loading 20 newsgroups test set... ")\nnews_test = load_mlcomp('20news-18828', 'test')\nt0 = time()\nprint("done in %fs" % (time() - t0))\n\nprint("Predicting the labels of the test set...")\nprint("%d documents" % len(news_test.filenames))\nprint("%d categories" % len(news_test.target_names))\n\nprint("Extracting features from the dataset using the same vectorizer")\nt0 = time()\nX_test = vectorizer.transform((open(f).read() for f in news_test.filenames))\ny_test = news_test.target\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X_test.shape)\n\n\n###############################################################################\n# Benchmark classifiers\ndef benchmark(clf_class, params, name):\n    print("parameters:", params)\n    t0 = time()\n    clf = clf_class(**params).fit(X_train, y_train)\n    print("done in %fs" % (time() - t0))\n\n    if hasattr(clf, 'coef_'):\n        print("Percentage of non zeros coef: %f"\n              % (np.mean(clf.coef_ != 0) * 100))\n    print("Predicting the outcomes of the testing set")\n    t0 = time()\n    pred = clf.predict(X_test)\n    print("done in %fs" % (time() - t0))\n\n    print("Classification report on test set for classifier:")\n    print(clf)\n    print()\n    print(classification_report(y_test, pred,\n                                target_names=news_test.target_names))\n\n    cm = confusion_matrix(y_test, pred)\n    print("Confusion matrix:")\n    print(cm)\n\n    # Show confusion matrix\n    pl.matshow(cm)\n    pl.title('Confusion matrix of the %s classifier' % name)\n    pl.colorbar()\n\n\nprint("Testbenching a linear classifier...")\nparameters = {\n    'loss': 'hinge',\n    'penalty': 'l2',\n    'n_iter': 50,\n    'alpha': 0.00001,\n    'fit_intercept': True,\n}\n\nbenchmark(SGDClassifier, parameters, 'SGD')\n\nprint("Testbenching a MultinomialNB classifier...")\nparameters = {'alpha': 0.01}\n\nbenchmark(MultinomialNB, parameters, 'MultinomialNB')\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/mlcomp_sparse_document_classification.html
Discrete versus Real AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_adaboost_hastie_10_2_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n#         Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import zero_one_loss\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nn_estimators = 400\n# A learning rate of 1. may not be optimal for both SAMME and SAMME.R\nlearning_rate = 1.\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n\nX_test, y_test = X[2000:], y[2000:]\nX_train, y_train = X[:2000], y[:2000]\n\ndt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)\n\nada_discrete = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm="SAMME")\nada_discrete.fit(X_train, y_train)\n\nada_real = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm="SAMME.R")\nada_real.fit(X_train, y_train)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n        label='Decision Stump Error')\nax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n        label='Decision Tree Error')\n\nada_discrete_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n\nada_discrete_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n\nada_real_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n\nada_real_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = zero_one_loss(y_pred, y_train)\n\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err,\n        label='Discrete AdaBoost Test Error',\n        color='red')\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,\n        label='Discrete AdaBoost Train Error',\n        color='blue')\nax.plot(np.arange(n_estimators) + 1, ada_real_err,\n        label='Real AdaBoost Test Error',\n        color='orange')\nax.plot(np.arange(n_estimators) + 1, ada_real_err_train,\n        label='Real AdaBoost Train Error',\n        color='green')\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel('n_estimators')\nax.set_ylabel('error rate')\n\nleg = ax.legend(loc='upper right', fancybox=True)\nleg.get_frame().set_alpha(0.7)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html
Multi-class AdaBoosted Decision Trees	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_adaboost_multiclass_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nfrom sklearn.externals.six.moves import zip\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_gaussian_quantiles\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nX, y = make_gaussian_quantiles(n_samples=13000, n_features=10,\n                               n_classes=3, random_state=1)\n\nn_split = 3000\n\nX_train, X_test = X[:n_split], X[n_split:]\ny_train, y_test = y[:n_split], y[n_split:]\n\nbdt_real = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1)\n\nbdt_discrete = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1.5,\n    algorithm="SAMME")\n\nbdt_real.fit(X_train, y_train)\nbdt_discrete.fit(X_train, y_train)\n\nreal_test_errors = []\ndiscrete_test_errors = []\n\nfor real_test_predict, discrete_train_predict in zip(\n        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n    real_test_errors.append(\n        1. - accuracy_score(real_test_predict, y_test))\n    discrete_test_errors.append(\n        1. - accuracy_score(discrete_train_predict, y_test))\n\nn_trees_discrete = len(bdt_discrete)\nn_trees_real = len(bdt_real)\n\n# Boosting might terminate early, but the following arrays are always\n# n_estimators long. We crop them to the actual number of trees here:\ndiscrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\nreal_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\ndiscrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.plot(range(1, n_trees_discrete + 1),\n         discrete_test_errors, c='black', label='SAMME')\nplt.plot(range(1, n_trees_real + 1),\n         real_test_errors, c='black',\n         linestyle='dashed', label='SAMME.R')\nplt.legend()\nplt.ylim(0.18, 0.62)\nplt.ylabel('Test Error')\nplt.xlabel('Number of Trees')\n\nplt.subplot(132)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n         "b", label='SAMME', alpha=.5)\nplt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n         "r", label='SAMME.R', alpha=.5)\nplt.legend()\nplt.ylabel('Error')\nplt.xlabel('Number of Trees')\nplt.ylim((.2,\n         max(real_estimator_errors.max(),\n             discrete_estimator_errors.max()) * 1.2))\nplt.xlim((-20, len(bdt_discrete) + 20))\n\nplt.subplot(133)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n         "b", label='SAMME')\nplt.legend()\nplt.ylabel('Weight')\nplt.xlabel('Number of Trees')\nplt.ylim((0, discrete_estimator_weights.max() * 1.2))\nplt.xlim((-20, n_trees_discrete + 20))\n\n# prevent overlapping y-axis labels\nplt.subplots_adjust(wspace=0.25)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html
Decision Tree Regression with AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_adaboost_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\n# importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c="k", label="training samples")\nplt.plot(X, y_1, c="g", label="n_estimators=1", linewidth=2)\nplt.plot(X, y_2, c="r", label="n_estimators=300", linewidth=2)\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Boosted Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html
Two-class AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_adaboost_twoclass_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n\n# Construct dataset\nX1, y1 = make_gaussian_quantiles(cov=2.,\n                                 n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n                                 n_samples=300, n_features=2,\n                                 n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, - y2 + 1))\n\n# Create and fit an AdaBoosted decision tree\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                         algorithm="SAMME",\n                         n_estimators=200)\n\nbdt.fit(X, y)\n\nplot_colors = "br"\nplot_step = 0.02\nclass_names = "AB"\n\nplt.figure(figsize=(10, 5))\n\n# Plot the decision boundaries\nplt.subplot(121)\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n\nZ = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis("tight")\n\n# Plot the training points\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1],\n                c=c, cmap=plt.cm.Paired,\n                label="Class %s" % n)\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.legend(loc='upper right')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Decision Boundary')\n\n# Plot the two-class decision scores\ntwoclass_output = bdt.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\nplt.subplot(122)\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    plt.hist(twoclass_output[y == i],\n             bins=10,\n             range=plot_range,\n             facecolor=c,\n             label='Class %s' % n,\n             alpha=.5)\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, y1, y2 * 1.2))\nplt.legend(loc='upper right')\nplt.ylabel('Samples')\nplt.xlabel('Score')\nplt.title('Decision Scores')\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.35)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html
Adjustment for chance in clustering performance evaluation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom sklearn import metrics\n\n\ndef uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                             fixed_n_classes=None, n_runs=5, seed=42):\n    """Compute score for 2 random uniform cluster labelings.\n\n    Both random labelings have the same number of clusters for each value\n    possible value in ``n_clusters_range``.\n\n    When fixed_n_classes is not None the first labeling is considered a ground\n    truth class assignment with fixed number of classes.\n    """\n    random_labels = np.random.RandomState(seed).random_integers\n    scores = np.zeros((len(n_clusters_range), n_runs))\n\n    if fixed_n_classes is not None:\n        labels_a = random_labels(low=0, high=fixed_n_classes - 1,\n                                 size=n_samples)\n\n    for i, k in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            if fixed_n_classes is None:\n                labels_a = random_labels(low=0, high=k - 1, size=n_samples)\n            labels_b = random_labels(low=0, high=k - 1, size=n_samples)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores\n\nscore_funcs = [\n    metrics.adjusted_rand_score,\n    metrics.v_measure_score,\n    metrics.adjusted_mutual_info_score,\n    metrics.mutual_info_score,\n]\n\n# 2 independent random clusterings with equal cluster number\n\nn_samples = 100\nn_clusters_range = np.linspace(2, n_samples, 10).astype(np.int)\n\nplt.figure(1)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print("Computing %s for %d values of n_clusters and n_samples=%d"\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\n    print("done in %0.3fs" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title("Clustering measures for 2 random uniform labelings\n"\n          "with equal number of clusters")\nplt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\nplt.ylabel('Score value')\nplt.legend(plots, names)\nplt.ylim(ymin=-0.05, ymax=1.05)\n\n\n# Random labeling with varying n_clusters against ground class labels\n# with fixed number of clusters\n\nn_samples = 1000\nn_clusters_range = np.linspace(2, 100, 10).astype(np.int)\nn_classes = 10\n\nplt.figure(2)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print("Computing %s for %d values of n_clusters and n_samples=%d"\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                                      fixed_n_classes=n_classes)\n    print("done in %0.3fs" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title("Clustering measures for random uniform labeling\n"\n          "against reference assignment with %d classes" % n_classes)\nplt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\nplt.ylabel('Score value')\nplt.ylim(ymin=-0.05, ymax=1.05)\nplt.legend(plots, names)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html
Demo of affinity propagation clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_affinity_propagation_001.png]]	<br><pre><code>print(__doc__)\n\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn import metrics\nfrom sklearn.datasets.samples_generator import make_blobs\n\n##############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                            random_state=0)\n\n##############################################################################\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\nprint("Adjusted Rand Index: %0.3f"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint("Adjusted Mutual Information: %0.3f"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n\n##############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.close('all')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html
Agglomerative clustering with and without structure	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Gael Varoquaux, Nelle Varoquaux\n# License: BSD 3 clause\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import kneighbors_graph\n\n# Generate sample data\nn_samples = 1500\nnp.random.seed(0)\nt = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\nx = t * np.cos(t)\ny = t * np.sin(t)\n\n\nX = np.concatenate((x, y))\nX += .7 * np.random.randn(2, n_samples)\nX = X.T\n\n# Create a graph capturing local connectivity. Larger number of neighbors\n# will give more homogeneous clusters to the cost of computation\n# time. A very large number of neighbors gives more evenly distributed\n# cluster sizes, but may not impose the local manifold structure of\n# the data\nknn_graph = kneighbors_graph(X, 30, include_self=False)\n\nfor connectivity in (None, knn_graph):\n    for n_clusters in (30, 3):\n        plt.figure(figsize=(10, 4))\n        for index, linkage in enumerate(('average', 'complete', 'ward')):\n            plt.subplot(1, 3, index + 1)\n            model = AgglomerativeClustering(linkage=linkage,\n                                            connectivity=connectivity,\n                                            n_clusters=n_clusters)\n            t0 = time.time()\n            model.fit(X)\n            elapsed_time = time.time() - t0\n            plt.scatter(X[:, 0], X[:, 1], c=model.labels_,\n                        cmap=plt.cm.spectral)\n            plt.title('linkage=%s (time %.2fs)' % (linkage, elapsed_time),\n                      fontdict=dict(verticalalignment='top'))\n            plt.axis('equal')\n            plt.axis('off')\n\n            plt.subplots_adjust(bottom=0, top=.89, wspace=0,\n                                left=0, right=1)\n            plt.suptitle('n_cluster=%i, connectivity=%r' %\n                         (n_clusters, connectivity is not None), size=17)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html
Agglomerative clustering with different metrics	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Gael Varoquaux\n# License: BSD 3-Clause or CC-0\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(0)\n\n# Generate waveform data\nn_features = 2000\nt = np.pi * np.linspace(0, 1, n_features)\n\n\ndef sqr(x):\n    return np.sign(np.cos(x))\n\nX = list()\ny = list()\nfor i, (phi, a) in enumerate([(.5, .15), (.5, .6), (.3, .2)]):\n    for _ in range(30):\n        phase_noise = .01 * np.random.normal()\n        amplitude_noise = .04 * np.random.normal()\n        additional_noise = 1 - 2 * np.random.rand(n_features)\n        # Make the noise sparse\n        additional_noise[np.abs(additional_noise) < .997] = 0\n\n        X.append(12 * ((a + amplitude_noise)\n                 * (sqr(6 * (t + phi + phase_noise)))\n                 + additional_noise))\n        y.append(i)\n\nX = np.array(X)\ny = np.array(y)\n\nn_clusters = 3\n\nlabels = ('Waveform 1', 'Waveform 2', 'Waveform 3')\n\n# Plot the ground-truth labelling\nplt.figure()\nplt.axes([0, 0, 1, 1])\nfor l, c, n in zip(range(n_clusters), 'rgb',\n                   labels):\n    lines = plt.plot(X[y == l].T, c=c, alpha=.5)\n    lines[0].set_label(n)\n\nplt.legend(loc='best')\n\nplt.axis('tight')\nplt.axis('off')\nplt.suptitle("Ground truth", size=20)\n\n\n# Plot the distances\nfor index, metric in enumerate(["cosine", "euclidean", "cityblock"]):\n    avg_dist = np.zeros((n_clusters, n_clusters))\n    plt.figure(figsize=(5, 4.5))\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            avg_dist[i, j] = pairwise_distances(X[y == i], X[y == j],\n                                                metric=metric).mean()\n    avg_dist /= avg_dist.max()\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            plt.text(i, j, '%5.3f' % avg_dist[i, j],\n                     verticalalignment='center',\n                     horizontalalignment='center')\n\n    plt.imshow(avg_dist, interpolation='nearest', cmap=plt.cm.gnuplot2,\n               vmin=0)\n    plt.xticks(range(n_clusters), labels, rotation=45)\n    plt.yticks(range(n_clusters), labels)\n    plt.colorbar()\n    plt.suptitle("Interclass %s distances" % metric, size=18)\n    plt.tight_layout()\n\n\n# Plot clustering results\nfor index, metric in enumerate(["cosine", "euclidean", "cityblock"]):\n    model = AgglomerativeClustering(n_clusters=n_clusters,\n                                    linkage="average", affinity=metric)\n    model.fit(X)\n    plt.figure()\n    plt.axes([0, 0, 1, 1])\n    for l, c in zip(np.arange(model.n_clusters), 'rgbk'):\n        plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)\n    plt.axis('tight')\n    plt.axis('off')\n    plt.suptitle("AgglomerativeClustering(affinity=%s)" % metric, size=20)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html
Hyper-parameters of Approximate Nearest Neighbors	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import division\nprint(__doc__)\n\n# Author: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>\n#\n# License: BSD 3 clause\n\n\n###############################################################################\nimport numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.neighbors import LSHForest\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\n\n\n# Initialize size of the database, iterations and required neighbors.\nn_samples = 10000\nn_features = 100\nn_queries = 30\nrng = np.random.RandomState(42)\n\n# Generate sample data\nX, _ = make_blobs(n_samples=n_samples + n_queries,\n                  n_features=n_features, centers=10,\n                  random_state=0)\nX_index = X[:n_samples]\nX_query = X[n_samples:]\n# Get exact neighbors\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='brute',\n                        metric='cosine').fit(X_index)\nneighbors_exact = nbrs.kneighbors(X_query, return_distance=False)\n\n# Set `n_candidate` values\nn_candidates_values = np.linspace(10, 500, 5).astype(np.int)\nn_estimators_for_candidate_value = [1, 5, 10]\nn_iter = 10\nstds_accuracies = np.zeros((len(n_estimators_for_candidate_value),\n                            n_candidates_values.shape[0]),\n                           dtype=float)\naccuracies_c = np.zeros((len(n_estimators_for_candidate_value),\n                         n_candidates_values.shape[0]), dtype=float)\n\n# LSH Forest is a stochastic index: perform several iteration to estimate\n# expected accuracy and standard deviation displayed as error bars in\n# the plots\nfor j, value in enumerate(n_estimators_for_candidate_value):\n    for i, n_candidates in enumerate(n_candidates_values):\n        accuracy_c = []\n        for seed in range(n_iter):\n            lshf = LSHForest(n_estimators=value,\n                             n_candidates=n_candidates, n_neighbors=1,\n                             random_state=seed)\n            # Build the LSH Forest index\n            lshf.fit(X_index)\n            # Get neighbors\n            neighbors_approx = lshf.kneighbors(X_query,\n                                               return_distance=False)\n            accuracy_c.append(np.sum(np.equal(neighbors_approx,\n                                              neighbors_exact)) /\n                              n_queries)\n\n        stds_accuracies[j, i] = np.std(accuracy_c)\n        accuracies_c[j, i] = np.mean(accuracy_c)\n\n# Set `n_estimators` values\nn_estimators_values = [1, 5, 10, 20, 30, 40, 50]\naccuracies_trees = np.zeros(len(n_estimators_values), dtype=float)\n\n# Calculate average accuracy for each value of `n_estimators`\nfor i, n_estimators in enumerate(n_estimators_values):\n    lshf = LSHForest(n_estimators=n_estimators, n_neighbors=1)\n    # Build the LSH Forest index\n    lshf.fit(X_index)\n    # Get neighbors\n    neighbors_approx = lshf.kneighbors(X_query, return_distance=False)\n    accuracies_trees[i] = np.sum(np.equal(neighbors_approx,\n                                          neighbors_exact))/n_queries\n\n###############################################################################\n# Plot the accuracy variation with `n_candidates`\nplt.figure()\ncolors = ['c', 'm', 'y']\nfor i, n_estimators in enumerate(n_estimators_for_candidate_value):\n    label = 'n_estimators = %d ' % n_estimators\n    plt.plot(n_candidates_values, accuracies_c[i, :],\n             'o-', c=colors[i], label=label)\n    plt.errorbar(n_candidates_values, accuracies_c[i, :],\n                 stds_accuracies[i, :], c=colors[i])\n\nplt.legend(loc='upper left', fontsize='small')\nplt.ylim([0, 1.2])\nplt.xlim(min(n_candidates_values), max(n_candidates_values))\nplt.ylabel("Accuracy")\nplt.xlabel("n_candidates")\nplt.grid(which='both')\nplt.title("Accuracy variation with n_candidates")\n\n# Plot the accuracy variation with `n_estimators`\nplt.figure()\nplt.scatter(n_estimators_values, accuracies_trees, c='k')\nplt.plot(n_estimators_values, accuracies_trees, c='g')\nplt.ylim([0, 1.2])\nplt.xlim(min(n_estimators_values), max(n_estimators_values))\nplt.ylabel("Accuracy")\nplt.xlabel("n_estimators")\nplt.grid(which='both')\nplt.title("Accuracy variation with n_estimators")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.html
Scalability of Approximate Nearest Neighbors	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import division\nprint(__doc__)\n\n# Authors: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#\n# License: BSD 3 clause\n\n\n###############################################################################\nimport time\nimport numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.neighbors import LSHForest\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\n\n# Parameters of the study\nn_samples_min = int(1e3)\nn_samples_max = int(1e5)\nn_features = 100\nn_centers = 100\nn_queries = 100\nn_steps = 6\nn_iter = 5\n\n# Initialize the range of `n_samples`\nn_samples_values = np.logspace(np.log10(n_samples_min),\n                               np.log10(n_samples_max),\n                               n_steps).astype(np.int)\n\n# Generate some structured data\nrng = np.random.RandomState(42)\nall_data, _ = make_blobs(n_samples=n_samples_max + n_queries,\n                         n_features=n_features, centers=n_centers, shuffle=True,\n                         random_state=0)\nqueries = all_data[:n_queries]\nindex_data = all_data[n_queries:]\n\n# Metrics to collect for the plots\naverage_times_exact = []\naverage_times_approx = []\nstd_times_approx = []\naccuracies = []\nstd_accuracies = []\naverage_speedups = []\nstd_speedups = []\n\n# Calculate the average query time\nfor n_samples in n_samples_values:\n    X = index_data[:n_samples]\n    # Initialize LSHForest for queries of a single neighbor\n    lshf = LSHForest(n_estimators=20, n_candidates=200,\n                     n_neighbors=10).fit(X)\n    nbrs = NearestNeighbors(algorithm='brute', metric='cosine',\n                            n_neighbors=10).fit(X)\n    time_approx = []\n    time_exact = []\n    accuracy = []\n\n    for i in range(n_iter):\n        # pick one query at random to study query time variability in LSHForest\n        query = queries[[rng.randint(0, n_queries)]]\n\n        t0 = time.time()\n        exact_neighbors = nbrs.kneighbors(query, return_distance=False)\n        time_exact.append(time.time() - t0)\n\n        t0 = time.time()\n        approx_neighbors = lshf.kneighbors(query, return_distance=False)\n        time_approx.append(time.time() - t0)\n\n        accuracy.append(np.in1d(approx_neighbors, exact_neighbors).mean())\n\n    average_time_exact = np.mean(time_exact)\n    average_time_approx = np.mean(time_approx)\n    speedup = np.array(time_exact) / np.array(time_approx)\n    average_speedup = np.mean(speedup)\n    mean_accuracy = np.mean(accuracy)\n    std_accuracy = np.std(accuracy)\n    print("Index size: %d, exact: %0.3fs, LSHF: %0.3fs, speedup: %0.1f, "\n          "accuracy: %0.2f +/-%0.2f" %\n          (n_samples, average_time_exact, average_time_approx, average_speedup,\n           mean_accuracy, std_accuracy))\n\n    accuracies.append(mean_accuracy)\n    std_accuracies.append(std_accuracy)\n    average_times_exact.append(average_time_exact)\n    average_times_approx.append(average_time_approx)\n    std_times_approx.append(np.std(time_approx))\n    average_speedups.append(average_speedup)\n    std_speedups.append(np.std(speedup))\n\n# Plot average query time against n_samples\nplt.figure()\nplt.errorbar(n_samples_values, average_times_approx, yerr=std_times_approx,\n             fmt='o-', c='r', label='LSHForest')\nplt.plot(n_samples_values, average_times_exact, c='b',\n         label="NearestNeighbors(algorithm='brute', metric='cosine')")\nplt.legend(loc='upper left', fontsize='small')\nplt.ylim(0, None)\nplt.ylabel("Average query time in seconds")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("Impact of index size on response time for first "\n          "nearest neighbors queries")\n\n# Plot average query speedup versus index size\nplt.figure()\nplt.errorbar(n_samples_values, average_speedups, yerr=std_speedups,\n             fmt='o-', c='r')\nplt.ylim(0, None)\nplt.ylabel("Average speedup")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("Speedup of the approximate NN queries vs brute force")\n\n# Plot average precision versus index size\nplt.figure()\nplt.errorbar(n_samples_values, accuracies, std_accuracies, fmt='o-', c='c')\nplt.ylim(0, 1.1)\nplt.ylabel("precision@10")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("precision of 10-nearest-neighbors queries with index size")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability.html
Automatic Relevance Determination Regression (ARD)	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import ARDRegression, LinearRegression\n\n###############################################################################\n# Generating simulated data with Gaussian weights\n\n# Parameters of the example\nnp.random.seed(0)\nn_samples, n_features = 100, 100\n# Create Gaussian data\nX = np.random.randn(n_samples, n_features)\n# Create weigts with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noite with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n###############################################################################\n# Fit the ARD Regression\nclf = ARDRegression(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n###############################################################################\n# Plot the true weights, the estimated weights and the histogram of the\n# weights\nplt.figure(figsize=(6, 5))\nplt.title("Weights of the model")\nplt.plot(clf.coef_, 'b-', label="ARD estimate")\nplt.plot(ols.coef_, 'r--', label="OLS estimate")\nplt.plot(w, 'g-', label="Ground truth")\nplt.xlabel("Features")\nplt.ylabel("Values of the weights")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title("Histogram of the weights")\nplt.hist(clf.coef_, bins=n_features, log=True)\nplt.plot(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),\n         'ro', label="Relevant features")\nplt.ylabel("Features")\nplt.xlabel("Values of the weights")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title("Marginal log-likelihood")\nplt.plot(clf.scores_)\nplt.ylabel("Score")\nplt.xlabel("Iterations")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html
Bayesian Ridge Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\n\n###############################################################################\n# Generating simulated data with Gaussian weigthts\nnp.random.seed(0)\nn_samples, n_features = 100, 100\nX = np.random.randn(n_samples, n_features)  # Create Gaussian data\n# Create weigts with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noise with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n###############################################################################\n# Fit the Bayesian Ridge Regression and an OLS for comparison\nclf = BayesianRidge(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n###############################################################################\n# Plot true weights, estimated weights and histogram of the weights\nplt.figure(figsize=(6, 5))\nplt.title("Weights of the model")\nplt.plot(clf.coef_, 'b-', label="Bayesian Ridge estimate")\nplt.plot(w, 'g-', label="Ground truth")\nplt.plot(ols.coef_, 'r--', label="OLS estimate")\nplt.xlabel("Features")\nplt.ylabel("Values of the weights")\nplt.legend(loc="best", prop=dict(size=12))\n\nplt.figure(figsize=(6, 5))\nplt.title("Histogram of the weights")\nplt.hist(clf.coef_, bins=n_features, log=True)\nplt.plot(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),\n         'ro', label="Relevant features")\nplt.ylabel("Features")\nplt.xlabel("Values of the weights")\nplt.legend(loc="lower left")\n\nplt.figure(figsize=(6, 5))\nplt.title("Marginal log-likelihood")\nplt.plot(clf.scores_)\nplt.ylabel("Score")\nplt.xlabel("Iterations")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html
Single estimator versus bagging: bias-variance decomposition	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_bias_variance_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Gilles Louppe <g.louppe@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Settings\nn_repeat = 50       # Number of iterations for computing expectations\nn_train = 50        # Size of the training set\nn_test = 1000       # Size of the test set\nnoise = 0.1         # Standard deviation of the noise\nnp.random.seed(0)\n\n# Change this for exploring the bias-variance decomposition of other\n# estimators. This should work well for estimators with high variance (e.g.,\n# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n# linear models).\nestimators = [("Tree", DecisionTreeRegressor()),\n              ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]\n\nn_estimators = len(estimators)\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\ndef generate(n_samples, noise, n_repeat=1):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X)\n\n    if n_repeat == 1:\n        y = f(X) + np.random.normal(0.0, noise, n_samples)\n    else:\n        y = np.zeros((n_samples, n_repeat))\n\n        for i in range(n_repeat):\n            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n\nX_train = []\ny_train = []\n\nfor i in range(n_repeat):\n    X, y = generate(n_samples=n_train, noise=noise)\n    X_train.append(X)\n    y_train.append(y)\n\nX_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n\n# Loop over estimators to compare\nfor n, (name, estimator) in enumerate(estimators):\n    # Compute predictions\n    y_predict = np.zeros((n_test, n_repeat))\n\n    for i in range(n_repeat):\n        estimator.fit(X_train[i], y_train[i])\n        y_predict[:, i] = estimator.predict(X_test)\n\n    # Bias^2 + Variance + Noise decomposition of the mean squared error\n    y_error = np.zeros(n_test)\n\n    for i in range(n_repeat):\n        for j in range(n_repeat):\n            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n\n    y_error /= (n_repeat * n_repeat)\n\n    y_noise = np.var(y_test, axis=1)\n    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n    y_var = np.var(y_predict, axis=1)\n\n    print("{0}: {1:.4f} (error) = {2:.4f} (bias^2) "\n          " + {3:.4f} (var) + {4:.4f} (noise)".format(name,\n                                                      np.mean(y_error),\n                                                      np.mean(y_bias),\n                                                      np.mean(y_var),\n                                                      np.mean(y_noise)))\n\n    # Plot figures\n    plt.subplot(2, n_estimators, n + 1)\n    plt.plot(X_test, f(X_test), "b", label="$f(x)$")\n    plt.plot(X_train[0], y_train[0], ".b", label="LS ~ $y = f(x)+noise$")\n\n    for i in range(n_repeat):\n        if i == 0:\n            plt.plot(X_test, y_predict[:, i], "r", label="$\^y(x)$")\n        else:\n            plt.plot(X_test, y_predict[:, i], "r", alpha=0.05)\n\n    plt.plot(X_test, np.mean(y_predict, axis=1), "c",\n             label="$\mathbb{E}_{LS} \^y(x)$")\n\n    plt.xlim([-5, 5])\n    plt.title(name)\n\n    if n == 0:\n        plt.legend(loc="upper left", prop={"size": 11})\n\n    plt.subplot(2, n_estimators, n_estimators + n + 1)\n    plt.plot(X_test, y_error, "r", label="$error(x)$")\n    plt.plot(X_test, y_bias, "b", label="$bias^2(x)$"),\n    plt.plot(X_test, y_var, "g", label="$variance(x)$"),\n    plt.plot(X_test, y_noise, "c", label="$noise(x)$")\n\n    plt.xlim([-5, 5])\n    plt.ylim([0, 0.1])\n\n    if n == 0:\n        plt.legend(loc="upper left", prop={"size": 11})\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html
Compare BIRCH and MiniBatchKMeans	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_birch_vs_minibatchkmeans_001.png]]	<br><pre><code># Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com\n#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom itertools import cycle\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import Birch, MiniBatchKMeans\nfrom sklearn.datasets.samples_generator import make_blobs\n\n\n# Generate centers for the blobs so that it forms a 10 X 10 grid.\nxx = np.linspace(-22, 22, 10)\nyy = np.linspace(-22, 22, 10)\nxx, yy = np.meshgrid(xx, yy)\nn_centres = np.hstack((np.ravel(xx)[:, np.newaxis],\n                       np.ravel(yy)[:, np.newaxis]))\n\n# Generate blobs to do a comparison between MiniBatchKMeans and Birch.\nX, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)\n   \n\n# Use all colors that matplotlib provides by default.\ncolors_ = cycle(colors.cnames.keys())\n\nfig = plt.figure(figsize=(12, 4))\nfig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)\n\n# Compute clustering with Birch with and without the final clustering step\n# and plot.\nbirch_models = [Birch(threshold=1.7, n_clusters=None),\n                Birch(threshold=1.7, n_clusters=100)]\nfinal_step = ['without global clustering', 'with global clustering']\n\nfor ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):\n    t = time()\n    birch_model.fit(X)\n    time_ = time() - t\n    print("Birch %s as the final step took %0.2f seconds" % (\n          info, (time() - t)))\n\n    # Plot result\n    labels = birch_model.labels_\n    centroids = birch_model.subcluster_centers_\n    n_clusters = np.unique(labels).size\n    print("n_clusters : %d" % n_clusters)\n\n    ax = fig.add_subplot(1, 3, ind + 1)\n    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\n        mask = labels == k\n        ax.plot(X[mask, 0], X[mask, 1], 'w',\n                markerfacecolor=col, marker='.')\n        if birch_model.n_clusters is None:\n            ax.plot(this_centroid[0], this_centroid[1], '+', markerfacecolor=col,\n                    markeredgecolor='k', markersize=5)\n    ax.set_ylim([-25, 25])\n    ax.set_xlim([-25, 25])\n    ax.set_autoscaley_on(False)\n    ax.set_title('Birch %s' % info)\n\n# Compute clustering with MiniBatchKMeans.\nmbk = MiniBatchKMeans(init='k-means++', n_clusters=100, batch_size=100,\n                      n_init=10, max_no_improvement=10, verbose=0,\n                      random_state=0)\nt0 = time()\nmbk.fit(X)\nt_mini_batch = time() - t0\nprint("Time taken to run MiniBatchKMeans %0.2f seconds" % t_mini_batch)\nmbk_means_labels_unique = np.unique(mbk.labels_)\n\nax = fig.add_subplot(1, 3, 3)\nfor this_centroid, k, col in zip(mbk.cluster_centers_,\n                                 range(n_clusters), colors_):\n    mask = mbk.labels_ == k\n    ax.plot(X[mask, 0], X[mask, 1], 'w', markerfacecolor=col, marker='.')\n    ax.plot(this_centroid[0], this_centroid[1], '+', markeredgecolor='k',\n            markersize=5)\nax.set_xlim([-25, 25])\nax.set_ylim([-25, 25])\nax.set_title("MiniBatchKMeans")\nax.set_autoscaley_on(False)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html
Probability calibration of classifiers	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Mathieu Blondel <mathieu@mblondel.org>\n#         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.cross_validation import train_test_split\n\n\nn_samples = 50000\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\n\n# Generate 3 blobs with 2 classes where the second blob contains\n# half positive samples and half negative samples. Probability in this\n# blob is therefore 0.5.\ncenters = [(-5, -5), (0, 0), (5, 5)]\nX, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0,\n                  centers=centers, shuffle=False, random_state=42)\n\ny[:n_samples // 2] = 0\ny[n_samples // 2:] = 1\nsample_weight = np.random.RandomState(42).rand(y.shape[0])\n\n# split train, test for calibration\nX_train, X_test, y_train, y_test, sw_train, sw_test = \\n    train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)\n\n# Gaussian Naive-Bayes with no calibration\nclf = GaussianNB()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with isotonic calibration\nclf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')\nclf_isotonic.fit(X_train, y_train, sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with sigmoid calibration\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train, sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint("Brier scores: (the smaller the better)")\n\nclf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)\nprint("No calibration: %1.3f" % clf_score)\n\nclf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)\nprint("With isotonic calibration: %1.3f" % clf_isotonic_score)\n\nclf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)\nprint("With sigmoid calibration: %1.3f" % clf_sigmoid_score)\n\n###############################################################################\n# Plot the data and the predicted probabilities\nplt.figure()\ny_unique = np.unique(y)\ncolors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color, alpha=0.5,\n                label="Class %s" % this_y)\nplt.legend(loc="best")\nplt.title("Data")\n\nplt.figure()\norder = np.lexsort((prob_pos_clf, ))\nplt.plot(prob_pos_clf[order], 'r', label='No calibration (%1.3f)' % clf_score)\nplt.plot(prob_pos_isotonic[order], 'g', linewidth=3,\n         label='Isotonic calibration (%1.3f)' % clf_isotonic_score)\nplt.plot(prob_pos_sigmoid[order], 'b', linewidth=3,\n         label='Sigmoid calibration (%1.3f)' % clf_sigmoid_score)\nplt.plot(np.linspace(0, y_test.size, 51)[1::2],\n         y_test[order].reshape(25, -1).mean(1),\n         'k', linewidth=3, label=r'Empirical')\nplt.ylim([-0.05, 1.05])\nplt.xlabel("Instances sorted according to predicted probability "\n           "(uncalibrated GNB)")\nplt.ylabel("P(y=1)")\nplt.legend(loc="upper left")\nplt.title("Gaussian naive Bayes probabilities")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html
Probability Calibration curves	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score)\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.cross_validation import train_test_split\n\n\n# Create dataset of classification task with many redundant and few\n# informative features\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=10,\n                                    random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,\n                                                    random_state=42)\n\n\ndef plot_calibration_curve(est, name, fig_index):\n    """Plot calibration curve for est w/o and with calibration. """\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1., solver='lbfgs')\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")\n    for clf, name in [(lr, 'Logistic'),\n                      (est, name),\n                      (isotonic, name + ' + Isotonic'),\n                      (sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        if hasattr(clf, "predict_proba"):\n            prob_pos = clf.predict_proba(X_test)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_test)\n            prob_pos = \\n                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n\n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n        print("%s:" % name)\n        print("\tBrier: %1.3f" % (clf_score))\n        print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))\n        print("\tRecall: %1.3f" % recall_score(y_test, y_pred))\n        print("\tF1: %1.3f\n" % f1_score(y_test, y_pred))\n\n        fraction_of_positives, mean_predicted_value = \\n            calibration_curve(y_test, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, "s-",\n                 label="%s (%1.3f)" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype="step", lw=2)\n\n    ax1.set_ylabel("Fraction of positives")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc="lower right")\n    ax1.set_title('Calibration plots  (reliability curve)')\n\n    ax2.set_xlabel("Mean predicted value")\n    ax2.set_ylabel("Count")\n    ax2.legend(loc="upper center", ncol=2)\n\n    plt.tight_layout()\n\n# Plot calibration cuve for Gaussian Naive Bayes\nplot_calibration_curve(GaussianNB(), "Naive Bayes", 1)\n\n# Plot calibration cuve for Linear SVC\nplot_calibration_curve(LinearSVC(), "SVC", 2)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html
Probability Calibration for 3-class classification	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss\n\nnp.random.seed(0)\n\n# Generate data\nX, y = make_blobs(n_samples=1000, n_features=2, random_state=42,\n                  cluster_std=5.0)\nX_train, y_train = X[:600], y[:600]\nX_valid, y_valid = X[600:800], y[600:800]\nX_train_valid, y_train_valid = X[:800], y[:800]\nX_test, y_test = X[800:], y[800:]\n\n# Train uncalibrated random forest classifier on whole train and validation\n# data and evaluate on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)\nclf_probs = clf.predict_proba(X_test)\nscore = log_loss(y_test, clf_probs)\n\n# Train random forest classifier, calibrate on validation data and evaluate\n# on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train, y_train)\nclf_probs = clf.predict_proba(X_test)\nsig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")\nsig_clf.fit(X_valid, y_valid)\nsig_clf_probs = sig_clf.predict_proba(X_test)\nsig_score = log_loss(y_test, sig_clf_probs)\n\n# Plot changes in predicted probabilities via arrows\nplt.figure(0)\ncolors = ["r", "g", "b"]\nfor i in range(clf_probs.shape[0]):\n    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],\n              sig_clf_probs[i, 0] - clf_probs[i, 0],\n              sig_clf_probs[i, 1] - clf_probs[i, 1],\n              color=colors[y_test[i]], head_width=1e-2)\n\n# Plot perfect predictions\nplt.plot([1.0], [0.0], 'ro', ms=20, label="Class 1")\nplt.plot([0.0], [1.0], 'go', ms=20, label="Class 2")\nplt.plot([0.0], [0.0], 'bo', ms=20, label="Class 3")\n\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")\n\n# Annotate points on the simplex\nplt.annotate(r'($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)',\n             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.plot([1.0/3], [1.0/3], 'ko', ms=5)\nplt.annotate(r'($\frac{1}{2}$, $0$, $\frac{1}{2}$)',\n             xy=(.5, .0), xytext=(.5, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $\frac{1}{2}$, $\frac{1}{2}$)',\n             xy=(.0, .5), xytext=(.1, .5), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($\frac{1}{2}$, $\frac{1}{2}$, $0$)',\n             xy=(.5, .5), xytext=(.6, .6), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $0$, $1$)',\n             xy=(0, 0), xytext=(.1, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($1$, $0$, $0$)',\n             xy=(1, 0), xytext=(1, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $1$, $0$)',\n             xy=(0, 1), xytext=(.1, 1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\n# Add grid\nplt.grid("off")\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n\nplt.title("Change of predicted probabilities after sigmoid calibration")\nplt.xlabel("Probability class 1")\nplt.ylabel("Probability class 2")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\nplt.legend(loc="best")\n\nprint("Log-loss of")\nprint(" * uncalibrated classifier trained on 800 datapoints: %.3f "\n      % score)\nprint(" * classifier trained on 600 datapoints and calibrated on "\n      "200 datapoint: %.3f" % sig_score)\n\n# Illustrate calibrator\nplt.figure(1)\n# generate grid over 2-simplex\np1d = np.linspace(0, 1, 20)\np0, p1 = np.meshgrid(p1d, p1d)\np2 = 1 - p0 - p1\np = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]\np = p[p[:, 2] >= 0]\n\ncalibrated_classifier = sig_clf.calibrated_classifiers_[0]\nprediction = np.vstack([calibrator.predict(this_p)\n                        for calibrator, this_p in\n                        zip(calibrated_classifier.calibrators_, p.T)]).T\nprediction /= prediction.sum(axis=1)[:, None]\n\n# Ploit modifications of calibrator\nfor i in range(prediction.shape[0]):\n    plt.arrow(p[i, 0], p[i, 1],\n              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],\n              head_width=1e-2, color=colors[np.argmax(p[i])])\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")\n\nplt.grid("off")\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n\nplt.title("Illustration of sigmoid calibrator")\nplt.xlabel("Probability class 1")\nplt.ylabel("Probability class 2")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_multiclass.html
Nearest Neighbors Classification	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title("3-Class classification (k = %i, weights = '%s')"\n              % (n_neighbors, weights))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html
Plot classification probability	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_classification_probability_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nC = 1.0\n\n# Create different classifiers. The logistic regression cannot do\n# multiclass out of the box.\nclassifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),\n               'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),\n               'Linear SVC': SVC(kernel='linear', C=C, probability=True,\n                                 random_state=0),\n               'L2 logistic (Multinomial)': LogisticRegression(\n                C=C, solver='lbfgs', multi_class='multinomial'\n                )}\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * 2, n_classifiers * 2))\nplt.subplots_adjust(bottom=.2, top=.95)\n\nxx = np.linspace(3, 9, 100)\nyy = np.linspace(1, 5, 100).T\nxx, yy = np.meshgrid(xx, yy)\nXfull = np.c_[xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100\n    print("classif_rate for %s : %f " % (name, classif_rate))\n\n    # View probabilities=\n    probas = classifier.predict_proba(Xfull)\n    n_classes = np.unique(y_pred).size\n    for k in range(n_classes):\n        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n        plt.title("Class %d" % k)\n        if k == 0:\n            plt.ylabel(name)\n        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                   extent=(3, 9, 1, 5), origin='lower')\n        plt.xticks(())\n        plt.yticks(())\n        idx = (y_pred == k)\n        if idx.any():\n            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='k')\n\nax = plt.axes([0.15, 0.04, 0.7, 0.05])\nplt.title("Probability")\nplt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html
Classifier comparison	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_classifier_comparison_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n#              Andreas Müller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Decision Tree",\n         "Random Forest", "AdaBoost", "Naive Bayes", "Linear Discriminant Analysis",\n         "Quadratic Discriminant Analysis"]\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel="linear", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds in datasets:\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, m_max]x[y_min, y_max].\n        if hasattr(clf, "decision_function"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot also the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nfigure.subplots_adjust(left=.02, right=.98)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html
Comparing different clustering algorithms on toy datasets	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_cluster_comparison_001.png]]	<br><pre><code>print(__doc__)\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(0)\n\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\ncolors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\ncolors = np.hstack([colors] * 20)\n\nclustering_names = [\n    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',\n    'SpectralClustering', 'Ward', 'AgglomerativeClustering',\n    'DBSCAN', 'Birch']\n\nplt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndatasets = [noisy_circles, noisy_moons, blobs, no_structure]\nfor i_dataset, dataset in enumerate(datasets):\n    X, y = dataset\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # create clustering estimators\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',\n                                           connectivity=connectivity)\n    spectral = cluster.SpectralClustering(n_clusters=2,\n                                          eigen_solver='arpack',\n                                          affinity="nearest_neighbors")\n    dbscan = cluster.DBSCAN(eps=.2)\n    affinity_propagation = cluster.AffinityPropagation(damping=.9,\n                                                       preference=-200)\n\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage="average", affinity="cityblock", n_clusters=2,\n        connectivity=connectivity)\n\n    birch = cluster.Birch(n_clusters=2)\n    clustering_algorithms = [\n        two_means, affinity_propagation, ms, spectral, ward, average_linkage,\n        dbscan, birch]\n\n    for name, algorithm in zip(clustering_names, clustering_algorithms):\n        # predict cluster memberships\n        t0 = time.time()\n        algorithm.fit(X)\n        t1 = time.time()\n        if hasattr(algorithm, 'labels_'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        # plot\n        plt.subplot(4, len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n\n        if hasattr(algorithm, 'cluster_centers_'):\n            centers = algorithm.cluster_centers_\n            center_colors = colors[:len(centers)]\n            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n        plt.xlim(-2, 2)\n        plt.ylim(-2, 2)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment='right')\n        plot_num += 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html
K-means Clustering	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nestimators = {'k_means_iris_3': KMeans(n_clusters=3),\n              'k_means_iris_8': KMeans(n_clusters=8),\n              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n                                              init='random')}\n\n\nfignum = 1\nfor name, est in estimators.items():\n    fig = plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\n    plt.cla()\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel('Petal width')\n    ax.set_ylabel('Sepal length')\n    ax.set_zlabel('Petal length')\n    fignum = fignum + 1\n\n# Plot the ground truth\nfig = plt.figure(fignum, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n\nfor name, label in [('Setosa', 0),\n                    ('Versicolour', 1),\n                    ('Virginica', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html
Color Quantization using K-Means	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Robert Layton <robertlayton@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nprint(__doc__)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.utils import shuffle\nfrom time import time\n\nn_colors = 64\n\n# Load the Summer Palace photo\nchina = load_sample_image("china.jpg")\n\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1]\nchina = np.array(china, dtype=np.float64) / 255\n\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = np.reshape(china, (w * h, d))\n\nprint("Fitting model on a small sub-sample of the data")\nt0 = time()\nimage_array_sample = shuffle(image_array, random_state=0)[:1000]\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\nprint("done in %0.3fs." % (time() - t0))\n\n# Get labels for all points\nprint("Predicting color indices on the full image (k-means)")\nt0 = time()\nlabels = kmeans.predict(image_array)\nprint("done in %0.3fs." % (time() - t0))\n\n\ncodebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]\nprint("Predicting color indices on the full image (random)")\nt0 = time()\nlabels_random = pairwise_distances_argmin(codebook_random,\n                                          image_array,\n                                          axis=0)\nprint("done in %0.3fs." % (time() - t0))\n\n\ndef recreate_image(codebook, labels, w, h):\n    """Recreate the (compressed) image from the code book & labels"""\n    d = codebook.shape[1]\n    image = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n\n# Display all results, alongside original image\nplt.figure(1)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Original image (96,615 colors)')\nplt.imshow(china)\n\nplt.figure(2)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, K-Means)')\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\nplt.figure(3)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, Random)')\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html
Comparison of Calibration of Classifiers	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_compare_calibration_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=2)\n\ntrain_samples = 100  # Samples used for training the models\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\n# Create classifiers\nlr = LogisticRegression()\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\nrfc = RandomForestClassifier(n_estimators=100)\n\n\n###############################################################################\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(X_train, y_train)\n    if hasattr(clf, "predict_proba"):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\n            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, "s-",\n             label="%s" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype="step", lw=2)\n\nax1.set_ylabel("Fraction of positives")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc="lower right")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel("Mean predicted value")\nax2.set_ylabel("Count")\nax2.legend(loc="upper center", ncol=2)\n\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html
Compare cross decomposition methods	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_compare_cross_decomposition_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_decomposition import PLSCanonical, PLSRegression, CCA\n\n###############################################################################\n# Dataset based latent variables model\n\nn = 500\n# 2 latents vars:\nl1 = np.random.normal(size=n)\nl2 = np.random.normal(size=n)\n\nlatents = np.array([l1, l1, l2, l2]).T\nX = latents + np.random.normal(size=4 * n).reshape((n, 4))\nY = latents + np.random.normal(size=4 * n).reshape((n, 4))\n\nX_train = X[:n / 2]\nY_train = Y[:n / 2]\nX_test = X[n / 2:]\nY_test = Y[n / 2:]\n\nprint("Corr(X)")\nprint(np.round(np.corrcoef(X.T), 2))\nprint("Corr(Y)")\nprint(np.round(np.corrcoef(Y.T), 2))\n\n###############################################################################\n# Canonical (symmetric) PLS\n\n# Transform data\n# ~~~~~~~~~~~~~~\nplsca = PLSCanonical(n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)\n\n# Scatter plot of scores\n# ~~~~~~~~~~~~~~~~~~~~~~\n# 1) On diagonal plot X vs Y scores on each components\nplt.figure(figsize=(12, 8))\nplt.subplot(221)\nplt.plot(X_train_r[:, 0], Y_train_r[:, 0], "ob", label="train")\nplt.plot(X_test_r[:, 0], Y_test_r[:, 0], "or", label="test")\nplt.xlabel("x scores")\nplt.ylabel("y scores")\nplt.title('Comp. 1: X vs Y (test corr = %.2f)' %\n          np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc="best")\n\nplt.subplot(224)\nplt.plot(X_train_r[:, 1], Y_train_r[:, 1], "ob", label="train")\nplt.plot(X_test_r[:, 1], Y_test_r[:, 1], "or", label="test")\nplt.xlabel("x scores")\nplt.ylabel("y scores")\nplt.title('Comp. 2: X vs Y (test corr = %.2f)' %\n          np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc="best")\n\n# 2) Off diagonal plot components 1 vs 2 for X and Y\nplt.subplot(222)\nplt.plot(X_train_r[:, 0], X_train_r[:, 1], "*b", label="train")\nplt.plot(X_test_r[:, 0], X_test_r[:, 1], "*r", label="test")\nplt.xlabel("X comp. 1")\nplt.ylabel("X comp. 2")\nplt.title('X comp. 1 vs X comp. 2 (test corr = %.2f)'\n          % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1])\nplt.legend(loc="best")\nplt.xticks(())\nplt.yticks(())\n\nplt.subplot(223)\nplt.plot(Y_train_r[:, 0], Y_train_r[:, 1], "*b", label="train")\nplt.plot(Y_test_r[:, 0], Y_test_r[:, 1], "*r", label="test")\nplt.xlabel("Y comp. 1")\nplt.ylabel("Y comp. 2")\nplt.title('Y comp. 1 vs Y comp. 2 , (test corr = %.2f)'\n          % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1])\nplt.legend(loc="best")\nplt.xticks(())\nplt.yticks(())\nplt.show()\n\n###############################################################################\n# PLS regression, with multivariate response, a.k.a. PLS2\n\nn = 1000\nq = 3\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\nB = np.array([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5\n\npls2 = PLSRegression(n_components=3)\npls2.fit(X, Y)\nprint("True B (such that: Y = XB + Err)")\nprint(B)\n# compare pls2.coef_ with B\nprint("Estimated B")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)\n\n###############################################################################\n# PLS regression, with univariate response, a.k.a. PLS1\n\nn = 1000\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5\npls1 = PLSRegression(n_components=3)\npls1.fit(X, y)\n# note that the number of compements exceeds 1 (the dimension of y)\nprint("Estimated betas")\nprint(np.round(pls1.coef_, 1))\n\n###############################################################################\n# CCA (PLS mode B with symmetric deflation)\n\ncca = CCA(n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)</code></pre>	http://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html
Comparison of Manifold Learning methods	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_compare_methods_001.png]]	<br><pre><code># Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>\n\nprint(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold, datasets\n\n# Next line to silence pyflakes. This import is needed.\nAxes3D\n\nn_points = 1000\nX, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\nn_neighbors = 10\nn_components = 2\n\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle("Manifold Learning with %i points, %i neighbors"\n             % (1000, n_neighbors), fontsize=14)\n\ntry:\n    # compatibility matplotlib < 1.0\n    ax = fig.add_subplot(251, projection='3d')\n    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n    ax.view_init(4, -72)\nexcept:\n    ax = fig.add_subplot(251, projection='3d')\n    plt.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)\n\nmethods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,\n                                        eigen_solver='auto',\n                                        method=method).fit_transform(X)\n    t1 = time()\n    print("%s: %.2g sec" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n    plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n\nt0 = time()\nY = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\nt1 = time()\nprint("Isomap: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(257)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("Isomap (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nY = mds.fit_transform(X)\nt1 = time()\nprint("MDS: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(258)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("MDS (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nY = se.fit_transform(X)\nt1 = time()\nprint("SpectralEmbedding: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(259)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("SpectralEmbedding (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nY = tsne.fit_transform(X)\nt1 = time()\nprint("t-SNE: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("t-SNE (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html
Confusion matrix	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into a training set and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Run classifier, using a model that is too regularized (C too low) to see\n# the impact on the results\nclassifier = svm.SVC(kernel='linear', C=0.01)\ny_pred = classifier.fit(X_train, y_train).predict(X_test)\n\n\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(iris.target_names))\n    plt.xticks(tick_marks, iris.target_names, rotation=45)\n    plt.yticks(tick_marks, iris.target_names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\n\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.figure()\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_covariance_estimation_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\n    log_likelihood, empirical_covariance\nfrom sklearn.grid_search import GridSearchCV\n\n\n###############################################################################\n# Generate sample data\nn_features, n_samples = 40, 20\nnp.random.seed(42)\nbase_X_train = np.random.normal(size=(n_samples, n_features))\nbase_X_test = np.random.normal(size=(n_samples, n_features))\n\n# Color samples\ncoloring_matrix = np.random.normal(size=(n_features, n_features))\nX_train = np.dot(base_X_train, coloring_matrix)\nX_test = np.dot(base_X_test, coloring_matrix)\n\n###############################################################################\n# Compute the likelihood on test data\n\n# spanning a range of possible shrinkage coefficient values\nshrinkages = np.logspace(-2, 0, 30)\nnegative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n                    for s in shrinkages]\n\n# under the ground-truth model, which we would not have access to in real\n# settings\nreal_cov = np.dot(coloring_matrix.T, coloring_matrix)\nemp_cov = empirical_covariance(X_train)\nloglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n\n###############################################################################\n# Compare different approaches to setting the parameter\n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{'shrinkage': shrinkages}]\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = LedoitWolf()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = OAS()\nloglik_oa = oa.fit(X_train).score(X_test)\n\n###############################################################################\n# Plot results\nfig = plt.figure()\nplt.title("Regularized covariance: likelihood and shrinkage coefficient")\nplt.xlabel('Regularizaton parameter: shrinkage coefficient')\nplt.ylabel('Error: negative log-likelihood on test data')\n# range shrinkage curve\nplt.loglog(shrinkages, negative_logliks, label="Negative log-likelihood")\n\nplt.plot(plt.xlim(), 2 * [loglik_real], '--r',\n         label="Real covariance likelihood")\n\n# adjust view\nlik_max = np.amax(negative_logliks)\nlik_min = np.amin(negative_logliks)\nymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\nymax = lik_max + 10. * np.log(lik_max - lik_min)\nxmin = shrinkages[0]\nxmax = shrinkages[-1]\n# LW likelihood\nplt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',\n           linewidth=3, label='Ledoit-Wolf estimate')\n# OAS likelihood\nplt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',\n           linewidth=3, label='OAS estimate')\n# best CV estimator likelihood\nplt.vlines(cv.best_estimator_.shrinkage, ymin,\n           -cv.best_estimator_.score(X_test), color='cyan',\n           linewidth=3, label='Cross-validation best estimate')\n\nplt.ylim(ymin, ymax)\nplt.xlim(xmin, xmax)\nplt.legend()\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html
SVM with custom kernel	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_custom_kernel_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\nY = iris.target\n\n\ndef my_kernel(X, Y):\n    """\n    We create a custom kernel:\n\n                 (2  0)\n    k(X, Y) = X  (    ) Y.T\n                 (0  1)\n    """\n    M = np.array([[2, 0], [0, 1.0]])\n    return np.dot(np.dot(X, M), Y.T)\n\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data.\nclf = svm.SVC(kernel=my_kernel)\nclf.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\nplt.title('3-Class classification using Support Vector Machine with custom'\n          ' kernel')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html
Cross-validation on diabetes Dataset Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_cv_diabetes_001.png]]	<br><pre><code>from __future__ import print_function\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cross_validation, datasets, linear_model\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:150]\ny = diabetes.target[:150]\n\nlasso = linear_model.Lasso()\nalphas = np.logspace(-4, -.5, 30)\n\nscores = list()\nscores_std = list()\n\nfor alpha in alphas:\n    lasso.alpha = alpha\n    this_scores = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\nplt.figure(figsize=(4, 3))\nplt.semilogx(alphas, scores)\n# plot error lines showing +/- std. errors of the scores\nplt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),\n             'b--')\nplt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),\n             'b--')\nplt.ylabel('CV score')\nplt.xlabel('alpha')\nplt.axhline(np.max(scores), linestyle='--', color='.5')\n\n##############################################################################\n# Bonus: how much can you trust the selection of alpha?\n\n# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\nlasso_cv = linear_model.LassoCV(alphas=alphas)\nk_fold = cross_validation.KFold(len(X), 3)\n\nprint("Answer to the bonus question:",\n      "how much can you trust the selection of alpha?")\nprint()\nprint("Alpha parameters maximising the generalization score on different")\nprint("subsets of the data:")\nfor k, (train, test) in enumerate(k_fold):\n    lasso_cv.fit(X[train], y[train])\n    print("[fold {0}] alpha: {1:.5f}, score: {2:.5f}".\n          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))\nprint()\nprint("Answer: Not very much since we obtained different alphas for different")\nprint("subsets of the data and moreover, the scores for these alphas differ")\nprint("quite substantially.")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html
Cross-validation on Digits Dataset Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_cv_digits_001.png]]	<br><pre><code>print(__doc__)\n\n\nimport numpy as np\nfrom sklearn import cross_validation, datasets, svm\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nsvc = svm.SVC(kernel='linear')\nC_s = np.logspace(-10, 0, 10)\n\nscores = list()\nscores_std = list()\nfor C in C_s:\n    svc.C = C\n    this_scores = cross_validation.cross_val_score(svc, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\n# Do the plotting\nimport matplotlib.pyplot as plt\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.semilogx(C_s, scores)\nplt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\nplt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\nlocs, labels = plt.yticks()\nplt.yticks(locs, list(map(lambda x: "%g" % x, locs)))\nplt.ylabel('CV score')\nplt.xlabel('Parameter C')\nplt.ylim(0, 1.1)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html
Plotting Cross-Validated Predictions	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_cv_predict_001.png]]	<br><pre><code>from sklearn import datasets\nfrom sklearn.cross_validation import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nlr = linear_model.LinearRegression()\nboston = datasets.load_boston()\ny = boston.target\n\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validated:\npredicted = cross_val_predict(lr, boston.data, y, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html
Demo of DBSCAN clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_dbscan_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n\n##############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n                            random_state=0)\n\nX = StandardScaler().fit_transform(X)\n\n##############################################################################\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\nprint("Adjusted Rand Index: %0.3f"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint("Adjusted Mutual Information: %0.3f"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, labels))\n\n##############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = 'k'\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html
Online learning of a dictionary of parts of faces	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_dict_face_patches_001.png]]	<br><pre><code>print(__doc__)\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nfrom sklearn import datasets\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.image import extract_patches_2d\n\nfaces = datasets.fetch_olivetti_faces()\n\n###############################################################################\n# Learn the dictionary of images\n\nprint('Learning the dictionary... ')\nrng = np.random.RandomState(0)\nkmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True)\npatch_size = (20, 20)\n\nbuffer = []\nindex = 1\nt0 = time.time()\n\n# The online learning part: cycle over the whole dataset 6 times\nindex = 0\nfor _ in range(6):\n    for img in faces.images:\n        data = extract_patches_2d(img, patch_size, max_patches=50,\n                                  random_state=rng)\n        data = np.reshape(data, (len(data), -1))\n        buffer.append(data)\n        index += 1\n        if index % 10 == 0:\n            data = np.concatenate(buffer, axis=0)\n            data -= np.mean(data, axis=0)\n            data /= np.std(data, axis=0)\n            kmeans.partial_fit(data)\n            buffer = []\n        if index % 100 == 0:\n            print('Partial fit of %4i out of %i'\n                  % (index, 6 * len(faces.images)))\n\ndt = time.time() - t0\nprint('done in %.2fs.' % dt)\n\n###############################################################################\n# Plot the results\nplt.figure(figsize=(4.2, 4))\nfor i, patch in enumerate(kmeans.cluster_centers_):\n    plt.subplot(9, 9, i + 1)\n    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n\n\nplt.suptitle('Patches of faces\nTrain time %.1fs on %d patches' %\n             (dt, 8 * len(faces.images)), fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html
Feature agglomeration	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_digits_agglomeration_001.png]]	<br><pre><code>print(__doc__)\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, cluster\nfrom sklearn.feature_extraction.image import grid_to_graph\n\ndigits = datasets.load_digits()\nimages = digits.images\nX = np.reshape(images, (len(images), -1))\nconnectivity = grid_to_graph(*images[0].shape)\n\nagglo = cluster.FeatureAgglomeration(connectivity=connectivity,\n                                     n_clusters=32)\n\nagglo.fit(X)\nX_reduced = agglo.transform(X)\n\nX_restored = agglo.inverse_transform(X_reduced)\nimages_restored = np.reshape(X_restored, images.shape)\nplt.figure(1, figsize=(4, 3.5))\nplt.clf()\nplt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.91)\nfor i in range(4):\n    plt.subplot(3, 4, i + 1)\n    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    if i == 1:\n        plt.title('Original data')\n    plt.subplot(3, 4, 4 + i + 1)\n    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16,\n               interpolation='nearest')\n    if i == 1:\n        plt.title('Agglomerated data')\n    plt.xticks(())\n    plt.yticks(())\n\nplt.subplot(3, 4, 10)\nplt.imshow(np.reshape(agglo.labels_, images[0].shape),\n           interpolation='nearest', cmap=plt.cm.spectral)\nplt.xticks(())\nplt.yticks(())\nplt.title('Labels')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_agglomeration.html
Recognizing hand-written digits	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_digits_classification_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, metrics\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n# The data that we are interested in is made of 8x8 images of digits, let's\n# have a look at the first 3 images, stored in the `images` attribute of the\n# dataset.  If we were working from image files, we could load them using\n# pylab.imread.  Note that each image must have the same size. For these\n# images, we know which digit they represent: it is given in the 'target' of\n# the dataset.\nimages_and_labels = list(zip(digits.images, digits.target))\nfor index, (image, label) in enumerate(images_and_labels[:4]):\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])\n\n# Now predict the value of the digit on the second half:\nexpected = digits.target[n_samples / 2:]\npredicted = classifier.predict(data[n_samples / 2:])\n\nprint("Classification report for classifier %s:\n%s\n"\n      % (classifier, metrics.classification_report(expected, predicted)))\nprint("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))\n\nimages_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))\nfor index, (image, prediction) in enumerate(images_and_predictions[:4]):\n    plt.subplot(2, 4, index + 5)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Prediction: %i' % prediction)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html
Kernel Density Estimation	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_digits_kde_sampling_001.png]]	<br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.decomposition import PCA\nfrom sklearn.grid_search import GridSearchCV\n\n# load the data\ndigits = load_digits()\ndata = digits.data\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {'bandwidth': np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n                             cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n                                 cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n\nax[0, 5].set_title('Selection from the input data')\nax[5, 5].set_title('"New" digits drawn from the kernel density model')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_digits_kde_sampling.html
The Digit Dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_digits_last_image_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nimport matplotlib.pyplot as plt\n\n#Load the digits dataset\ndigits = datasets.load_digits()\n\n#Display the first digit\nplt.figure(1, figsize=(3, 3))\nplt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html
Various Agglomerative Clustering on a 2D embedding of digits	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2014\n\nprint(__doc__)\nfrom time import time\n\nimport numpy as np\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import manifold, datasets\n\ndigits = datasets.load_digits(n_class=10)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\n\nnp.random.seed(0)\n\ndef nudge_images(X, y):\n    # Having a larger dataset shows more clearly the behavior of the\n    # methods, but we multiply the size of the dataset only by 2, as the\n    # cost of the hierarchical clustering methods are strongly\n    # super-linear in n_samples\n    shift = lambda x: ndimage.shift(x.reshape((8, 8)),\n                                  .3 * np.random.normal(size=2),\n                                  mode='constant',\n                                  ).ravel()\n    X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])\n    Y = np.concatenate([y, y], axis=0)\n    return X, Y\n\n\nX, y = nudge_images(X, y)\n\n\n#----------------------------------------------------------------------\n# Visualize the clustering\ndef plot_clustering(X_red, X, labels, title=None):\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n    X_red = (X_red - x_min) / (x_max - x_min)\n\n    plt.figure(figsize=(6, 4))\n    for i in range(X_red.shape[0]):\n        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),\n                 color=plt.cm.spectral(labels[i] / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    plt.xticks([])\n    plt.yticks([])\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis('off')\n    plt.tight_layout()\n\n#----------------------------------------------------------------------\n# 2D embedding of the digits dataset\nprint("Computing embedding")\nX_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\nprint("Done.")\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nfor linkage in ('ward', 'average', 'complete'):\n    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)\n    t0 = time()\n    clustering.fit(X_red)\n    print("%s : %.2fs" % (linkage, time() - t0))\n\n    plot_clustering(X_red, X, clustering.labels_, "%s linkage" % linkage)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html
Pipelining: chaining a PCA and a logistic regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_digits_pipe_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\nlogistic = linear_model.LogisticRegression()\n\npca = decomposition.PCA()\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n###############################################################################\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_')\n\n###############################################################################\n# Prediction\n\nn_components = [20, 40, 64]\nCs = np.logspace(-4, 4, 3)\n\n#Parameters of pipelines can be set using ‘__’ separated parameter names:\n\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_components,\n                              logistic__C=Cs))\nestimator.fit(X_digits, y_digits)\n\nplt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n            linestyle=':', label='n_components chosen')\nplt.legend(prop=dict(size=12))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html
OOB Errors for Random Forests	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ensemble_oob_001.png]]	<br><pre><code>import matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n# Author: Kian Ho <hui.kian.ho@gmail.com>\n#         Gilles Louppe <g.louppe@gmail.com>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 Clause\n\nprint(__doc__)\n\nRANDOM_STATE = 123\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=500, n_features=25,\n                           n_clusters_per_class=1, n_informative=15,\n                           random_state=RANDOM_STATE)\n\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\n# support for paralellised ensembles but is necessary for tracking the OOB\n# error trajectory during training.\nensemble_clfs = [\n    ("RandomForestClassifier, max_features='sqrt'",\n        RandomForestClassifier(warm_start=True, oob_score=True,\n                               max_features="sqrt",\n                               random_state=RANDOM_STATE)),\n    ("RandomForestClassifier, max_features='log2'",\n        RandomForestClassifier(warm_start=True, max_features='log2',\n                               oob_score=True,\n                               random_state=RANDOM_STATE)),\n    ("RandomForestClassifier, max_features=None",\n        RandomForestClassifier(warm_start=True, max_features=None,\n                               oob_score=True,\n                               random_state=RANDOM_STATE))\n]\n\n# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n# Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 175\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n# Generate the "OOB error rate" vs. "n_estimators" plot.\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel("n_estimators")\nplt.ylabel("OOB error rate")\nplt.legend(loc="upper right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html
Vector Quantization Example	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\nn_clusters = 5\nnp.random.seed(0)\n    \nX = face.reshape((-1, 1))  # We need an (n_sample, n_feature) array\nk_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\nk_means.fit(X)\nvalues = k_means.cluster_centers_.squeeze()\nlabels = k_means.labels_\n\n# create an array from labels and values\nface_compressed = np.choose(labels, values)\nface_compressed.shape = face.shape\n\nvmin = face.min()\nvmax = face.max()\n\n# original face\nplt.figure(1, figsize=(3, 2.2))\nplt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n\n# compressed face\nplt.figure(2, figsize=(3, 2.2))\nplt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# equal bins face\nregular_values = np.linspace(0, 256, n_clusters + 1)\nregular_labels = np.searchsorted(regular_values, face) - 1\nregular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\nregular_face = np.choose(regular_labels.ravel(), regular_values, mode="clip")\nregular_face.shape = face.shape\nplt.figure(3, figsize=(3, 2.2))\nplt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# histogram\nplt.figure(4, figsize=(3, 2.2))\nplt.clf()\nplt.axes([.01, .01, .98, .98])\nplt.hist(X, bins=256, color='.5', edgecolor='.5')\nplt.yticks(())\nplt.xticks(regular_values)\nvalues = np.sort(values)\nfor center_1, center_2 in zip(values[:-1], values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color='b')\n\nfor center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html
Segmenting the picture of a raccoon face in regions	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n\n# load the raccoon face as a numpy array\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\n# Resize it to 10% of the original size to speed up the processing\nface = sp.misc.imresize(face, 0.10) / 255.\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(face)\n\n# Take a decreasing function of the gradient: an exponential\n# The smaller beta is, the more independent the segmentation is of the\n# actual image. For beta=1, the segmentation is close to a voronoi\nbeta = 5\neps = 1e-6\ngraph.data = np.exp(-beta * graph.data / graph.data.std()) + eps\n\n# Apply spectral clustering (this step goes much faster if you have pyamg\n# installed)\nN_REGIONS = 25\n\n#############################################################################\n# Visualize the resulting regions\n\nfor assign_labels in ('kmeans', 'discretize'):\n    t0 = time.time()\n    labels = spectral_clustering(graph, n_clusters=N_REGIONS,\n                                 assign_labels=assign_labels, random_state=1)\n    t1 = time.time()\n    labels = labels.reshape(face.shape)\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(face, cmap=plt.cm.gray)\n    for l in range(N_REGIONS):\n        plt.contour(labels == l, contours=1,\n                    colors=[plt.cm.spectral(l / float(N_REGIONS))])\n    plt.xticks(())\n    plt.yticks(())\n    title = 'Spectral clustering: %s, %.2fs' % (assign_labels, (t1 - t0))\n    print(title)\n    plt.title(title)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_segmentation.html
A demo of structured Ward hierarchical clustering on a raccoon face image	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_face_ward_segmentation_001.png]]	<br><pre><code># Author : Vincent Michel, 2010\n#          Alexandre Gramfort, 2011\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\n\nimport numpy as np\nimport scipy as sp\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n\n###############################################################################\n# Generate data\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\n# Resize it to 10% of the original size to speed up the processing\nface = sp.misc.imresize(face, 0.10) / 255.\n\nX = np.reshape(face, (-1, 1))\n\n###############################################################################\n# Define the structure A of the data. Pixels connected to their neighbors.\nconnectivity = grid_to_graph(*face.shape)\n\n###############################################################################\n# Compute clustering\nprint("Compute structured hierarchical clustering...")\nst = time.time()\nn_clusters = 15  # number of regions\nward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n                               connectivity=connectivity)\nward.fit(X)\nlabel = np.reshape(ward.labels_, face.shape)\nprint("Elapsed time: ", time.time() - st)\nprint("Number of pixels: ", label.size)\nprint("Number of clusters: ", np.unique(label).size)\n\n###############################################################################\n# Plot the results on an image\nplt.figure(figsize=(5, 5))\nplt.imshow(face, cmap=plt.cm.gray)\nfor l in range(n_clusters):\n    plt.contour(label == l, contours=1,\n                colors=[plt.cm.spectral(l / float(n_clusters)), ])\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_ward_segmentation.html
Faces dataset decompositions	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Authors: Vlad Niculae, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport logging\nfrom time import time\n\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn import decomposition\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\nn_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\nrng = RandomState(0)\n\n###############################################################################\n# Load faces data\ndataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\nfaces = dataset.data\n\nn_samples, n_features = faces.shape\n\n# global centering\nfaces_centered = faces - faces.mean(axis=0)\n\n# local centering\nfaces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n\nprint("Dataset consists of %d faces" % n_samples)\n\n\n###############################################################################\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row):\n    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n    plt.suptitle(title, size=16)\n    for i, comp in enumerate(images):\n        plt.subplot(n_row, n_col, i + 1)\n        vmax = max(comp.max(), -comp.min())\n        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n                   interpolation='nearest',\n                   vmin=-vmax, vmax=vmax)\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n\n###############################################################################\n# List of the different estimators, whether to center and transpose the\n# problem, and whether the transformer uses the clustering API.\nestimators = [\n    ('Eigenfaces - RandomizedPCA',\n     decomposition.RandomizedPCA(n_components=n_components, whiten=True),\n     True),\n\n    ('Non-negative components - NMF',\n     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n     False),\n\n    ('Independent components - FastICA',\n     decomposition.FastICA(n_components=n_components, whiten=True),\n     True),\n\n    ('Sparse comp. - MiniBatchSparsePCA',\n     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n                                      n_iter=100, batch_size=3,\n                                      random_state=rng),\n     True),\n\n    ('MiniBatchDictionaryLearning',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  random_state=rng),\n     True),\n\n    ('Cluster centers - MiniBatchKMeans',\n        MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n                        max_iter=50, random_state=rng),\n     True),\n\n    ('Factor Analysis components - FA',\n     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n     True),\n]\n\n\n###############################################################################\n# Plot a sample of the input data\n\nplot_gallery("First centered Olivetti faces", faces_centered[:n_components])\n\n###############################################################################\n# Do the estimation and plot it\n\nfor name, estimator, center in estimators:\n    print("Extracting the top %d %s..." % (n_components, name))\n    t0 = time()\n    data = faces\n    if center:\n        data = faces_centered\n    estimator.fit(data)\n    train_time = (time() - t0)\n    print("done in %0.3fs" % train_time)\n    if hasattr(estimator, 'cluster_centers_'):\n        components_ = estimator.cluster_centers_\n    else:\n        components_ = estimator.components_\n    if hasattr(estimator, 'noise_variance_'):\n        plot_gallery("Pixelwise variance",\n                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n                     n_row=1)\n    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n                 components_[:n_components])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html
Feature agglomeration vs. univariate selection	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_feature_agglomeration_vs_univariate_selection_001.png]]	<br><pre><code># Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport shutil\nimport tempfile\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg, ndimage\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn import feature_selection\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.cross_validation import KFold\n\n###############################################################################\n# Generate data\nn_samples = 200\nsize = 40  # image size\nroi_size = 15\nsnr = 5.\nnp.random.seed(0)\nmask = np.ones([size, size], dtype=np.bool)\n\ncoef = np.zeros((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.\ncoef[-roi_size:, -roi_size:] = 1.\n\nX = np.random.randn(n_samples, size ** 2)\nfor x in X:  # smooth data\n    x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = np.dot(X, coef.ravel())\nnoise = np.random.randn(y.shape[0])\nnoise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.)) / linalg.norm(noise, 2)\ny += noise_coef * noise  # add noise\n\n###############################################################################\n# Compute the coefs of a Bayesian Ridge with GridSearch\ncv = KFold(len(y), 2)  # cross-validation generator for model selection\nridge = BayesianRidge()\ncachedir = tempfile.mkdtemp()\nmem = Memory(cachedir=cachedir, verbose=1)\n\n# Ward agglomeration followed by BayesianRidge\nconnectivity = grid_to_graph(n_x=size, n_y=size)\nward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity,\n                            memory=mem)\nclf = Pipeline([('ward', ward), ('ridge', ridge)])\n# Select the optimal number of parcels with grid search\nclf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)\ncoef_agglomeration_ = coef_.reshape(size, size)\n\n# Anova univariate feature selection followed by BayesianRidge\nf_regression = mem.cache(feature_selection.f_regression)  # caching function\nanova = feature_selection.SelectPercentile(f_regression)\nclf = Pipeline([('anova', anova), ('ridge', ridge)])\n# Select the optimal percentage of features with grid search\nclf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))\ncoef_selection_ = coef_.reshape(size, size)\n\n###############################################################################\n# Inverse the transformation to plot the results on an image\nplt.close('all')\nplt.figure(figsize=(7.3, 2.7))\nplt.subplot(1, 3, 1)\nplt.imshow(coef, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("True weights")\nplt.subplot(1, 3, 2)\nplt.imshow(coef_selection_, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("Feature Selection")\nplt.subplot(1, 3, 3)\nplt.imshow(coef_agglomeration_, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("Feature Agglomeration")\nplt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)\nplt.show()\n\n# Attempt to remove the temporary cachedir, but don't worry if it fails\nshutil.rmtree(cachedir, ignore_errors=True)</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html
Univariate Feature Selection	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_feature_selection_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, svm\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\n###############################################################################\n# import some data to play with\n\n# The iris dataset\niris = datasets.load_iris()\n\n# Some noisy data not correlated\nE = np.random.uniform(0, 0.1, size=(len(iris.data), 20))\n\n# Add the noisy data to the informative features\nX = np.hstack((iris.data, E))\ny = iris.target\n\n###############################################################################\nplt.figure(1)\nplt.clf()\n\nX_indices = np.arange(X.shape[-1])\n\n###############################################################################\n# Univariate feature selection with F-test for feature scoring\n# We use the default selection function: the 10% most significant features\nselector = SelectPercentile(f_classif, percentile=10)\nselector.fit(X, y)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\nplt.bar(X_indices - .45, scores, width=.2,\n        label=r'Univariate score ($-Log(p_{value})$)', color='g')\n\n###############################################################################\n# Compare to the weights of an SVM\nclf = svm.SVC(kernel='linear')\nclf.fit(X, y)\n\nsvm_weights = (clf.coef_ ** 2).sum(axis=0)\nsvm_weights /= svm_weights.max()\n\nplt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')\n\nclf_selected = svm.SVC(kernel='linear')\nclf_selected.fit(selector.transform(X), y)\n\nsvm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\nsvm_weights_selected /= svm_weights_selected.max()\n\nplt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label='SVM weights after selection', color='b')\n\n\nplt.title("Comparing feature selection")\nplt.xlabel('Feature number')\nplt.yticks(())\nplt.axis('tight')\nplt.legend(loc='upper right')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html
Feature transformations with ensembles of trees	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Tim Head <betatim@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nnp.random.seed(10)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.pipeline import make_pipeline\n\nn_estimator = 10\nX, y = make_classification(n_samples=80000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n# It is important to train the ensemble of trees on a different subset\n# of the training data than the linear regression model to avoid\n# overfitting, in particular if the total number of leaves is\n# similar to the number of training samples\nX_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,\n                                                            y_train,\n                                                            test_size=0.5)\n\n# Unsupervised transformation based on totally random trees\nrt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n	random_state=0)\n\nrt_lm = LogisticRegression()\npipeline = make_pipeline(rt, rt_lm)\npipeline.fit(X_train, y_train)\ny_pred_rt = pipeline.predict_proba(X_test)[:, 1]\nfpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n\n# Supervised transformation based on random forests\nrf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\nrf_enc = OneHotEncoder()\nrf_lm = LogisticRegression()\nrf.fit(X_train, y_train)\nrf_enc.fit(rf.apply(X_train))\nrf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n\ny_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\nfpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n\ngrd = GradientBoostingClassifier(n_estimators=n_estimator)\ngrd_enc = OneHotEncoder()\ngrd_lm = LogisticRegression()\ngrd.fit(X_train, y_train)\ngrd_enc.fit(grd.apply(X_train)[:, :, 0])\ngrd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n\ny_pred_grd_lm = grd_lm.predict_proba(\n    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\nfpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n\n\n# The gradient boosted model by itself\ny_pred_grd = grd.predict_proba(X_test)[:, 1]\nfpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n\n\n# The random forest model by itself\ny_pred_rf = rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\nplt.plot(fpr_rf, tpr_rf, label='RF')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\nplt.plot(fpr_grd, tpr_grd, label='GBT')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n\nplt.figure(2)\nplt.xlim(0, 0.2)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\nplt.plot(fpr_rf, tpr_rf, label='RF')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\nplt.plot(fpr_grd, tpr_grd, label='GBT')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend(loc='best')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html
Feature importances with forests of trees	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_forest_importances_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000,\n                           n_features=10,\n                           n_informative=3,\n                           n_redundant=0,\n                           n_repeated=0,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint("Feature ranking:")\n\nfor f in range(X.shape[1]):\n    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title("Feature importances")\nplt.bar(range(X.shape[1]), importances[indices],\n       color="r", yerr=std[indices], align="center")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
Pixel importances with a parallel forest of trees	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_forest_importances_faces_001.png]]	<br><pre><code>print(__doc__)\n\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Number of cores to use to perform parallel fitting of the forest model\nn_jobs = 1\n\n# Load the faces dataset\ndata = fetch_olivetti_faces()\nX = data.images.reshape((len(data.images), -1))\ny = data.target\n\nmask = y < 5  # Limit to 5 classes\nX = X[mask]\ny = y[mask]\n\n# Build a forest and compute the pixel importances\nprint("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs)\nt0 = time()\nforest = ExtraTreesClassifier(n_estimators=1000,\n                              max_features=128,\n                              n_jobs=n_jobs,\n                              random_state=0)\n\nforest.fit(X, y)\nprint("done in %0.3fs" % (time() - t0))\nimportances = forest.feature_importances_\nimportances = importances.reshape(data.images[0].shape)\n\n# Plot pixel importances\nplt.matshow(importances, cmap=plt.cm.hot)\nplt.title("Pixel importances with forests of trees")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html
Plot the decision surfaces of ensembles of trees on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_forest_iris_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import clone\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nn_estimators = 30\nplot_colors = "ryb"\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = load_iris()\n\nplot_idx = 1\n\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n                             n_estimators=n_estimators)]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = np.arange(X.shape[0])\n        np.random.seed(RANDOM_SEED)\n        np.random.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) / std\n\n        # Train\n        clf = clone(model)\n        clf = model.fit(X, y)\n\n        scores = clf.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(".")[-1][:-2][:-len("Classifier")]\n        model_details = model_title\n        if hasattr(model, "estimators_"):\n            model_details += " with {} estimators".format(len(model.estimators_))\n        print( model_details + " with features", pair, "has a score of", scores )\n\n        plt.subplot(3, 4, plot_idx)\n        if plot_idx <= len(models):\n            # Add a title at the top of each column\n            plt.title(model_title)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                             np.arange(y_min, y_max, plot_step))\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, DecisionTreeClassifier):\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 / len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a black outline\n        xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),\n                                             np.arange(y_min, y_max, plot_step_coarser))\n        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)\n        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors="none")\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        for i, c in zip(xrange(n_classes), plot_colors):\n            idx = np.where(y == i)\n            plt.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n                        cmap=cmap)\n\n        plot_idx += 1  # move on to the next plot in sequence\n\nplt.suptitle("Classifiers on feature subsets of the Iris dataset")\nplt.axis("tight")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html
Using FunctionTransformer to select columns	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n\ndef _generate_vector(shift=0.5, noise=15):\n    return np.arange(1000) + (np.random.rand(1000) - shift) * noise\n\n\ndef generate_dataset():\n    """\n    This dataset is two lines with a slope ~ 1, where one has\n    a y offset of ~100\n    """\n    return np.vstack((\n        np.vstack((\n            _generate_vector(),\n            _generate_vector() + 100,\n        )).T,\n        np.vstack((\n            _generate_vector(),\n            _generate_vector(),\n        )).T,\n    )), np.hstack((np.zeros(1000), np.ones(1000)))\n\n\ndef all_but_first_column(X):\n    return X[:, 1:]\n\n\ndef drop_first_component(X, y):\n    """\n    Create a pipeline with PCA and the column selector and use it to\n    transform the dataset.\n    """\n    pipeline = make_pipeline(\n        PCA(), FunctionTransformer(all_but_first_column),\n    )\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    pipeline.fit(X_train, y_train)\n    return pipeline.transform(X_test), y_test\n\n\nif __name__ == '__main__':\n    X, y = generate_dataset()\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n    plt.show()\n    X_transformed, y_transformed = drop_first_component(*generate_dataset())\n    plt.scatter(\n        X_transformed[:, 0],\n        np.zeros(len(X_transformed)),\n        c=y_transformed,\n        s=50,\n    )\n    plt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/preprocessing/plot_function_transformer.html
Gaussian Mixture Model Ellipsoids	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gmm_001.png]]	<br><pre><code>import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\n# Fit a mixture of Gaussians with EM using five components\ngmm = mixture.GMM(n_components=5, covariance_type='full')\ngmm.fit(X)\n\n# Fit a Dirichlet process mixture of Gaussians using five components\ndpgmm = mixture.DPGMM(n_components=5, covariance_type='full')\ndpgmm.fit(X)\n\ncolor_iter = itertools.cycle(['r', 'g', 'b', 'c', 'm'])\n\nfor i, (clf, title) in enumerate([(gmm, 'GMM'),\n                                  (dpgmm, 'Dirichlet Process GMM')]):\n    splot = plt.subplot(2, 1, 1 + i)\n    Y_ = clf.predict(X)\n    for i, (mean, covar, color) in enumerate(zip(\n            clf.means_, clf._get_covars(), color_iter)):\n        v, w = linalg.eigh(covar)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-10, 10)\n    plt.ylim(-3, 6)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(title)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html
GMM classification	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gmm_classifier_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux\n# License: BSD 3 clause\n\n# $Id$\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.mixture import GMM\n\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate('rgb'):\n        v, w = np.linalg.eigh(gmm._get_covars()[n][:2, :2])\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v *= 9\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%) and testing\n# (25%) sets.\nskf = StratifiedKFold(iris.target, n_folds=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nclassifiers = dict((covar_type, GMM(n_components=n_classes,\n                    covariance_type=covar_type, init_params='wc', n_iter=20))\n                   for covar_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * n_classifiers / 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    classifier.means_ = np.array([X_train[y_train == i].mean(axis=0)\n                                  for i in xrange(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    classifier.fit(X_train)\n\n    h = plt.subplot(2, n_classifiers / 2, index + 1)\n    make_ellipses(classifier, h)\n\n    for n, color in enumerate('rgb'):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], 0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate('rgb'):\n        data = X_test[y_test == n]\n        plt.plot(data[:, 0], data[:, 1], 'x', color=color)\n\n    y_train_pred = classifier.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = classifier.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(loc='lower right', prop=dict(size=12))\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_classifier.html
Density Estimation for a mixture of Gaussians	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gmm_pdf_001.png]]	<br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\nnp.random.seed(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = np.array([[0., -0.7], [3.5, .7]])\nstretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = np.vstack([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = mixture.GMM(n_components=2, covariance_type='full')\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = np.linspace(-20.0, 30.0)\ny = np.linspace(-20.0, 40.0)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)[0]\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n                 levels=np.logspace(0, 3, 10))\nCB = plt.colorbar(CS, shrink=0.8, extend='both')\nplt.scatter(X_train[:, 0], X_train[:, 1], .8)\n\nplt.title('Negative log-likelihood predicted by a GMM')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html
Gaussian Mixture Model Selection	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gmm_selection_001.png]]	<br><pre><code>print(__doc__)\n\nimport itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = mixture.GMM(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.bic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic)\ncolor_iter = itertools.cycle(['k', 'r', 'g', 'b', 'c', 'm', 'y'])\nclf = best_gmm\nbars = []\n\n# Plot the BIC scores\nspl = plt.subplot(2, 1, 1)\nfor i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n    xpos = np.array(n_components_range) + .2 * (i - 2)\n    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\nplt.xticks(n_components_range)\nplt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\nplt.title('BIC score per model')\nxpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\n    .2 * np.floor(bic.argmin() / len(n_components_range))\nplt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\nspl.set_xlabel('Number of components')\nspl.legend([b[0] for b in bars], cv_types)\n\n# Plot the winner\nsplot = plt.subplot(2, 1, 2)\nY_ = clf.predict(X)\nfor i, (mean, covar, color) in enumerate(zip(clf.means_, clf.covars_,\n                                             color_iter)):\n    v, w = linalg.eigh(covar)\n    if not np.any(Y_ == i):\n        continue\n    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n    # Plot an ellipse to show the Gaussian component\n    angle = np.arctan2(w[0][1], w[0][0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    v *= 4\n    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(.5)\n    splot.add_artist(ell)\n\nplt.xlim(-10, 10)\nplt.ylim(-3, 6)\nplt.xticks(())\nplt.yticks(())\nplt.title('Selected GMM: full model, 2 components')\nplt.subplots_adjust(hspace=.35, bottom=.02)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html
Gaussian Mixture Model Sine Curve	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gmm_sin_001.png]]	<br><pre><code>import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\nfrom sklearn.externals.six.moves import xrange\n\n# Number of samples per component\nn_samples = 100\n\n# Generate random sample following a sine curve\nnp.random.seed(0)\nX = np.zeros((n_samples, 2))\nstep = 4 * np.pi / n_samples\n\nfor i in xrange(X.shape[0]):\n    x = i * step - 6\n    X[i, 0] = x + np.random.normal(0, 0.1)\n    X[i, 1] = 3 * (np.sin(x) + np.random.normal(0, .2))\n\n\ncolor_iter = itertools.cycle(['r', 'g', 'b', 'c', 'm'])\n\n\nfor i, (clf, title) in enumerate([\n        (mixture.GMM(n_components=10, covariance_type='full', n_iter=100),\n         "Expectation-maximization"),\n        (mixture.DPGMM(n_components=10, covariance_type='full', alpha=0.01,\n                       n_iter=100),\n         "Dirichlet Process,alpha=0.01"),\n        (mixture.DPGMM(n_components=10, covariance_type='diag', alpha=100.,\n                       n_iter=100),\n         "Dirichlet Process,alpha=100.")]):\n\n    clf.fit(X)\n    splot = plt.subplot(3, 1, 1 + i)\n    Y_ = clf.predict(X)\n    for i, (mean, covar, color) in enumerate(zip(\n            clf.means_, clf._get_covars(), color_iter)):\n        v, w = linalg.eigh(covar)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-6, 4 * np.pi - 6)\n    plt.ylim(-5, 5)\n    plt.title(title)\n    plt.xticks(())\n    plt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_sin.html
Gaussian Processes classification example: exploiting the probabilistic output	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gp_probabilistic_classification_after_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n# Licence: BSD 3 clause\n\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.gaussian_process import GaussianProcess\nfrom matplotlib import pyplot as pl\nfrom matplotlib import cm\n\n# Standard normal distribution functions\nphi = stats.distributions.norm().pdf\nPHI = stats.distributions.norm().cdf\nPHIinv = stats.distributions.norm().ppf\n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    """The function to predict (classification will then consist in predicting\n    whether g(x) <= 0 or not)"""\n    return 5. - x[:, 1] - .5 * x[:, 0] ** 2.\n\n# Design of experiments\nX = np.array([[-4.61611719, -6.00099547],\n              [4.10469096, 5.32782448],\n              [0.00000000, -0.50000000],\n              [-6.17289014, -4.6984743],\n              [1.3109306, -6.93271427],\n              [-5.03823144, 3.10584743],\n              [-2.87600388, 6.74310541],\n              [5.21301203, 4.26386883]])\n\n# Observations\ny = g(X)\n\n# Instanciate and fit Gaussian Process Model\ngp = GaussianProcess(theta0=5e-1)\n\n# Don't perform MLE or you'll get a perfect prediction for this simple example!\ngp.fit(X, y)\n\n# Evaluate real function, the prediction and its MSE on a grid\nres = 50\nx1, x2 = np.meshgrid(np.linspace(- lim, lim, res),\n                     np.linspace(- lim, lim, res))\nxx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_pred, MSE = gp.predict(xx, eval_MSE=True)\nsigma = np.sqrt(MSE)\ny_true = y_true.reshape((res, res))\ny_pred = y_pred.reshape((res, res))\nsigma = sigma.reshape((res, res))\nk = PHIinv(.975)\n\n# Plot the probabilistic classification iso-values using the Gaussian property\n# of the prediction\nfig = pl.figure(1)\nax = fig.add_subplot(111)\nax.axes.set_aspect('equal')\npl.xticks([])\npl.yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\npl.xlabel('$x_1$')\npl.ylabel('$x_2$')\n\ncax = pl.imshow(np.flipud(PHI(- y_pred / sigma)), cmap=cm.gray_r, alpha=0.8,\n                extent=(- lim, lim, - lim, lim))\nnorm = pl.matplotlib.colors.Normalize(vmin=0., vmax=0.9)\ncb = pl.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)\ncb.set_label('${\\rm \mathbb{P}}\left[\widehat{G}(\mathbf{x}) \leq 0\\right]$')\n\npl.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)\n\npl.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)\n\ncs = pl.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.025], colors='b',\n                linestyles='solid')\npl.clabel(cs, fontsize=11)\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.5], colors='k',\n                linestyles='dashed')\npl.clabel(cs, fontsize=11)\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.975], colors='r',\n                linestyles='solid')\npl.clabel(cs, fontsize=11)\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html
Gaussian Processes regression: basic introductory example	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n#         Jake Vanderplas <vanderplas@astro.washington.edu>\n# Licence: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcess\nfrom matplotlib import pyplot as pl\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """The function to predict."""\n    return x * np.sin(x)\n\n#----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n\n# Observations\ny = f(X).ravel()\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Instanciate a Gaussian Process model\ngp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,\n                     random_start=100)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, MSE = gp.predict(x, eval_MSE=True)\nsigma = np.sqrt(MSE)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nfig = pl.figure()\npl.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')\npl.plot(X, y, 'r.', markersize=10, label=u'Observations')\npl.plot(x, y_pred, 'b-', label=u'Prediction')\npl.fill(np.concatenate([x, x[::-1]]),\n        np.concatenate([y_pred - 1.9600 * sigma,\n                       (y_pred + 1.9600 * sigma)[::-1]]),\n        alpha=.5, fc='b', ec='None', label='95% confidence interval')\npl.xlabel('$x$')\npl.ylabel('$f(x)$')\npl.ylim(-10, 20)\npl.legend(loc='upper left')\n\n#----------------------------------------------------------------------\n# now the noisy case\nX = np.linspace(0.1, 9.9, 20)\nX = np.atleast_2d(X).T\n\n# Observations and noise\ny = f(X).ravel()\ndy = 0.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Instanciate a Gaussian Process model\ngp = GaussianProcess(corr='squared_exponential', theta0=1e-1,\n                     thetaL=1e-3, thetaU=1,\n                     nugget=(dy / y) ** 2,\n                     random_start=100)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, MSE = gp.predict(x, eval_MSE=True)\nsigma = np.sqrt(MSE)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nfig = pl.figure()\npl.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')\npl.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')\npl.plot(x, y_pred, 'b-', label=u'Prediction')\npl.fill(np.concatenate([x, x[::-1]]),\n        np.concatenate([y_pred - 1.9600 * sigma,\n                       (y_pred + 1.9600 * sigma)[::-1]]),\n        alpha=.5, fc='b', ec='None', label='95% confidence interval')\npl.xlabel('$x$')\npl.ylabel('$f(x)$')\npl.ylim(-10, 20)\npl.legend(loc='upper left')\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_regression.html
Gradient Boosting Out-of-Bag estimates	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gradient_boosting_oob_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.cross_validation import KFold\nfrom sklearn.cross_validation import train_test_split\n\n\n# Generate data (adapted from G. Ridgeway's gbm example)\nn_samples = 1000\nrandom_state = np.random.RandomState(13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = 1 / (1.0 + np.exp(-(np.sin(3 * x1) - 4 * x2 + x3)))\ny = random_state.binomial(1, p, size=n_samples)\n\nX = np.c_[x1, x2, x3]\n\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                    random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\nclf = ensemble.GradientBoostingClassifier(**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint("Accuracy: {:.4f}".format(acc))\n\nn_estimators = params['n_estimators']\nx = np.arange(n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    """compute deviance scores on ``X_test`` and ``y_test``. """\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        score[i] = clf.loss_(y_test, y_pred)\n    return score\n\n\ndef cv_estimate(n_folds=3):\n    cv = KFold(n=X_train.shape[0], n_folds=n_folds)\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv:\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_folds\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test, y_test)\n\n# negative cumulative sum of oob improvements\ncumsum = -np.cumsum(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[np.argmin(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[np.argmin(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[np.argmin(cv_score)]\n\n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# plot curves and vertical lines for best iterations\nplt.plot(x, cumsum, label='OOB loss', color=oob_color)\nplt.plot(x, test_score, label='Test loss', color=test_color)\nplt.plot(x, cv_score, label='CV loss', color=cv_color)\nplt.axvline(x=oob_best_iter, color=oob_color)\nplt.axvline(x=test_best_iter, color=test_color)\nplt.axvline(x=cv_best_iter, color=cv_color)\n\n# add three vertical lines to xticks\nxticks = plt.xticks()\nxticks_pos = np.array(xticks[0].tolist() +\n                      [oob_best_iter, cv_best_iter, test_best_iter])\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n                        ['OOB', 'CV', 'Test'])\nind = np.argsort(xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\nplt.xticks(xticks_pos, xticks_label)\n\nplt.legend(loc='upper right')\nplt.ylabel('normalized loss')\nplt.xlabel('number of iterations')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html
Prediction Intervals for Gradient Boosting Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gradient_boosting_quantile_001.png]]	<br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """The function to predict."""\n    return x * np.sin(x)\n\n#----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T\nX = X.astype(np.float32)\n\n# Observations\ny = f(X).ravel()\n\ndy = 1.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\ny = y.astype(np.float32)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nxx = np.atleast_2d(np.linspace(0, 10, 1000)).T\nxx = xx.astype(np.float32)\n\nalpha = 0.95\n\nclf = GradientBoostingRegressor(loss='quantile', alpha=alpha,\n                                n_estimators=250, max_depth=3,\n                                learning_rate=.1, min_samples_leaf=9,\n                                min_samples_split=9)\n\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_upper = clf.predict(xx)\n\nclf.set_params(alpha=1.0 - alpha)\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_lower = clf.predict(xx)\n\nclf.set_params(loss='ls')\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_pred = clf.predict(xx)\n\n# Plot the function, the prediction and the 90% confidence interval based on\n# the MSE\nfig = plt.figure()\nplt.plot(xx, f(xx), 'g:', label=u'$f(x) = x\,\sin(x)$')\nplt.plot(X, y, 'b.', markersize=10, label=u'Observations')\nplt.plot(xx, y_pred, 'r-', label=u'Prediction')\nplt.plot(xx, y_upper, 'k-')\nplt.plot(xx, y_lower, 'k-')\nplt.fill(np.concatenate([xx, xx[::-1]]),\n         np.concatenate([y_upper, y_lower[::-1]]),\n         alpha=.5, fc='b', ec='None', label='90% prediction interval')\nplt.xlabel('$x$')\nplt.ylabel('$f(x)$')\nplt.ylim(-10, 20)\nplt.legend(loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html
Gradient Boosting regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gradient_boosting_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\n\n###############################################################################\n# Load data\nboston = datasets.load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=13)\nX = X.astype(np.float32)\noffset = int(X.shape[0] * 0.9)\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\n\n###############################################################################\n# Fit regression model\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n          'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\nmse = mean_squared_error(y_test, clf.predict(X_test))\nprint("MSE: %.4f" % mse)\n\n###############################################################################\n# Plot training deviance\n\n# compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\n\n###############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html
Gradient Boosting regularization	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_gradient_boosting_regularization_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\n\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\nX = X.astype(np.float32)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = np.unique(y, return_inverse=True)\n\nX_train, X_test = X[:2000], X[2000:]\ny_train, y_test = y[:2000], y[2000:]\n\noriginal_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,\n                   'min_samples_split': 5}\n\nplt.figure()\n\nfor label, color, setting in [('No shrinkage', 'orange',\n                               {'learning_rate': 1.0, 'subsample': 1.0}),\n                              ('learning_rate=0.1', 'turquoise',\n                               {'learning_rate': 0.1, 'subsample': 1.0}),\n                              ('subsample=0.5', 'blue',\n                               {'learning_rate': 1.0, 'subsample': 0.5}),\n                              ('learning_rate=0.1, subsample=0.5', 'gray',\n                               {'learning_rate': 0.1, 'subsample': 0.5}),\n                              ('learning_rate=0.1, max_features=2', 'magenta',\n                               {'learning_rate': 0.1, 'max_features': 2})]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)\n\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        test_deviance[i] = clf.loss_(y_test, y_pred)\n\n    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n            '-', color=color, label=label)\n\nplt.legend(loc='upper left')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Test Set Deviance')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html
Blind source separation using FastICA	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ica_blind_source_separation_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nfrom sklearn.decomposition import FastICA, PCA\n\n###############################################################################\n# Generate sample data\nnp.random.seed(0)\nn_samples = 2000\ntime = np.linspace(0, 8, n_samples)\n\ns1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\ns2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\ns3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\nS = np.c_[s1, s2, s3]\nS += 0.2 * np.random.normal(size=S.shape)  # Add noise\n\nS /= S.std(axis=0)  # Standardize data\n# Mix data\nA = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\nX = np.dot(S, A.T)  # Generate observations\n\n# Compute ICA\nica = FastICA(n_components=3)\nS_ = ica.fit_transform(X)  # Reconstruct signals\nA_ = ica.mixing_  # Get estimated mixing matrix\n\n# We can `prove` that the ICA model applies by reverting the unmixing.\nassert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n\n# For comparison, compute PCA\npca = PCA(n_components=3)\nH = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n\n###############################################################################\n# Plot results\n\nplt.figure()\n\nmodels = [X, S, S_, H]\nnames = ['Observations (mixed signal)',\n         'True Sources',\n         'ICA recovered signals', \n         'PCA recovered signals']\ncolors = ['red', 'steelblue', 'orange']\n\nfor ii, (model, name) in enumerate(zip(models, names), 1):\n    plt.subplot(4, 1, ii)\n    plt.title(name)\n    for sig, color in zip(model.T, colors):\n        plt.plot(sig, color=color)\n\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html
FastICA on 2D point clouds	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ica_vs_pca_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort, Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, FastICA\n\n###############################################################################\n# Generate sample data\nrng = np.random.RandomState(42)\nS = rng.standard_t(1.5, size=(20000, 2))\nS[:, 0] *= 2.\n\n# Mix data\nA = np.array([[1, 1], [0, 2]])  # Mixing matrix\n\nX = np.dot(S, A.T)  # Generate observations\n\npca = PCA()\nS_pca_ = pca.fit(X).transform(X)\n\nica = FastICA(random_state=rng)\nS_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n\nS_ica_ /= S_ica_.std(axis=0)\n\n\n###############################################################################\n# Plot results\n\ndef plot_samples(S, axis_list=None):\n    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n                color='steelblue', alpha=0.5)\n    if axis_list is not None:\n        colors = ['orange', 'red']\n        for color, axis in zip(colors, axis_list):\n            axis /= axis.std()\n            x_axis, y_axis = axis\n            # Trick to get legend to work\n            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,\n                       color=color)\n\n    plt.hlines(0, -3, 3)\n    plt.vlines(0, -3, 3)\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.xlabel('x')\n    plt.ylabel('y')\n\nplt.figure()\nplt.subplot(2, 2, 1)\nplot_samples(S / S.std())\nplt.title('True Independent Sources')\n\naxis_list = [pca.components_.T, ica.mixing_]\nplt.subplot(2, 2, 2)\nplot_samples(X / np.std(X), axis_list=axis_list)\nlegend = plt.legend(['PCA', 'ICA'], loc='upper right')\nlegend.set_zorder(100)\n\nplt.title('Observations')\n\nplt.subplot(2, 2, 3)\nplot_samples(S_pca_ / np.std(S_pca_, axis=0))\nplt.title('PCA recovered signals')\n\nplt.subplot(2, 2, 4)\nplot_samples(S_ica_ / np.std(S_ica_))\nplt.title('ICA recovered signals')\n\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html
Image denoising using dictionary learning	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.feature_extraction.image import extract_patches_2d\nfrom sklearn.feature_extraction.image import reconstruct_from_patches_2d\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n###############################################################################\ntry:\n    from scipy import misc\n    face = misc.face(gray=True)\nexcept AttributeError:\n    # Old versions of scipy have face in the top level package\n    face = sp.face(gray=True)\n\n# Convert from uint8 representation with values between 0 and 255 to\n# a floating point representation with values between 0 and 1.\nface = face / 255\n\n# downsample for higher speed\nface = face[::2, ::2] + face[1::2, ::2] + face[::2, 1::2] + face[1::2, 1::2]\nface /= 4.0\nheight, width = face.shape\n\n# Distort the right half of the image\nprint('Distorting image...')\ndistorted = face.copy()\ndistorted[:, width // 2:] += 0.075 * np.random.randn(height, width // 2)\n\n# Extract all reference patches from the left half of the image\nprint('Extracting reference patches...')\nt0 = time()\npatch_size = (7, 7)\ndata = extract_patches_2d(distorted[:, :width // 2], patch_size)\ndata = data.reshape(data.shape[0], -1)\ndata -= np.mean(data, axis=0)\ndata /= np.std(data, axis=0)\nprint('done in %.2fs.' % (time() - t0))\n\n###############################################################################\n# Learn the dictionary from reference patches\n\nprint('Learning the dictionary...')\nt0 = time()\ndico = MiniBatchDictionaryLearning(n_components=100, alpha=1, n_iter=500)\nV = dico.fit(data).components_\ndt = time() - t0\nprint('done in %.2fs.' % dt)\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle('Dictionary learned from face patches\n' +\n             'Train time %.1fs on %d patches' % (dt, len(data)),\n             fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n###############################################################################\n# Display the distorted image\n\ndef show_with_diff(image, reference, title):\n    """Helper function to display denoising"""\n    plt.figure(figsize=(5, 3.3))\n    plt.subplot(1, 2, 1)\n    plt.title('Image')\n    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    plt.subplot(1, 2, 2)\n    difference = image - reference\n\n    plt.title('Difference (norm: %.2f)' % np.sqrt(np.sum(difference ** 2)))\n    plt.imshow(difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    plt.suptitle(title, size=16)\n    plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)\n\nshow_with_diff(distorted, face, 'Distorted image')\n\n###############################################################################\n# Extract noisy patches and reconstruct them using the dictionary\n\nprint('Extracting noisy patches... ')\nt0 = time()\ndata = extract_patches_2d(distorted[:, width // 2:], patch_size)\ndata = data.reshape(data.shape[0], -1)\nintercept = np.mean(data, axis=0)\ndata -= intercept\nprint('done in %.2fs.' % (time() - t0))\n\ntransform_algorithms = [\n    ('Orthogonal Matching Pursuit\n1 atom', 'omp',\n     {'transform_n_nonzero_coefs': 1}),\n    ('Orthogonal Matching Pursuit\n2 atoms', 'omp',\n     {'transform_n_nonzero_coefs': 2}),\n    ('Least-angle regression\n5 atoms', 'lars',\n     {'transform_n_nonzero_coefs': 5}),\n    ('Thresholding\n alpha=0.1', 'threshold', {'transform_alpha': .1})]\n\nreconstructions = {}\nfor title, transform_algorithm, kwargs in transform_algorithms:\n    print(title + '...')\n    reconstructions[title] = face.copy()\n    t0 = time()\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\n    code = dico.transform(data)\n    patches = np.dot(code, V)\n\n    if transform_algorithm == 'threshold':\n        patches -= patches.min()\n        patches /= patches.max()\n\n    patches += intercept\n    patches = patches.reshape(len(data), *patch_size)\n    if transform_algorithm == 'threshold':\n        patches -= patches.min()\n        patches /= patches.max()\n    reconstructions[title][:, width // 2:] = reconstruct_from_patches_2d(\n        patches, (height, width // 2))\n    dt = time() - t0\n    print('done in %.2fs.' % dt)\n    show_with_diff(reconstructions[title], face,\n                   title + ' (time: %.1fs)' % dt)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html
Incremental PCA	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Authors: Kyle Kastner\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nn_components = 2\nipca = IncrementalPCA(n_components=n_components, batch_size=10)\nX_ipca = ipca.fit_transform(X)\n\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X)\n\nfor X_transformed, title in [(X_ipca, "Incremental PCA"), (X_pca, "PCA")]:\n    plt.figure(figsize=(8, 8))\n    for c, i, target_name in zip("rgb", [0, 1, 2], iris.target_names):\n        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n                    c=c, label=target_name)\n\n    if "Incremental" in title:\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n        plt.title(title + " of iris dataset\nMean absolute unsigned error "\n                  "%.6f" % err)\n    else:\n        plt.title(title + " of iris dataset")\n    plt.legend(loc="best")\n    plt.axis([-4, 4, -1.5, 1.5])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html
Plot different SVM classifiers in the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_iris_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html
Plot the decision surface of a decision tree on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_iris_0011.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nplot_colors = "bry"\nplot_step = 0.02\n\n# Load data\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n                                [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Shuffle\n    idx = np.arange(X.shape[0])\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X = X[idx]\n    y = y[idx]\n\n    # Standardize\n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    X = (X - mean) / std\n\n    # Train\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n    plt.axis("tight")\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.Paired)\n\n    plt.axis("tight")\n\nplt.suptitle("Decision surface of a decision tree using paired features")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html
The Iris Dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nplt.figure(2, figsize=(8, 6))\nplt.clf()\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(iris.data)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,\n           cmap=plt.cm.Paired)\nax.set_title("First three PCA directions")\nax.set_xlabel("1st eigenvector")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel("2nd eigenvector")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel("3rd eigenvector")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html
SVM Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, svm\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 0, :2]\ny = y[y != 0]\n\nn_sample = len(X)\n\nnp.random.seed(0)\norder = np.random.permutation(n_sample)\nX = X[order]\ny = y[order].astype(np.float)\n\nX_train = X[:.9 * n_sample]\ny_train = y[:.9 * n_sample]\nX_test = X[.9 * n_sample:]\ny_test = y[.9 * n_sample:]\n\n# fit the model\nfor fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):\n    clf = svm.SVC(kernel=kernel, gamma=10)\n    clf.fit(X_train, y_train)\n\n    plt.figure(fig_num)\n    plt.clf()\n    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)\n\n    # Circle out the test data\n    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)\n\n    plt.axis('tight')\n    x_min = X[:, 0].min()\n    x_max = X[:, 0].max()\n    y_min = X[:, 1].min()\n    y_max = X[:, 1].max()\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\n    plt.title(kernel)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html
Logistic Regression 3-class Classifier	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_iris_logistic_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nh = .02  # step size in the mesh\n\nlogreg = linear_model.LogisticRegression(C=1e5)\n\n# we create an instance of Neighbours Classifier and fit the data.\nlogreg.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html
Isotonic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_isotonic_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n###############################################################################\n# plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html
The Johnson-Lindenstrauss bound for embedding with random projections	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport sys\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# Part 1: plot the theoretical dependency between n_components_min and\n# n_samples\n\n# range of admissible distortions\neps_range = np.linspace(0.1, 0.99, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(1, 9, 9)\n\nplt.figure()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n    plt.loglog(n_samples_range, min_n_components, color=color)\n\nplt.legend(["eps = %0.1f" % eps for eps in eps_range], loc="lower right")\nplt.xlabel("Number of observations to eps-embed")\nplt.ylabel("Minimum number of dimensions")\nplt.title("Johnson-Lindenstrauss bounds:\nn_samples vs n_components")\n\n# range of admissible distortions\neps_range = np.linspace(0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(2, 6, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\n\nplt.figure()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n    plt.semilogy(eps_range, min_n_components, color=color)\n\nplt.legend(["n_samples = %d" % n for n in n_samples_range], loc="upper right")\nplt.xlabel("Distortion eps")\nplt.ylabel("Minimum number of dimensions")\nplt.title("Johnson-Lindenstrauss bounds:\nn_components vs eps")\n\n# Part 2: perform sparse random projection of some digits images which are\n# quite low dimensional and dense or documents of the 20 newsgroups dataset\n# which is both high dimensional and sparse\n\nif '--twenty-newsgroups' in sys.argv:\n    # Need an internet connection hence not enabled by default\n    data = fetch_20newsgroups_vectorized().data[:500]\nelse:\n    data = load_digits().data[:500]\n\nn_samples, n_features = data.shape\nprint("Embedding %d samples with dim %d using various random projections"\n      % (n_samples, n_features))\n\nn_components_range = np.array([300, 1000, 10000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = time()\n    rp = SparseRandomProjection(n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print("Projected %d samples from %d to %d in %0.3fs"\n          % (n_samples, n_features, n_components, time() - t0))\n    if hasattr(rp, 'components_'):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print("Random matrix with size: %0.3fMB" % (n_bytes / 1e6))\n\n    projected_dists = euclidean_distances(\n        projected_data, squared=True).ravel()[nonzero]\n\n    plt.figure()\n    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu)\n    plt.xlabel("Pairwise squared distances in original space")\n    plt.ylabel("Pairwise squared distances in projected space")\n    plt.title("Pairwise distances distribution for n_components=%d" %\n              n_components)\n    cb = plt.colorbar()\n    cb.set_label('Sample pairs counts')\n\n    rates = projected_dists / dists\n    print("Mean distances rate: %0.2f (%0.2f)"\n          % (np.mean(rates), np.std(rates)))\n\n    plt.figure()\n    plt.hist(rates, bins=50, normed=True, range=(0., 2.))\n    plt.xlabel("Squared distances rate: projected / original")\n    plt.ylabel("Distribution of samples pairs")\n    plt.title("Histogram of pairwise distance rates for n_components=%d" %\n              n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html
Simple 1D Kernel Density Estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.neighbors import KernelDensity\n\n\n#----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\nnp.random.seed(1)\nN = 20\nX = np.concatenate((np.random.normal(0, 1, 0.3 * N),\n                    np.random.normal(5, 1, 0.7 * N)))[:, np.newaxis]\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\nbins = np.linspace(-5, 10, 10)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True)\nax[0, 0].text(-3.5, 0.31, "Histogram")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', normed=True)\nax[0, 1].text(-3.5, 0.31, "Histogram, bins shifted")\n\n# tophat KDE\nkde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\nax[1, 0].text(-3.5, 0.31, "Tophat Kernel Density")\n\n# Gaussian KDE\nkde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\nax[1, 1].text(-3.5, 0.31, "Gaussian Kernel Density")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], np.zeros(X.shape[0]) - 0.01, '+k')\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel('Normalized Density')\n\nfor axi in ax[1, :]:\n    axi.set_xlabel('x')\n\n#----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return '0'\n    elif x == 1:\n        return 'h'\n    elif x == -1:\n        return '-h'\n    else:\n        return '%ih' % x\n\nfor i, kernel in enumerate(['gaussian', 'tophat', 'epanechnikov',\n                            'exponential', 'linear', 'cosine']):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF')\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title('Available Kernels')\n\n#----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate((np.random.normal(0, 1, 0.3 * N),\n                    np.random.normal(5, 1, 0.7 * N)))[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = (0.3 * norm(0, 1).pdf(X_plot[:, 0])\n             + 0.7 * norm(5, 1).pdf(X_plot[:, 0]))\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc='black', alpha=0.2,\n        label='input distribution')\n\nfor kernel in ['gaussian', 'tophat', 'epanechnikov']:\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(X_plot[:, 0], np.exp(log_dens), '-',\n            label="kernel = '{0}'".format(kernel))\n\nax.text(6, 0.38, "N={0} points".format(N))\n\nax.legend(loc='upper left')\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html
Explicit feature map approximation for RBF kernels	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom time import time\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, pipeline\nfrom sklearn.kernel_approximation import (RBFSampler,\n                                          Nystroem)\nfrom sklearn.decomposition import PCA\n\n# The digits dataset\ndigits = datasets.load_digits(n_class=9)\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.data)\ndata = digits.data / 16.\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:]\n#data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = svm.SVC(gamma=.2)\nlinear_svm = svm.LinearSVC()\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = RBFSampler(gamma=.2, random_state=1)\nfeature_map_nystroem = Nystroem(gamma=.2, random_state=1)\nfourier_approx_svm = pipeline.Pipeline([("feature_map", feature_map_fourier),\n                                        ("svm", svm.LinearSVC())])\n\nnystroem_approx_svm = pipeline.Pipeline([("feature_map", feature_map_nystroem),\n                                        ("svm", svm.LinearSVC())])\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = time()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = time() - kernel_svm_time\n\nlinear_svm_time = time()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = time() - linear_svm_time\n\nsample_sizes = 30 * np.arange(1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = time()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(time() - start)\n\n    start = time()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(time() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\nplt.figure(figsize=(8, 8))\naccuracy = plt.subplot(211)\n# second y axis for timeings\ntimescale = plt.subplot(212)\n\naccuracy.plot(sample_sizes, nystroem_scores, label="Nystroem approx. kernel")\ntimescale.plot(sample_sizes, nystroem_times, '--',\n               label='Nystroem approx. kernel')\n\naccuracy.plot(sample_sizes, fourier_scores, label="Fourier approx. kernel")\ntimescale.plot(sample_sizes, fourier_times, '--',\n               label='Fourier approx. kernel')\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [linear_svm_score, linear_svm_score], label="linear svm")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [linear_svm_time, linear_svm_time], '--', label='linear svm')\n\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [kernel_svm_score, kernel_svm_score], label="rbf svm")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [kernel_svm_time, kernel_svm_time], '--', label='rbf svm')\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label="n_features")\n\n# legends and labels\naccuracy.set_title("Classification accuracy")\ntimescale.set_title("Training times")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel("Sampling steps = transformed feature dimension")\naccuracy.set_ylabel("Classification accuracy")\ntimescale.set_ylabel("Training time in seconds")\naccuracy.legend(loc='best')\ntimescale.legend(loc='best')\n\n# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = PCA(n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Gemerate grid along first two principal components\nmultiples = np.arange(-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, np.newaxis] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, np.newaxis] * pca.components_[1, :]\n# combine\ngrid = first[np.newaxis, :, :] + second[:, np.newaxis, :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = ['SVC with rbf kernel',\n          'SVC (linear kernel)\n with Fourier rbf feature map\n'\n          'n_components=100',\n          'SVC (linear kernel)\n with Nystroem rbf feature map\n'\n          'n_components=100']\n\nplt.tight_layout()\nplt.figure(figsize=(12, 5))\n\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm,\n                         fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired)\n    plt.axis('off')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired)\n\n    plt.title(titles[i])\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html
Kernel PCA	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_kernel_pca_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Mathieu Blondel\n#          Andreas Mueller\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(0)\n\nX, y = make_circles(n_samples=400, factor=.3, noise=.05)\n\nkpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10)\nX_kpca = kpca.fit_transform(X)\nX_back = kpca.inverse_transform(X_kpca)\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Plot results\n\nplt.figure()\nplt.subplot(2, 2, 1, aspect='equal')\nplt.title("Original space")\nreds = y == 0\nblues = y == 1\n\nplt.plot(X[reds, 0], X[reds, 1], "ro")\nplt.plot(X[blues, 0], X[blues, 1], "bo")\nplt.xlabel("$x_1$")\nplt.ylabel("$x_2$")\n\nX1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\nX_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n# projection on the first principal component (in the phi space)\nZ_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\nplt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n\nplt.subplot(2, 2, 2, aspect='equal')\nplt.plot(X_pca[reds, 0], X_pca[reds, 1], "ro")\nplt.plot(X_pca[blues, 0], X_pca[blues, 1], "bo")\nplt.title("Projection by PCA")\nplt.xlabel("1st principal component")\nplt.ylabel("2nd component")\n\nplt.subplot(2, 2, 3, aspect='equal')\nplt.plot(X_kpca[reds, 0], X_kpca[reds, 1], "ro")\nplt.plot(X_kpca[blues, 0], X_kpca[blues, 1], "bo")\nplt.title("Projection by KPCA")\nplt.xlabel("1st principal component in space induced by $\phi$")\nplt.ylabel("2nd component")\n\nplt.subplot(2, 2, 4, aspect='equal')\nplt.plot(X_back[reds, 0], X_back[reds, 1], "ro")\nplt.plot(X_back[blues, 0], X_back[blues, 1], "bo")\nplt.title("Original space after inverse transform")\nplt.xlabel("$x_1$")\nplt.ylabel("$x_2$")\n\nplt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html
Comparison of kernel ridge regression and SVR	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nfrom __future__ import division\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.learning_curve import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n#############################################################################\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n#############################################################################\n# Fit regression model\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={"C": [1e0, 1e1, 1e2, 1e3],\n                               "gamma": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n                  param_grid={"alpha": [1e0, 0.1, 1e-2, 1e-3],\n                              "gamma": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint("SVR complexity and bandwidth selected and model fitted in %.3f s"\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint("KRR complexity and bandwidth selected and model fitted in %.3f s"\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint("Support vector ratio: %.3f" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint("SVR prediction for %d inputs in %.3f s"\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint("KRR prediction for %d inputs in %.3f s"\n      % (X_plot.shape[0], kr_predict))\n\n\n#############################################################################\n# look at the results\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors')\nplt.scatter(X[:100], y[:100], c='k', label='data')\nplt.hold('on')\nplt.plot(X_plot, y_svr, c='r',\n         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c='g',\n         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('SVR versus Kernel Ridge')\nplt.legend()\n\n# Visualize training and prediction time\nplt.figure()\n\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\nsizes = np.logspace(1, 4, 7)\nfor name, estimator in {"KRR": KernelRidge(kernel='rbf', alpha=0.1,\n                                           gamma=10),\n                        "SVR": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, 'o-', color="r" if name == "SVR" else "g",\n             label="%s (train)" % name)\n    plt.plot(sizes, test_time, 'o--', color="r" if name == "SVR" else "g",\n             label="%s (test)" % name)\n\nplt.xscale("log")\nplt.yscale("log")\nplt.xlabel("Train size")\nplt.ylabel("Time (seconds)")\nplt.title('Execution Time')\nplt.legend(loc="best")\n\n# Visualize learning curves\nplt.figure()\n\nsvr = SVR(kernel='rbf', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring="mean_squared_error", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring="mean_squared_error", cv=10)\n\nplt.plot(train_sizes, test_scores_svr.mean(1), 'o-', color="r",\n         label="SVR")\nplt.plot(train_sizes, test_scores_kr.mean(1), 'o-', color="g",\n         label="KRR")\nplt.xlabel("Train size")\nplt.ylabel("Mean Squared Error")\nplt.title('Learning curves')\nplt.legend(loc="best")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html
Demonstration of k-means assumptions	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_kmeans_assumptions_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Phil Roth <mr.phil.roth@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# Incorrect number of clusters\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title("Incorrect Number of Blobs")\n\n# Anisotropicly distributed data\ntransformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title("Anisotropicly Distributed Blobs")\n\n# Different variance\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title("Unequal Variance")\n\n# Unevenly sized blobs\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title("Unevenly Sized Blobs")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html
A demo of K-Means clustering on the handwritten digits data	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_kmeans_digits_001.png]]	<br><pre><code>print(__doc__)\n\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\nnp.random.seed(42)\n\ndigits = load_digits()\ndata = scale(digits.data)\n\nn_samples, n_features = data.shape\nn_digits = len(np.unique(digits.target))\nlabels = digits.target\n\nsample_size = 300\n\nprint("n_digits: %d, \t n_samples %d, \t n_features %d"\n      % (n_digits, n_samples, n_features))\n\n\nprint(79 * '_')\nprint('% 9s' % 'init'\n      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n\n\ndef bench_k_means(estimator, name, data):\n    t0 = time()\n    estimator.fit(data)\n    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n          % (name, (time() - t0), estimator.inertia_,\n             metrics.homogeneity_score(labels, estimator.labels_),\n             metrics.completeness_score(labels, estimator.labels_),\n             metrics.v_measure_score(labels, estimator.labels_),\n             metrics.adjusted_rand_score(labels, estimator.labels_),\n             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric='euclidean',\n                                      sample_size=sample_size)))\n\nbench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n              name="k-means++", data=data)\n\nbench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n              name="random", data=data)\n\n# in this case the seeding of the centers is deterministic, hence we run the\n# kmeans algorithm only once with n_init=1\npca = PCA(n_components=n_digits).fit(data)\nbench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n              name="PCA-based",\n              data=data)\nprint(79 * '_')\n\n###############################################################################\n# Visualize the results on PCA-reduced data\n\nreduced_data = PCA(n_components=2).fit_transform(data)\nkmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\nkmeans.fit(reduced_data)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means clustering on the digits dataset (PCA-reduced data)\n'\n          'Centroids are marked with white cross')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html
Selecting the number of clusters with silhouette analysis on KMeans clustering	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import print_function\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distict cluster and 3 clusters placed close\n# together.\nX, y = make_blobs(n_samples=500,\n                  n_features=2,\n                  centers=4,\n                  cluster_std=1,\n                  center_box=(-10.0, 10.0),\n                  shuffle=True,\n                  random_state=1)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print("For n_clusters =", n_clusters,\n          "The average silhouette_score is :", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title("The silhouette plot for the various clusters.")\n    ax1.set_xlabel("The silhouette coefficient values")\n    ax1.set_ylabel("Cluster label")\n\n    # The vertical line for average silhoutte score of all the values\n    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c="white", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title("The visualization of the clustered data.")\n    ax2.set_xlabel("Feature space for the 1st feature")\n    ax2.set_ylabel("Feature space for the 2nd feature")\n\n    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "\n                  "with n_clusters = %d" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
Empirical evaluation of the impact of k-means initialization	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import check_random_state\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\n\nrandom_state = np.random.RandomState(0)\n\n# Number of run (with randomly generated dataset) for each strategy so as\n# to be able to compute an estimate of the standard deviation\nn_runs = 5\n\n# k-means models can do several random inits so as to be able to trade\n# CPU time for convergence robustness\nn_init_range = np.array([1, 5, 10, 15, 20])\n\n# Datasets generation parameters\nn_samples_per_center = 100\ngrid_size = 3\nscale = 0.1\nn_clusters = grid_size ** 2\n\n\ndef make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j]\n                        for i in range(grid_size)\n                        for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1]))\n\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center\n                        for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)\n\n# Part 1: Quantitative evaluation of various init methods\n\nfig = plt.figure()\nplots = []\nlegends = []\n\ncases = [\n    (KMeans, 'k-means++', {}),\n    (KMeans, 'random', {}),\n    (MiniBatchKMeans, 'k-means++', {'max_no_improvement': 3}),\n    (MiniBatchKMeans, 'random', {'max_no_improvement': 3, 'init_size': 500}),\n]\n\nfor factory, init, params in cases:\n    print("Evaluation of %s with %s init" % (factory.__name__, init))\n    inertia = np.empty((len(n_init_range), n_runs))\n\n    for run_id in range(n_runs):\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n        for i, n_init in enumerate(n_init_range):\n            km = factory(n_clusters=n_clusters, init=init, random_state=run_id,\n                         n_init=n_init, **params).fit(X)\n            inertia[i, run_id] = km.inertia_\n    p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))\n    plots.append(p[0])\n    legends.append("%s with %s init" % (factory.__name__, init))\n\nplt.xlabel('n_init')\nplt.ylabel('inertia')\nplt.legend(plots, legends)\nplt.title("Mean inertia for various k-means init across %d runs" % n_runs)\n\n# Part 2: Qualitative visual inspection of the convergence\n\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\nkm = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=1,\n                     random_state=random_state).fit(X)\n\nfig = plt.figure()\nfor k in range(n_clusters):\n    my_members = km.labels_ == k\n    color = cm.spectral(float(k) / n_clusters, 1)\n    plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)\n    cluster_center = km.cluster_centers_[k]\n    plt.plot(cluster_center[0], cluster_center[1], 'o',\n             markerfacecolor=color, markeredgecolor='k', markersize=6)\n    plt.title("Example cluster allocation with a single random init\n"\n              "with MiniBatchKMeans")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html
Label Propagation digits: Demonstrating performance	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_label_propagation_digits_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import label_propagation\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(0)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 30\n\nindices = np.arange(n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]\n\n# shuffle everything around\ny_train = np.copy(y)\ny_train[unlabeled_set] = -1\n\n###############################################################################\n# Learn with LabelSpreading\nlp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)\nlp_model.fit(X, y_train)\npredicted_labels = lp_model.transduction_[unlabeled_set]\ntrue_labels = y[unlabeled_set]\n\ncm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\n\nprint("Label Spreading model: %d labeled & %d unlabeled points (%d total)" %\n      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n\nprint(classification_report(true_labels, predicted_labels))\n\nprint("Confusion matrix")\nprint(cm)\n\n# calculate uncertainty values for each transduced distribution\npred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n\n# pick the top 10 most uncertain labels\nuncertainty_index = np.argsort(pred_entropies)[-10:]\n\n###############################################################################\n# plot\nf = plt.figure(figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    plt.xticks([])\n    plt.yticks([])\n    sub.set_title('predict: %i\ntrue: %i' % (\n        lp_model.transduction_[image_index], y[image_index]))\n\nf.suptitle('Learning with small amount of labeled data')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits.html
Label Propagation digits active learning	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_label_propagation_digits_active_learning_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import label_propagation\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(0)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 10\n\nunlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]\nf = plt.figure()\n\nfor i in range(5):\n    y_train = np.copy(y)\n    y_train[unlabeled_indices] = -1\n\n    lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)\n    lp_model.fit(X, y_train)\n\n    predicted_labels = lp_model.transduction_[unlabeled_indices]\n    true_labels = y[unlabeled_indices]\n\n    cm = confusion_matrix(true_labels, predicted_labels,\n                          labels=lp_model.classes_)\n\n    print('Iteration %i %s' % (i, 70 * '_'))\n    print("Label Spreading model: %d labeled & %d unlabeled (%d total)"\n          % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n\n    print(classification_report(true_labels, predicted_labels))\n\n    print("Confusion matrix")\n    print(cm)\n\n    # compute the entropies of transduced label distributions\n    pred_entropies = stats.distributions.entropy(\n        lp_model.label_distributions_.T)\n\n    # select five digit examples that the classifier is most uncertain about\n    uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]\n\n    # keep track of indices that we get labels for\n    delete_indices = np.array([])\n\n    f.text(.05, (1 - (i + 1) * .183),\n           "model %d\n\nfit with\n%d labels" % ((i + 1), i * 5 + 10), size=10)\n    for index, image_index in enumerate(uncertainty_index):\n        image = images[image_index]\n\n        sub = f.add_subplot(5, 5, index + 1 + (5 * i))\n        sub.imshow(image, cmap=plt.cm.gray_r)\n        sub.set_title('predict: %i\ntrue: %i' % (\n            lp_model.transduction_[image_index], y[image_index]), size=10)\n        sub.axis('off')\n\n        # labeling 5 points, remote from labeled set\n        delete_index, = np.where(unlabeled_indices == image_index)\n        delete_indices = np.concatenate((delete_indices, delete_index))\n\n    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n    n_labeled_points += 5\n\nf.suptitle("Active learning with Label Propagation.\nRows show 5 most "\n           "uncertain labels to learn with the next model.")\nplt.subplots_adjust(0.12, 0.03, 0.9, 0.8, 0.2, 0.45)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html
Label Propagation learning a complex structure	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_label_propagation_structure_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.semi_supervised import label_propagation\nfrom sklearn.datasets import make_circles\n\n# generate ring with inner box\nn_samples = 200\nX, y = make_circles(n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = -np.ones(n_samples)\nlabels[0] = outer\nlabels[-1] = inner\n\n###############################################################################\n# Learn with LabelSpreading\nlabel_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0)\nlabel_spread.fit(X, labels)\n\n###############################################################################\n# Plot output labels\noutput_labels = label_spread.transduction_\nplt.figure(figsize=(8.5, 4))\nplt.subplot(1, 2, 1)\nplot_outer_labeled, = plt.plot(X[labels == outer, 0],\n                               X[labels == outer, 1], 'rs')\nplot_unlabeled, = plt.plot(X[labels == -1, 0], X[labels == -1, 1], 'g.')\nplot_inner_labeled, = plt.plot(X[labels == inner, 0],\n                               X[labels == inner, 1], 'bs')\nplt.legend((plot_outer_labeled, plot_inner_labeled, plot_unlabeled),\n           ('Outer Labeled', 'Inner Labeled', 'Unlabeled'), loc='upper left',\n           numpoints=1, shadow=False)\nplt.title("Raw data (2 classes=red and blue)")\n\nplt.subplot(1, 2, 2)\noutput_label_array = np.asarray(output_labels)\nouter_numbers = np.where(output_label_array == outer)[0]\ninner_numbers = np.where(output_label_array == inner)[0]\nplot_outer, = plt.plot(X[outer_numbers, 0], X[outer_numbers, 1], 'rs')\nplot_inner, = plt.plot(X[inner_numbers, 0], X[inner_numbers, 1], 'bs')\nplt.legend((plot_outer, plot_inner), ('Outer Learned', 'Inner Learned'),\n           loc='upper left', numpoints=1, shadow=False)\nplt.title("Labels learned with Label Spreading (KNN)")\n\nplt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_structure.html
Decision boundary of label propagation versus SVM on the Iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_label_propagation_versus_svm_iris_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.semi_supervised import label_propagation\n\nrng = np.random.RandomState(0)\n\niris = datasets.load_iris()\n\nX = iris.data[:, :2]\ny = iris.target\n\n# step size in the mesh\nh = .02\n\ny_30 = np.copy(y)\ny_30[rng.rand(len(y)) < 0.3] = -1\ny_50 = np.copy(y)\ny_50[rng.rand(len(y)) < 0.5] = -1\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nls30 = (label_propagation.LabelSpreading().fit(X, y_30),\n        y_30)\nls50 = (label_propagation.LabelSpreading().fit(X, y_50),\n        y_50)\nls100 = (label_propagation.LabelSpreading().fit(X, y), y)\nrbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['Label Spreading 30% data',\n          'Label Spreading 50% data',\n          'Label Spreading 100% data',\n          'SVC with rbf kernel']\n\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}\n\nfor i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.axis('off')\n\n    # Plot also the training points\n    colors = [color_map[y] for y in y_train]\n    plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)\n\n    plt.title(titles[i])\n\nplt.text(.90, 0, "Unlabeled points are colored white")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_versus_svm_iris.html
Lasso and Elastic Net for Sparse Signals	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_lasso_and_elasticnet_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import r2_score\n\n###############################################################################\n# generate some sparse data to play with\nnp.random.seed(42)\n\nn_samples, n_features = 50, 200\nX = np.random.randn(n_samples, n_features)\ncoef = 3 * np.random.randn(n_features)\ninds = np.arange(n_features)\nnp.random.shuffle(inds)\ncoef[inds[10:]] = 0  # sparsify coef\ny = np.dot(X, coef)\n\n# add noise\ny += 0.01 * np.random.normal((n_samples,))\n\n# Split data in train set and test set\nn_samples = X.shape[0]\nX_train, y_train = X[:n_samples / 2], y[:n_samples / 2]\nX_test, y_test = X[n_samples / 2:], y[n_samples / 2:]\n\n###############################################################################\n# Lasso\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.1\nlasso = Lasso(alpha=alpha)\n\ny_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\nr2_score_lasso = r2_score(y_test, y_pred_lasso)\nprint(lasso)\nprint("r^2 on test data : %f" % r2_score_lasso)\n\n###############################################################################\n# ElasticNet\nfrom sklearn.linear_model import ElasticNet\n\nenet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n\ny_pred_enet = enet.fit(X_train, y_train).predict(X_test)\nr2_score_enet = r2_score(y_test, y_pred_enet)\nprint(enet)\nprint("r^2 on test data : %f" % r2_score_enet)\n\nplt.plot(enet.coef_, label='Elastic net coefficients')\nplt.plot(lasso.coef_, label='Lasso coefficients')\nplt.plot(coef, '--', label='original coefficients')\nplt.legend(loc='best')\nplt.title("Lasso R^2: %f, Elastic Net R^2: %f"\n          % (r2_score_lasso, r2_score_enet))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html
Lasso and Elastic Net	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import lasso_path, enet_path\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nX /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)\n\n# Compute paths\n\neps = 5e-3  # the smaller it is the longer is the path\n\nprint("Computing regularization path using the lasso...")\nalphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n\nprint("Computing regularization path using the positive lasso...")\nalphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(\n    X, y, eps, positive=True, fit_intercept=False)\nprint("Computing regularization path using the elastic net...")\nalphas_enet, coefs_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)\n\nprint("Computing regularization path using the positve elastic net...")\nalphas_positive_enet, coefs_positive_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)\n\n# Display results\n\nplt.figure(1)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_lasso), coefs_lasso.T)\nl2 = plt.plot(-np.log10(alphas_enet), coefs_enet.T, linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Lasso and Elastic-Net Paths')\nplt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\nplt.axis('tight')\n\n\nplt.figure(2)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_lasso), coefs_lasso.T)\nl2 = plt.plot(-np.log10(alphas_positive_lasso), coefs_positive_lasso.T,\n              linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Lasso and positive Lasso')\nplt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')\nplt.axis('tight')\n\n\nplt.figure(3)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_enet), coefs_enet.T)\nl2 = plt.plot(-np.log10(alphas_positive_enet), coefs_positive_enet.T,\n              linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Elastic-Net and positive Elastic-Net')\nplt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),\n           loc='lower left')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html
Lasso path using LARS	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_lasso_lars_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nprint("Computing regularization path using the LARS ...")\nalphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)\n\nxx = np.sum(np.abs(coefs.T), axis=1)\nxx /= xx[-1]\n\nplt.plot(xx, coefs.T)\nymin, ymax = plt.ylim()\nplt.vlines(xx, ymin, ymax, linestyle='dashed')\nplt.xlabel('|coef| / max|coef|')\nplt.ylabel('Coefficients')\nplt.title('LASSO Path')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html
Lasso model selection: Cross-Validation / AIC / BIC	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nrng = np.random.RandomState(42)\nX = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n\n# normalize data as done by Lars to allow for comparison\nX /= np.sqrt(np.sum(X ** 2, axis=0))\n\n##############################################################################\n# LassoLarsIC: least angle regression with BIC/AIC criterion\n\nmodel_bic = LassoLarsIC(criterion='bic')\nt1 = time.time()\nmodel_bic.fit(X, y)\nt_bic = time.time() - t1\nalpha_bic_ = model_bic.alpha_\n\nmodel_aic = LassoLarsIC(criterion='aic')\nmodel_aic.fit(X, y)\nalpha_aic_ = model_aic.alpha_\n\n\ndef plot_ic_criterion(model, name, color):\n    alpha_ = model.alpha_\n    alphas_ = model.alphas_\n    criterion_ = model.criterion_\n    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n             linewidth=3, label='%s criterion' % name)\n    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n                label='alpha: %s estimate' % name)\n    plt.xlabel('-log(alpha)')\n    plt.ylabel('criterion')\n\nplt.figure()\nplot_ic_criterion(model_aic, 'AIC', 'b')\nplot_ic_criterion(model_bic, 'BIC', 'r')\nplt.legend()\nplt.title('Information-criterion for model selection (training time %.3fs)'\n          % t_bic)\n\n##############################################################################\n# LassoCV: coordinate descent\n\n# Compute paths\nprint("Computing regularization path using the coordinate descent lasso...")\nt1 = time.time()\nmodel = LassoCV(cv=20).fit(X, y)\nt_lasso_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.alphas_)\n\nplt.figure()\nymin, ymax = 2300, 3800\nplt.plot(m_log_alphas, model.mse_path_, ':')\nplt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n         label='Average across the folds', linewidth=2)\nplt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n            label='alpha: CV estimate')\n\nplt.legend()\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Mean square error')\nplt.title('Mean square error on each fold: coordinate descent '\n          '(train time: %.2fs)' % t_lasso_cv)\nplt.axis('tight')\nplt.ylim(ymin, ymax)\n\n##############################################################################\n# LassoLarsCV: least angle regression\n\n# Compute paths\nprint("Computing regularization path using the Lars lasso...")\nt1 = time.time()\nmodel = LassoLarsCV(cv=20).fit(X, y)\nt_lasso_lars_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.cv_alphas_)\n\nplt.figure()\nplt.plot(m_log_alphas, model.cv_mse_path_, ':')\nplt.plot(m_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',\n         label='Average across the folds', linewidth=2)\nplt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n            label='alpha CV')\nplt.legend()\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Mean square error')\nplt.title('Mean square error on each fold: Lars (train time: %.2fs)'\n          % t_lasso_lars_cv)\nplt.axis('tight')\nplt.ylim(ymin, ymax)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html
Normal and Shrinkage Linear Discriminant Analysis for classification	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_lda_001.png]]	<br><pre><code>from __future__ import division\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nn_train = 20  # samples for training\nn_test = 200  # samples for testing\nn_averages = 50  # how often to repeat classification\nn_features_max = 75  # maximum number of features\nstep = 4  # step size for the calculation\n\n\ndef generate_data(n_samples, n_features):\n    """Generate random blob-ish data with noisy features.\n\n    This returns an array of input data with shape `(n_samples, n_features)`\n    and an array of `n_samples` target labels.\n\n    Only one feature contains discriminative information, the other features\n    contain only noise.\n    """\n    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n\n    # add non-discriminative features\n    if n_features > 1:\n        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\n    return X, y\n\nacc_clf1, acc_clf2 = [], []\nn_features_range = range(1, n_features_max + 1, step)\nfor n_features in n_features_range:\n    score_clf1, score_clf2 = 0, 0\n    for _ in range(n_averages):\n        X, y = generate_data(n_train, n_features)\n\n        clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(X, y)\n        clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(X, y)\n\n        X, y = generate_data(n_test, n_features)\n        score_clf1 += clf1.score(X, y)\n        score_clf2 += clf2.score(X, y)\n\n    acc_clf1.append(score_clf1 / n_averages)\n    acc_clf2.append(score_clf2 / n_averages)\n\nfeatures_samples_ratio = np.array(n_features_range) / n_train\n\nplt.plot(features_samples_ratio, acc_clf1, linewidth=2,\n         label="Linear Discriminant Analysis with shrinkage", color='r')\nplt.plot(features_samples_ratio, acc_clf2, linewidth=2,\n         label="Linear Discriminant Analysis", color='g')\n\nplt.xlabel('n_features / n_samples')\nplt.ylabel('Classification accuracy')\n\nplt.legend(loc=1, prop={'size': 12})\nplt.suptitle('Linear Discriminant Analysis vs. \\nshrinkage Linear Discriminant Analysis (1 discriminative feature)')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html
Linear and Quadratic Discriminant Analysis with confidence ellipsoid	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_lda_qda_001.png]]	<br><pre><code>print(__doc__)\n\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n###############################################################################\n# colormap\ncmap = colors.LinearSegmentedColormap(\n    'red_blue_classes',\n    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\nplt.cm.register_cmap(cmap=cmap)\n\n\n###############################################################################\n# generate datasets\ndef dataset_fixed_cov():\n    '''Generate 2 Gaussians samples with the same covariance matrix'''\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -0.23], [0.83, .23]])\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\ndef dataset_cov():\n    '''Generate 2 Gaussians samples with different covariance matrices'''\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\n###############################################################################\n# plot functions\ndef plot_data(lda, X, y, y_pred, fig_index):\n    splot = plt.subplot(2, 2, fig_index)\n    if fig_index == 1:\n        plt.title('Linear Discriminant Analysis')\n        plt.ylabel('Data with fixed covariance')\n    elif fig_index == 2:\n        plt.title('Quadratic Discriminant Analysis')\n    elif fig_index == 3:\n        plt.ylabel('Data with varying covariances')\n\n    tp = (y == y_pred)  # True Positive\n    tp0, tp1 = tp[y == 0], tp[y == 1]\n    X0, X1 = X[y == 0], X[y == 1]\n    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n\n    # class 0: dots\n    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', color='red')\n    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '.', color='#990000')  # dark red\n\n    # class 1: dots\n    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', color='blue')\n    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '.', color='#000099')  # dark blue\n\n    # class 0 and 1 : areas\n    nx, ny = 200, 100\n    x_min, x_max = plt.xlim()\n    y_min, y_max = plt.ylim()\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n                         np.linspace(y_min, y_max, ny))\n    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n                   norm=colors.Normalize(0., 1.))\n    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n\n    # means\n    plt.plot(lda.means_[0][0], lda.means_[0][1],\n             'o', color='black', markersize=10)\n    plt.plot(lda.means_[1][0], lda.means_[1][1],\n             'o', color='black', markersize=10)\n\n    return splot\n\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = linalg.eigh(cov)\n    u = w[0] / linalg.norm(w[0])\n    angle = np.arctan(u[1] / u[0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n                              180 + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(0.5)\n    splot.add_artist(ell)\n    splot.set_xticks(())\n    splot.set_yticks(())\n\n\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n\n\ndef plot_qda_cov(qda, splot):\n    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')\n    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')\n\n###############################################################################\nfor i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n    # Linear Discriminant Analysis\n    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)\n    y_pred = lda.fit(X, y).predict(X)\n    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n    plot_lda_cov(lda, splot)\n    plt.axis('tight')\n\n    # Quadratic Discriminant Analysis\n    qda = QuadraticDiscriminantAnalysis(store_covariances=True)\n    y_pred = qda.fit(X, y).predict(X)\n    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n    plot_qda_cov(qda, splot)\n    plt.axis('tight')\nplt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html
Plotting Learning Curves	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import cross_validation\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.learning_curve import learning_curve\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    """\n    Generate a simple plot of the test and traning learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the "fit" and "predict" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : integer, cross-validation generator, optional\n        If an integer is passed, it is the number of folds (defaults to 3).\n        Specific cross-validation objects can be passed, see\n        sklearn.cross_validation module for the list of possible objects\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    """\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel("Training examples")\n    plt.ylabel("Score")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color="r")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color="g")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",\n             label="Training score")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",\n             label="Cross-validation score")\n\n    plt.legend(loc="best")\n    return plt\n\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n\ntitle = "Learning Curves (Naive Bayes)"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100,\n                                   test_size=0.2, random_state=0)\n\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = "Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,\n                                   test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2011\n\nprint(__doc__)\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n                     discriminant_analysis, random_projection)\n\ndigits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nn_neighbors = 30\n\n\n#----------------------------------------------------------------------\n# Scale and visualize the embedding vectors\ndef plot_embedding(X, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)\n\n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n                 color=plt.cm.Set1(y[i] / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    if hasattr(offsetbox, 'AnnotationBbox'):\n        # only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(digits.data.shape[0]):\n            dist = np.sum((X[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                # don't show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [X[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n                X[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n\n\n#----------------------------------------------------------------------\n# Plot images of the digits\nn_img_per_row = 20\nimg = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\nfor i in range(n_img_per_row):\n    ix = 10 * i + 1\n    for j in range(n_img_per_row):\n        iy = 10 * j + 1\n        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\nplt.imshow(img, cmap=plt.cm.binary)\nplt.xticks([])\nplt.yticks([])\nplt.title('A selection from the 64-dimensional digits dataset')\n\n\n#----------------------------------------------------------------------\n# Random 2D projection using a random unitary matrix\nprint("Computing random projection")\nrp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, "Random Projection of the digits")\n\n\n#----------------------------------------------------------------------\n# Projection on to the first 2 principal components\n\nprint("Computing PCA projection")\nt0 = time()\nX_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\nplot_embedding(X_pca,\n               "Principal Components projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Projection on to the first 2 linear discriminant components\n\nprint("Computing Linear Discriminant Analysis projection")\nX2 = X.copy()\nX2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\nt0 = time()\nX_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)\nplot_embedding(X_lda,\n               "Linear Discriminant projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Isomap projection of the digits dataset\nprint("Computing Isomap embedding")\nt0 = time()\nX_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\nprint("Done.")\nplot_embedding(X_iso,\n               "Isomap projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Locally linear embedding of the digits dataset\nprint("Computing LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='standard')\nt0 = time()\nX_lle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_lle,\n               "Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Modified Locally linear embedding of the digits dataset\nprint("Computing modified LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='modified')\nt0 = time()\nX_mlle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_mlle,\n               "Modified Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# HLLE embedding of the digits dataset\nprint("Computing Hessian LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='hessian')\nt0 = time()\nX_hlle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_hlle,\n               "Hessian Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# LTSA embedding of the digits dataset\nprint("Computing LTSA embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='ltsa')\nt0 = time()\nX_ltsa = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_ltsa,\n               "Local Tangent Space Alignment of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# MDS  embedding of the digits dataset\nprint("Computing MDS embedding")\nclf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\nt0 = time()\nX_mds = clf.fit_transform(X)\nprint("Done. Stress: %f" % clf.stress_)\nplot_embedding(X_mds,\n               "MDS embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Random Trees embedding of the digits dataset\nprint("Computing Totally Random Trees embedding")\nhasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n                                       max_depth=5)\nt0 = time()\nX_transformed = hasher.fit_transform(X)\npca = decomposition.TruncatedSVD(n_components=2)\nX_reduced = pca.fit_transform(X_transformed)\n\nplot_embedding(X_reduced,\n               "Random forest embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Spectral embedding of the digits dataset\nprint("Computing Spectral embedding")\nembedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n                                      eigen_solver="arpack")\nt0 = time()\nX_se = embedder.fit_transform(X)\n\nplot_embedding(X_se,\n               "Spectral embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# t-SNE embedding of the digits dataset\nprint("Computing t-SNE embedding")\ntsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\nt0 = time()\nX_tsne = tsne.fit_transform(X)\n\nplot_embedding(X_tsne,\n               "t-SNE embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
Logit function	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_logistic_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\n# this is our test set, it's just a straight line with some\n# Gaussian noise\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(np.float)\nX[X > 0] *= 4\nX += .3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n# run the classifier\nclf = linear_model.LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# and plot the result\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color='black', zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\n\ndef model(x):\n    return 1 / (1 + np.exp(-x))\nloss = model(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color='blue', linewidth=3)\n\nols = linear_model.LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(.5, color='.5')\n\nplt.ylabel('y')\nplt.xlabel('X')\nplt.xticks(())\nplt.yticks(())\nplt.ylim(-.25, 1.25)\nplt.xlim(-4, 10)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html
L1 Penalty and Sparsity in Logistic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_logistic_l1_l2_sparsity_001.png]]	<br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits = datasets.load_digits()\n\nX, y = digits.data, digits.target\nX = StandardScaler().fit_transform(X)\n\n# classify small against large digits\ny = (y > 4).astype(np.int)\n\n\n# Set regularization parameter\nfor i, C in enumerate((100, 1, 0.01)):\n    # turn down tolerance for short training time\n    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n\n    print("C=%.2f" % C)\n    print("Sparsity with L1 penalty: %.2f%%" % sparsity_l1_LR)\n    print("score with L1 penalty: %.4f" % clf_l1_LR.score(X, y))\n    print("Sparsity with L2 penalty: %.2f%%" % sparsity_l2_LR)\n    print("score with L2 penalty: %.4f" % clf_l2_LR.score(X, y))\n\n    l1_plot = plt.subplot(3, 2, 2 * i + 1)\n    l2_plot = plt.subplot(3, 2, 2 * (i + 1))\n    if i == 0:\n        l1_plot.set_title("L1 penalty")\n        l2_plot.set_title("L2 penalty")\n\n    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    plt.text(-8, 3, "C = %.2f" % C)\n\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l2_plot.set_xticks(())\n    l2_plot.set_yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html
Path with L1- Logistic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_logistic_path_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.svm import l1_min_c\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX -= np.mean(X, 0)\n\n###############################################################################\n# Demo path functions\n\ncs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)\n\n\nprint("Computing regularization path ...")\nstart = datetime.now()\nclf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\nprint("This took ", datetime.now() - start)\n\ncoefs_ = np.array(coefs_)\nplt.plot(np.log10(cs), coefs_)\nymin, ymax = plt.ylim()\nplt.xlabel('log(C)')\nplt.ylabel('Coefficients')\nplt.title('Logistic Regression Path')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html
Ledoit-Wolf vs OAS estimation	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_lw_vs_oas_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz, cholesky\n\nfrom sklearn.covariance import LedoitWolf, OAS\n\nnp.random.seed(0)\n###############################################################################\nn_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = toeplitz(r ** np.arange(n_features))\ncoloring_matrix = cholesky(real_cov)\n\nn_samples_range = np.arange(6, 31, 1)\nrepeat = 100\nlw_mse = np.zeros((n_samples_range.size, repeat))\noa_mse = np.zeros((n_samples_range.size, repeat))\nlw_shrinkage = np.zeros((n_samples_range.size, repeat))\noa_shrinkage = np.zeros((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = np.dot(\n            np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = LedoitWolf(store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = OAS(store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\nplt.subplot(2, 1, 1)\nplt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),\n             label='Ledoit-Wolf', color='g')\nplt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),\n             label='OAS', color='r')\nplt.ylabel("Squared error")\nplt.legend(loc="upper right")\nplt.title("Comparison of covariance estimators")\nplt.xlim(5, 31)\n\n# plot shrinkage coefficient\nplt.subplot(2, 1, 2)\nplt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),\n             label='Ledoit-Wolf', color='g')\nplt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),\n             label='OAS', color='r')\nplt.xlabel("n_samples")\nplt.ylabel("Shrinkage")\nplt.legend(loc="lower right")\nplt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)\nplt.xlim(5, 31)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_lw_vs_oas.html
Robust covariance estimation and Mahalanobis distances relevance	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_mahalanobis_distances_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nn_samples = 125\nn_outliers = 25\nn_features = 2\n\n# generate data\ngen_cov = np.eye(n_features)\ngen_cov[0, 0] = 2.\nX = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n# add some outliers\noutliers_cov = np.eye(n_features)\noutliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.\nX[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\n\n# fit a Minimum Covariance Determinant (MCD) robust estimator to data\nrobust_cov = MinCovDet().fit(X)\n\n# compare estimators learnt from the full data set with true parameters\nemp_cov = EmpiricalCovariance().fit(X)\n\n###############################################################################\n# Display results\nfig = plt.figure()\nplt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)\n\n# Show data set\nsubfig1 = plt.subplot(3, 1, 1)\ninlier_plot = subfig1.scatter(X[:, 0], X[:, 1],\n                              color='black', label='inliers')\noutlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],\n                               color='red', label='outliers')\nsubfig1.set_xlim(subfig1.get_xlim()[0], 11.)\nsubfig1.set_title("Mahalanobis distances of a contaminated data set:")\n\n# Show contours of the distance functions\nxx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\nzz = np.c_[xx.ravel(), yy.ravel()]\n\nmahal_emp_cov = emp_cov.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),\n                                  cmap=plt.cm.PuBu_r,\n                                  linestyles='dashed')\n\nmahal_robust_cov = robust_cov.mahalanobis(zz)\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\nrobust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),\n                                 cmap=plt.cm.YlOrBr_r, linestyles='dotted')\n\nsubfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],\n                inlier_plot, outlier_plot],\n               ['MLE dist', 'robust dist', 'inliers', 'outliers'],\n               loc="upper right", borderaxespad=0)\nplt.xticks(())\nplt.yticks(())\n\n# Plot the scores for each point\nemp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)\nsubfig2 = plt.subplot(2, 2, 3)\nsubfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)\nsubfig2.plot(1.26 * np.ones(n_samples - n_outliers),\n             emp_mahal[:-n_outliers], '+k', markeredgewidth=1)\nsubfig2.plot(2.26 * np.ones(n_outliers),\n             emp_mahal[-n_outliers:], '+k', markeredgewidth=1)\nsubfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)\nsubfig2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)\nsubfig2.set_title("1. from non-robust estimates\n(Maximum Likelihood)")\nplt.yticks(())\n\nrobust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\nsubfig3 = plt.subplot(2, 2, 4)\nsubfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],\n                widths=.25)\nsubfig3.plot(1.26 * np.ones(n_samples - n_outliers),\n             robust_mahal[:-n_outliers], '+k', markeredgewidth=1)\nsubfig3.plot(2.26 * np.ones(n_outliers),\n             robust_mahal[-n_outliers:], '+k', markeredgewidth=1)\nsubfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)\nsubfig3.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)\nsubfig3.set_title("2. from robust estimates\n(Minimum Covariance Determinant)")\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html
Manifold Learning methods on a severed sphere	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_manifold_sphere_001.png]]	<br><pre><code># Author: Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nfrom sklearn.utils import check_random_state\n\n# Next line to silence pyflakes.\nAxes3D\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = check_random_state(0)\np = random_state.rand(n_samples) * (2 * np.pi - 0.55)\nt = random_state.rand(n_samples) * np.pi\n\n# Sever the poles from the sphere.\nindices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))\ncolors = p[indices]\nx, y, z = np.sin(t[indices]) * np.cos(p[indices]), \\n    np.sin(t[indices]) * np.sin(p[indices]), \\n    np.cos(t[indices])\n\n# Plot our dataset.\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle("Manifold Learning with %i points, %i neighbors"\n             % (1000, n_neighbors), fontsize=14)\n\nax = fig.add_subplot(251, projection='3d')\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\ntry:\n    # compatibility matplotlib < 1.0\n    ax.view_init(40, -10)\nexcept:\n    pass\n\nsphere_data = np.array([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    trans_data = manifold\\n        .LocallyLinearEmbedding(n_neighbors, 2,\n                                method=method).fit_transform(sphere_data).T\n    t1 = time()\n    print("%s: %.2g sec" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n\n# Perform Isomap Manifold learning.\nt0 = time()\ntrans_data = manifold.Isomap(n_neighbors, n_components=2)\\n    .fit_transform(sphere_data).T\nt1 = time()\nprint("%s: %.2g sec" % ('ISO', t1 - t0))\n\nax = fig.add_subplot(257)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("%s (%.2g sec)" % ('Isomap', t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform Multi-dimensional scaling.\nt0 = time()\nmds = manifold.MDS(2, max_iter=100, n_init=1)\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = time()\nprint("MDS: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(258)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("MDS (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform Spectral Embedding.\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=2,\n                                n_neighbors=n_neighbors)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = time()\nprint("Spectral Embedding: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(259)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("Spectral Embedding (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = time()\ntsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = time()\nprint("t-SNE: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("t-SNE (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html
Multi-dimensional scaling	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_mds_001.png]]	<br><pre><code># Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# Licence: BSD\n\nprint(__doc__)\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.decomposition import PCA\n\nn_samples = 20\nseed = np.random.RandomState(seed=3)\nX_true = seed.randint(0, 20, 2 * n_samples).astype(np.float)\nX_true = X_true.reshape((n_samples, 2))\n# Center the data\nX_true -= X_true.mean()\n\nsimilarities = euclidean_distances(X_true)\n\n# Add noise to the similarities\nnoise = np.random.rand(n_samples, n_samples)\nnoise = noise + noise.T\nnoise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0\nsimilarities += noise\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,\n                   dissimilarity="precomputed", n_jobs=1)\npos = mds.fit(similarities).embedding_\n\nnmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,\n                    dissimilarity="precomputed", random_state=seed, n_jobs=1,\n                    n_init=1)\nnpos = nmds.fit_transform(similarities, init=pos)\n\n# Rescale the data\npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\nnpos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n\n# Rotate the data\nclf = PCA(n_components=2)\nX_true = clf.fit_transform(X_true)\n\npos = clf.fit_transform(pos)\n\nnpos = clf.fit_transform(npos)\n\nfig = plt.figure(1)\nax = plt.axes([0., 0., 1., 1.])\n\nplt.scatter(X_true[:, 0], X_true[:, 1], c='r', s=20)\nplt.scatter(pos[:, 0], pos[:, 1], s=20, c='g')\nplt.scatter(npos[:, 0], npos[:, 1], s=20, c='b')\nplt.legend(('True position', 'MDS', 'NMDS'), loc='best')\n\nsimilarities = similarities.max() / similarities * 100\nsimilarities[np.isinf(similarities)] = 0\n\n# Plot the edges\nstart_idx, end_idx = np.where(pos)\n#a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[X_true[i, :], X_true[j, :]]\n            for i in range(len(pos)) for j in range(len(pos))]\nvalues = np.abs(similarities)\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, values.max()))\nlc.set_array(similarities.flatten())\nlc.set_linewidths(0.5 * np.ones(len(segments)))\nax.add_collection(lc)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html
A demo of the mean-shift clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_mean_shift_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom sklearn.datasets.samples_generator import make_blobs\n\n###############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)\n\n###############################################################################\n# Compute clustering with MeanShift\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\n\nprint("number of estimated clusters : %d" % n_clusters_)\n\n###############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    my_members = labels == k\n    cluster_center = cluster_centers[k]\n    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html
Comparison of the K-Means and MiniBatchKMeans clustering algorithms	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_mini_batch_kmeans_001.png]]	<br><pre><code>print(__doc__)\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\nfrom sklearn.datasets.samples_generator import make_blobs\n\n##############################################################################\n# Generate sample data\nnp.random.seed(0)\n\nbatch_size = 45\ncenters = [[1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nX, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n\n##############################################################################\n# Compute clustering with Means\n\nk_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\nt0 = time.time()\nk_means.fit(X)\nt_batch = time.time() - t0\nk_means_labels = k_means.labels_\nk_means_cluster_centers = k_means.cluster_centers_\nk_means_labels_unique = np.unique(k_means_labels)\n\n##############################################################################\n# Compute clustering with MiniBatchKMeans\n\nmbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,\n                      n_init=10, max_no_improvement=10, verbose=0)\nt0 = time.time()\nmbk.fit(X)\nt_mini_batch = time.time() - t0\nmbk_means_labels = mbk.labels_\nmbk_means_cluster_centers = mbk.cluster_centers_\nmbk_means_labels_unique = np.unique(mbk_means_labels)\n\n##############################################################################\n# Plot result\n\nfig = plt.figure(figsize=(8, 3))\nfig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06']\n\n# We want to have the same colors for the same cluster from the\n# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per\n# closest one.\n\norder = pairwise_distances_argmin(k_means_cluster_centers,\n                                  mbk_means_cluster_centers)\n\n# KMeans\nax = fig.add_subplot(1, 3, 1)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = k_means_labels == k\n    cluster_center = k_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n            markerfacecolor=col, marker='.')\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n            markeredgecolor='k', markersize=6)\nax.set_title('KMeans')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8,  'train time: %.2fs\ninertia: %f' % (\n    t_batch, k_means.inertia_))\n\n# MiniBatchKMeans\nax = fig.add_subplot(1, 3, 2)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = mbk_means_labels == order[k]\n    cluster_center = mbk_means_cluster_centers[order[k]]\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n            markerfacecolor=col, marker='.')\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n            markeredgecolor='k', markersize=6)\nax.set_title('MiniBatchKMeans')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8, 'train time: %.2fs\ninertia: %f' %\n         (t_mini_batch, mbk.inertia_))\n\n# Initialise the different array to all False\ndifferent = (mbk_means_labels == 4)\nax = fig.add_subplot(1, 3, 3)\n\nfor l in range(n_clusters):\n    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))\n\nidentic = np.logical_not(different)\nax.plot(X[identic, 0], X[identic, 1], 'w',\n        markerfacecolor='#bbbbbb', marker='.')\nax.plot(X[different, 0], X[different, 1], 'w',\n        markerfacecolor='m', marker='.')\nax.set_title('Difference')\nax.set_xticks(())\nax.set_yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html
Model Complexity Influence	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.parasite_axes import host_subplot\nfrom mpl_toolkits.axisartist.axislines import Axes\nfrom scipy.sparse.csr import csr_matrix\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm.classes import NuSVR\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.metrics import hamming_loss\n\n###############################################################################\n# Routines\n\n\n# initialize random generator\nnp.random.seed(0)\n\n\ndef generate_data(case, sparse=False):\n    """Generate regression/classification data."""\n    bunch = None\n    if case == 'regression':\n        bunch = datasets.load_boston()\n    elif case == 'classification':\n        bunch = datasets.fetch_20newsgroups_vectorized(subset='all')\n    X, y = shuffle(bunch.data, bunch.target)\n    offset = int(X.shape[0] * 0.8)\n    X_train, y_train = X[:offset], y[:offset]\n    X_test, y_test = X[offset:], y[offset:]\n    if sparse:\n        X_train = csr_matrix(X_train)\n        X_test = csr_matrix(X_test)\n    else:\n        X_train = np.array(X_train)\n        X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    y_train = np.array(y_train)\n    data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,\n            'y_test': y_test}\n    return data\n\n\ndef benchmark_influence(conf):\n    """\n    Benchmark influence of :changing_param: on both MSE and latency.\n    """\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf['changing_param_values']:\n        conf['tuned_params'][conf['changing_param']] = param_value\n        estimator = conf['estimator'](**conf['tuned_params'])\n        print("Benchmarking %s" % estimator)\n        estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n        conf['postfit_hook'](estimator)\n        complexity = conf['complexity_computer'](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf['n_samples']):\n            y_pred = estimator.predict(conf['data']['X_test'])\n        elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n        prediction_times.append(elapsed_time)\n        pred_score = conf['prediction_performance_computer'](\n            conf['data']['y_test'], y_pred)\n        prediction_powers.append(pred_score)\n        print("Complexity: %d | %s: %.4f | Pred. Time: %fs\n" % (\n            complexity, conf['prediction_performance_label'], pred_score,\n            elapsed_time))\n    return prediction_powers, prediction_times, complexities\n\n\ndef plot_influence(conf, mse_values, prediction_times, complexities):\n    """\n    Plot influence of model complexity on both accuracy and latency.\n    """\n    plt.figure(figsize=(12, 6))\n    host = host_subplot(111, axes_class=Axes)\n    plt.subplots_adjust(right=0.75)\n    par1 = host.twinx()\n    host.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])\n    y1_label = conf['prediction_performance_label']\n    y2_label = "Time (s)"\n    host.set_ylabel(y1_label)\n    par1.set_ylabel(y2_label)\n    p1, = host.plot(complexities, mse_values, 'b-', label="prediction error")\n    p2, = par1.plot(complexities, prediction_times, 'r-',\n                    label="latency")\n    host.legend(loc='upper right')\n    host.axis["left"].label.set_color(p1.get_color())\n    par1.axis["right"].label.set_color(p2.get_color())\n    plt.title('Influence of Model Complexity - %s' % conf['estimator'].__name__)\n    plt.show()\n\n\ndef _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return np.count_nonzero(a)\n\n###############################################################################\n# main code\nregression_data = generate_data('regression')\nclassification_data = generate_data('classification', sparse=True)\nconfigurations = [\n    {'estimator': SGDClassifier,\n     'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':\n                      'modified_huber', 'fit_intercept': True},\n     'changing_param': 'l1_ratio',\n     'changing_param_values': [0.25, 0.5, 0.75, 0.9],\n     'complexity_label': 'non_zero coefficients',\n     'complexity_computer': _count_nonzero_coefficients,\n     'prediction_performance_computer': hamming_loss,\n     'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',\n     'postfit_hook': lambda x: x.sparsify(),\n     'data': classification_data,\n     'n_samples': 30},\n    {'estimator': NuSVR,\n     'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},\n     'changing_param': 'nu',\n     'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],\n     'complexity_label': 'n_support_vectors',\n     'complexity_computer': lambda x: len(x.support_vectors_),\n     'data': regression_data,\n     'postfit_hook': lambda x: x,\n     'prediction_performance_computer': mean_squared_error,\n     'prediction_performance_label': 'MSE',\n     'n_samples': 30},\n    {'estimator': GradientBoostingRegressor,\n     'tuned_params': {'loss': 'ls'},\n     'changing_param': 'n_estimators',\n     'changing_param_values': [10, 50, 100, 200, 500],\n     'complexity_label': 'n_trees',\n     'complexity_computer': lambda x: x.n_estimators,\n     'data': regression_data,\n     'postfit_hook': lambda x: x,\n     'prediction_performance_computer': mean_squared_error,\n     'prediction_performance_label': 'MSE',\n     'n_samples': 30},\n]\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = \\n        benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times,\n                   complexities)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html
Joint feature selection with multi-task Lasso	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import MultiTaskLasso, Lasso\n\nrng = np.random.RandomState(42)\n\n# Generate some 2D coefficients with sine waves with random frequency and phase\nn_samples, n_features, n_tasks = 100, 30, 40\nn_relevant_features = 5\ncoef = np.zeros((n_tasks, n_features))\ntimes = np.linspace(0, 2 * np.pi, n_tasks)\nfor k in range(n_relevant_features):\n    coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))\n\nX = rng.randn(n_samples, n_features)\nY = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)\n\ncoef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])\ncoef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_\n\n###############################################################################\n# Plot support and time series\nfig = plt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\nplt.spy(coef_lasso_)\nplt.xlabel('Feature')\nplt.ylabel('Time (or Task)')\nplt.text(10, 5, 'Lasso')\nplt.subplot(1, 2, 2)\nplt.spy(coef_multi_task_lasso_)\nplt.xlabel('Feature')\nplt.ylabel('Time (or Task)')\nplt.text(10, 5, 'MultiTaskLasso')\nfig.suptitle('Coefficient non-zero location')\n\nfeature_to_plot = 0\nplt.figure()\nplt.plot(coef[:, feature_to_plot], 'k', label='Ground truth')\nplt.plot(coef_lasso_[:, feature_to_plot], 'g', label='Lasso')\nplt.plot(coef_multi_task_lasso_[:, feature_to_plot],\n         'r', label='MultiTaskLasso')\nplt.legend(loc='upper center')\nplt.axis('tight')\nplt.ylim([-1.1, 1.1])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html
Multilabel classification	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_multilabel_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import CCA\n\n\ndef plot_hyperplane(clf, min_x, max_x, linestyle, label):\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n    plt.plot(xx, yy, linestyle, label=label)\n\n\ndef plot_subfigure(X, Y, subplot, title, transform):\n    if transform == "pca":\n        X = PCA(n_components=2).fit_transform(X)\n    elif transform == "cca":\n        X = CCA(n_components=2).fit(X, Y).transform(X)\n    else:\n        raise ValueError\n\n    min_x = np.min(X[:, 0])\n    max_x = np.max(X[:, 0])\n\n    min_y = np.min(X[:, 1])\n    max_y = np.max(X[:, 1])\n\n    classif = OneVsRestClassifier(SVC(kernel='linear'))\n    classif.fit(X, Y)\n\n    plt.subplot(2, 2, subplot)\n    plt.title(title)\n\n    zero_class = np.where(Y[:, 0])\n    one_class = np.where(Y[:, 1])\n    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray')\n    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n               facecolors='none', linewidths=2, label='Class 1')\n    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n               facecolors='none', linewidths=2, label='Class 2')\n\n    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n                    'Boundary\nfor class 1')\n    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n                    'Boundary\nfor class 2')\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n    if subplot == 2:\n        plt.xlabel('First principal component')\n        plt.ylabel('Second principal component')\n        plt.legend(loc="upper left")\n\n\nplt.figure(figsize=(8, 6))\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=True,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 1, "With unlabeled samples + CCA", "cca")\nplot_subfigure(X, Y, 2, "With unlabeled samples + PCA", "pca")\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=False,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 3, "Without unlabeled samples + CCA", "cca")\nplot_subfigure(X, Y, 4, "Without unlabeled samples + PCA", "pca")\n\nplt.subplots_adjust(.04, .02, .97, .94, .09, .2)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_multilabel.html
Face completion with a multi-output estimators	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_multioutput_face_completion_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.utils.validation import check_random_state\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV\n\n# Load the faces datasets\ndata = fetch_olivetti_faces()\ntargets = data.target\n\ndata = data.images.reshape((len(data.images), -1))\ntrain = data[targets < 30]\ntest = data[targets >= 30]  # Test on independent people\n\n# Test on a subset of people\nn_faces = 5\nrng = check_random_state(4)\nface_ids = rng.randint(test.shape[0], size=(n_faces, ))\ntest = test[face_ids, :]\n\nn_pixels = data.shape[1]\nX_train = train[:, :np.ceil(0.5 * n_pixels)]  # Upper half of the faces\ny_train = train[:, np.floor(0.5 * n_pixels):]  # Lower half of the faces\nX_test = test[:, :np.ceil(0.5 * n_pixels)]\ny_test = test[:, np.floor(0.5 * n_pixels):]\n\n# Fit estimators\nESTIMATORS = {\n    "Extra trees": ExtraTreesRegressor(n_estimators=10, max_features=32,\n                                       random_state=0),\n    "K-nn": KNeighborsRegressor(),\n    "Linear regression": LinearRegression(),\n    "Ridge": RidgeCV(),\n}\n\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n\n# Plot the completed faces\nimage_shape = (64, 64)\n\nn_cols = 1 + len(ESTIMATORS)\nplt.figure(figsize=(2. * n_cols, 2.26 * n_faces))\nplt.suptitle("Face completion with multi-output estimators", size=16)\n\nfor i in range(n_faces):\n    true_face = np.hstack((X_test[i], y_test[i]))\n\n    if i:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,\n                          title="true faces")\n\n\n    sub.axis("off")\n    sub.imshow(true_face.reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation="nearest")\n\n    for j, est in enumerate(sorted(ESTIMATORS)):\n        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n\n        if i:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n\n        else:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,\n                              title=est)\n\n        sub.axis("off")\n        sub.imshow(completed_face.reshape(image_shape),\n                   cmap=plt.cm.gray,\n                   interpolation="nearest")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html
Nearest Centroid Classification	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestCentroid\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\nfor shrinkage in [None, 0.1]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = NearestCentroid(shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, np.mean(y == y_pred))\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.title("3-Class classification (shrink_threshold=%r)"\n              % shrinkage)\n    plt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html
Linear Regression Example	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ols_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# The coefficients\nprint('Coefficients: \n', regr.coef_)\n# The mean square error\nprint("Residual sum of squares: %.2f"\n      % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue',\n         linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html
Sparsity Example: Fitting only features 1  and 2	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn import datasets, linear_model\n\ndiabetes = datasets.load_diabetes()\nindices = (0, 1)\n\nX_train = diabetes.data[:-20, indices]\nX_test = diabetes.data[-20:, indices]\ny_train = diabetes.target[:-20]\ny_test = diabetes.target[-20:]\n\nols = linear_model.LinearRegression()\nols.fit(X_train, y_train)\n\n\n###############################################################################\n# Plot the figure\ndef plot_figs(fig_num, elev, azim, X_train, clf):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, elev=elev, azim=azim)\n\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c='k', marker='+')\n    ax.plot_surface(np.array([[-.1, -.1], [.15, .15]]),\n                    np.array([[-.1, .15], [-.1, .15]]),\n                    clf.predict(np.array([[-.1, -.1, .15, .15],\n                                          [-.1, .15, -.1, .15]]).T\n                                ).reshape((2, 2)),\n                    alpha=.5)\n    ax.set_xlabel('X_1')\n    ax.set_ylabel('X_2')\n    ax.set_zlabel('Y')\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n#Generate the three different figures from different views\nelev = 43.5\nazim = -110\nplot_figs(1, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 0\nplot_figs(2, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 90\nplot_figs(3, elev, azim, X_train, ols)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_3d.html
Ordinary Least Squares and Ridge Regression Variance	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = np.c_[.5, 1].T\ny_train = [.5, 1]\nX_test = np.c_[0, 2].T\n\nnp.random.seed(0)\n\nclassifiers = dict(ols=linear_model.LinearRegression(),\n                   ridge=linear_model.Ridge(alpha=.1))\n\nfignum = 1\nfor name, clf in classifiers.items():\n    fig = plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.title(name)\n    ax = plt.axes([.12, .12, .8, .8])\n\n    for _ in range(6):\n        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color='.5')\n        ax.scatter(this_X, y_train, s=3, c='.5', marker='o', zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n    ax.scatter(X_train, y_train, s=30, c='r', marker='+', zorder=10)\n\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel('X')\n    ax.set_ylabel('y')\n    ax.set_xlim(0, 2)\n    fignum += 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html
Orthogonal Matching Pursuit	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_omp_001.png]]	<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import OrthogonalMatchingPursuitCV\nfrom sklearn.datasets import make_sparse_coded_signal\n\nn_components, n_features = 512, 100\nn_nonzero_coefs = 17\n\n# generate the data\n###################\n\n# y = Xw\n# |x|_0 = n_nonzero_coefs\n\ny, X, w = make_sparse_coded_signal(n_samples=1,\n                                   n_components=n_components,\n                                   n_features=n_features,\n                                   n_nonzero_coefs=n_nonzero_coefs,\n                                   random_state=0)\n\nidx, = w.nonzero()\n\n# distort the clean signal\n##########################\ny_noisy = y + 0.05 * np.random.randn(len(y))\n\n# plot the sparse signal\n########################\nplt.figure(figsize=(7, 7))\nplt.subplot(4, 1, 1)\nplt.xlim(0, 512)\nplt.title("Sparse signal")\nplt.stem(idx, w[idx])\n\n# plot the noise-free reconstruction\n####################################\n\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\nomp.fit(X, y)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 2)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noise-free measurements")\nplt.stem(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction\n###############################\nomp.fit(X, y_noisy)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 3)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noisy measurements")\nplt.stem(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction with number of non-zeros set by CV\n##################################################################\nomp_cv = OrthogonalMatchingPursuitCV()\nomp_cv.fit(X, y_noisy)\ncoef = omp_cv.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 4)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noisy measurements with CV")\nplt.stem(idx_r, coef[idx_r])\n\nplt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\nplt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',\n             fontsize=16)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html
One-class SVM with non-linear kernel (RBF)	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_oneclass_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n# Generate train data\nX = 0.3 * np.random.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * np.random.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the line, the points, and the nearest vectors to the plane\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title("Novelty Detection")\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\nplt.axis('tight')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([a.collections[0], b1, b2, c],\n           ["learned frontier", "training observations",\n            "new regular observations", "new abnormal observations"],\n           loc="upper left",\n           prop=matplotlib.font_manager.FontProperties(size=11))\nplt.xlabel(\n    "error train: %d/200 ; errors novel regular: %d/40 ; "\n    "errors novel abnormal: %d/40"\n    % (n_error_train, n_error_test, n_error_outliers))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html
Out-of-core classification of text documents	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Eustache Diemert <eustache@diemert.fr>\n#          @FedericoV <https://github.com/FedericoV/>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom glob import glob\nimport itertools\nimport os.path\nimport re\nimport tarfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom sklearn.externals.six.moves import html_parser\nfrom sklearn.externals.six.moves import urllib\nfrom sklearn.datasets import get_data_home\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import MultinomialNB\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return '__file__' in globals()\n\n\n###############################################################################\n# Reuters Dataset related routines\n###############################################################################\n\n\nclass ReutersParser(html_parser.HTMLParser):\n    """Utility class to parse a SGML file and yield documents one at a time."""\n\n    def __init__(self, encoding='latin-1'):\n        html_parser.HTMLParser.__init__(self)\n        self._reset()\n        self.encoding = encoding\n\n    def handle_starttag(self, tag, attrs):\n        method = 'start_' + tag\n        getattr(self, method, lambda x: None)(attrs)\n\n    def handle_endtag(self, tag):\n        method = 'end_' + tag\n        getattr(self, method, lambda: None)()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = ""\n        self.body = ""\n        self.topics = []\n        self.topic_d = ""\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk.decode(self.encoding))\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def start_reuters(self, attributes):\n        pass\n\n    def end_reuters(self):\n        self.body = re.sub(r'\s+', r' ', self.body)\n        self.docs.append({'title': self.title,\n                          'body': self.body,\n                          'topics': self.topics})\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = ""\n\n\ndef stream_reuters_documents(data_path=None):\n    """Iterate over documents of the Reuters dataset.\n\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n\n    Documents are represented as dictionaries with 'body' (str),\n    'title' (str), 'topics' (list(str)) keys.\n\n    """\n\n    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n                    'reuters21578-mld/reuters21578.tar.gz')\n    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n\n    if data_path is None:\n        data_path = os.path.join(get_data_home(), "reuters")\n    if not os.path.exists(data_path):\n        """Download the dataset."""\n        print("downloading dataset (once and for all) into %s" %\n              data_path)\n        os.mkdir(data_path)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = '%.2f MB' % (size / 1e6)\n            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                print('\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n                      end='')\n\n        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n                                   reporthook=progress)\n        if _not_in_sphinx():\n            print('\r', end='')\n        print("untarring Reuters dataset...")\n        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n        print("done.")\n\n    parser = ReutersParser()\n    for filename in glob(os.path.join(data_path, "*.sgm")):\n        for doc in parser.parse(open(filename, 'rb')):\n            yield doc\n\n\n###############################################################################\n# Main\n###############################################################################\n# Create the vectorizer and limit the number of features to a reasonable\n# maximum\nvectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n                               non_negative=True)\n\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the "acq" class and all the others.\n# "acq" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = np.array([0, 1])\npositive_class = 'acq'\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    'SGD': SGDClassifier(),\n    'Perceptron': Perceptron(),\n    'NB Multinomial': MultinomialNB(alpha=0.01),\n    'Passive-Aggressive': PassiveAggressiveClassifier(),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    """Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    """\n    data = [(u'{title}\n\n{body}'.format(**doc), pos_class in doc['topics'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc['topics']]\n    if not len(data):\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, np.asarray(y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    """Generator of minibatches."""\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {'n_test': 0, 'n_test_pos': 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = time.time()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = time.time() - tick\ntick = time.time()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = time.time() - tick\ntest_stats['n_test'] += len(y_test)\ntest_stats['n_test_pos'] += sum(y_test)\nprint("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    """Report progress information, return a string."""\n    duration = time.time() - stats['t0']\n    s = "%20s classifier : \t" % cls_name\n    s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats\n    s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats\n    s += "accuracy: %(accuracy).3f " % stats\n    s += "in %.2fs (%5d docs/s)" % (duration, stats['n_train'] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {'n_train': 0, 'n_train_pos': 0,\n             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batchs of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = time.time()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += time.time() - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = time.time()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n        tick = time.time()\n        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n        acc_history = (cls_stats[cls_name]['accuracy'],\n                       cls_stats[cls_name]['n_train'])\n        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n        run_history = (cls_stats[cls_name]['accuracy'],\n                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n        cls_stats[cls_name]['runtime_history'].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print('\n')\n\n\n###############################################################################\n# Plot results\n###############################################################################\n\n\ndef plot_accuracy(x, y, x_legend):\n    """Plot accuracy as a function of x."""\n    x = np.array(x)\n    y = np.array(y)\n    plt.title('Classification accuracy as a function of %s' % x_legend)\n    plt.xlabel('%s' % x_legend)\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.plot(x, y)\n\nrcParams['legend.fontsize'] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats['accuracy_history'])\n    plot_accuracy(n_examples, accuracy, "training examples (#)")\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc='best')\n\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats['runtime_history'])\n    plot_accuracy(runtime, accuracy, 'runtime (s)')\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc='best')\n\n# Plot fitting times\nplt.figure()\nfig = plt.gcf()\ncls_runtime = []\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats['total_fit_time'])\n\ncls_runtime.append(total_vect_time)\ncls_names.append('Vectorization')\nbar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel('runtime (s)')\nax.set_title('Training Times')\n\n\ndef autolabel(rectangles):\n    """attach some text vi autolabel on rectangles."""\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width() / 2.,\n                1.05 * height, '%.4f' % height,\n                ha='center', va='bottom')\n\nautolabel(rectangles)\nplt.show()\n\n# Plot prediction times\nplt.figure()\n#fig = plt.gcf()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats['prediction_time'])\ncls_runtime.append(parsing_time)\ncls_names.append('Read/Parse\n+Feat.Extr.')\ncls_runtime.append(vectorizing_time)\ncls_names.append('Hashing\n+Vect.')\nbar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\nplt.setp(plt.xticks()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel('runtime (s)')\nax.set_title('Prediction Times (%d instances)' % n_test_documents)\nautolabel(rectangles)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html
Outlier detection with several methods.	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom scipy import stats\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\n\n# Example settings\nn_samples = 200\noutliers_fraction = 0.25\nclusters_separation = [0, 1, 2]\n\n# define two outlier detection tools to be compared\nclassifiers = {\n    "One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                     kernel="rbf", gamma=0.1),\n    "robust covariance estimator": EllipticEnvelope(contamination=.1)}\n\n# Compare given classifiers under given settings\nxx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.ones(n_samples, dtype=int)\nground_truth[-n_outliers:] = 0\n\n# Fit the problem with varying cluster separation\nfor i, offset in enumerate(clusters_separation):\n    np.random.seed(42)\n    # Data generation\n    X1 = 0.3 * np.random.randn(0.5 * n_inliers, 2) - offset\n    X2 = 0.3 * np.random.randn(0.5 * n_inliers, 2) + offset\n    X = np.r_[X1, X2]\n    # Add outliers\n    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]\n\n    # Fit the model with the One-Class SVM\n    plt.figure(figsize=(10, 5))\n    for i, (clf_name, clf) in enumerate(classifiers.items()):\n        # fit the data and tag outliers\n        clf.fit(X)\n        y_pred = clf.decision_function(X).ravel()\n        threshold = stats.scoreatpercentile(y_pred,\n                                            100 * outliers_fraction)\n        y_pred = y_pred > threshold\n        n_errors = (y_pred != ground_truth).sum()\n        # plot the levels lines and the points\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        subplot = plt.subplot(1, 2, i + 1)\n        subplot.set_title("Outlier detection")\n        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                         cmap=plt.cm.Blues_r)\n        a = subplot.contour(xx, yy, Z, levels=[threshold],\n                            linewidths=2, colors='red')\n        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                         colors='orange')\n        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n        subplot.axis('tight')\n        subplot.legend(\n            [a.collections[0], b, c],\n            ['learned decision function', 'true inliers', 'true outliers'],\n            prop=matplotlib.font_manager.FontProperties(size=11))\n        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))\n        subplot.set_xlim((-7, 7))\n        subplot.set_ylim((-7, 7))\n    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html
Outlier detection on a real data set	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Virgile Fritsch <virgile.fritsch@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.svm import OneClassSVM\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn.datasets import load_boston\n\n# Get data\nX1 = load_boston()['data'][:, [8, 10]]  # two clusters\nX2 = load_boston()['data'][:, [5, 12]]  # "banana"-shaped\n\n# Define "classifiers" to be used\nclassifiers = {\n    "Empirical Covariance": EllipticEnvelope(support_fraction=1.,\n                                             contamination=0.261),\n    "Robust Covariance (Minimum Covariance Determinant)":\n    EllipticEnvelope(contamination=0.261),\n    "OCSVM": OneClassSVM(nu=0.261, gamma=0.05)}\ncolors = ['m', 'g', 'b']\nlegend1 = {}\nlegend2 = {}\n\n# Learn a frontier for outlier detection with several classifiers\nxx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))\nxx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    plt.figure(1)\n    clf.fit(X1)\n    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n    Z1 = Z1.reshape(xx1.shape)\n    legend1[clf_name] = plt.contour(\n        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n    plt.figure(2)\n    clf.fit(X2)\n    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = plt.contour(\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n\nlegend1_values_list = list( legend1.values() )\nlegend1_keys_list = list( legend1.keys() )\n\n# Plot the results (= shape of the data points cloud)\nplt.figure(1)  # two clusters\nplt.title("Outlier detection on a real data set (boston housing)")\nplt.scatter(X1[:, 0], X1[:, 1], color='black')\nbbox_args = dict(boxstyle="round", fc="0.8")\narrow_args = dict(arrowstyle="->")\nplt.annotate("several confounded points", xy=(24, 19),\n             xycoords="data", textcoords="data",\n             xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\nplt.xlim((xx1.min(), xx1.max()))\nplt.ylim((yy1.min(), yy1.max()))\nplt.legend((legend1_values_list[0].collections[0],\n            legend1_values_list[1].collections[0],\n            legend1_values_list[2].collections[0]),\n           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n           loc="upper center",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel("accessibility to radial highways")\nplt.xlabel("pupil-teacher ratio by town")\n\nlegend2_values_list = list( legend2.values() )\nlegend2_keys_list = list( legend2.keys() )\n\nplt.figure(2)  # "banana" shape\nplt.title("Outlier detection on a real data set (boston housing)")\nplt.scatter(X2[:, 0], X2[:, 1], color='black')\nplt.xlim((xx2.min(), xx2.max()))\nplt.ylim((yy2.min(), yy2.max()))\nplt.legend((legend2_values_list[0].collections[0],\n            legend2_values_list[1].collections[0],\n            legend2_values_list[2].collections[0]),\n           (legend2_values_list[0], legend2_values_list[1], legend2_values_list[2]),\n           loc="upper center",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel("% lower status of the population")\nplt.xlabel("average number of rooms per dwelling")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_housing.html
Partial Dependence Plots	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.datasets.california_housing import fetch_california_housing\n\n\ndef main():\n    cal_housing = fetch_california_housing()\n\n    # split 80/20 train-test\n    X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n    names = cal_housing.feature_names\n\n    print('_' * 80)\n    print("Training GBRT...")\n    clf = GradientBoostingRegressor(n_estimators=100, max_depth=4,\n                                    learning_rate=0.1, loss='huber',\n                                    random_state=1)\n    clf.fit(X_train, y_train)\n    print("done.")\n\n    print('_' * 80)\n    print('Convenience plot with ``partial_dependence_plots``')\n    print\n\n    features = [0, 5, 1, 2, (5, 1)]\n    fig, axs = plot_partial_dependence(clf, X_train, features,\n                                       feature_names=names,\n                                       n_jobs=3, grid_resolution=50)\n    fig.suptitle('Partial dependence of house value on nonlocation features\n'\n                 'for the California housing dataset')\n    plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n\n    print('_' * 80)\n    print('Custom 3d plot via ``partial_dependence``')\n    print\n    fig = plt.figure()\n\n    target_feature = (1, 5)\n    pdp, (x_axis, y_axis) = partial_dependence(clf, target_feature,\n                                               X=X_train, grid_resolution=50)\n    XX, YY = np.meshgrid(x_axis, y_axis)\n    Z = pdp.T.reshape(XX.shape).T\n    ax = Axes3D(fig)\n    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)\n    ax.set_xlabel(names[target_feature[0]])\n    ax.set_ylabel(names[target_feature[1]])\n    ax.set_zlabel('Partial dependence')\n    #  pretty init view\n    ax.view_init(elev=22, azim=122)\n    plt.colorbar(surf)\n    plt.suptitle('Partial dependence of house value on median age and '\n                 'average occupancy')\n    plt.subplots_adjust(top=0.9)\n\n    plt.show()\n\n\n# Needed on Windows because plot_partial_dependence uses multiprocessing\nif __name__ == '__main__':\n    main()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html
Principal components analysis (PCA)	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Authors: Gael Varoquaux\n#          Jaques Grobler\n#          Kevin Hughes\n# License: BSD 3 clause\n\nfrom sklearn.decomposition import PCA\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\n###############################################################################\n# Create the data\n\ne = np.exp(1)\nnp.random.seed(4)\n\n\ndef pdf(x):\n    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)\n                  + stats.norm(scale=4 / e).pdf(x))\n\ny = np.random.normal(scale=0.5, size=(30000))\nx = np.random.normal(scale=0.5, size=(30000))\nz = np.random.normal(scale=0.1, size=len(x))\n\ndensity = pdf(x) * pdf(y)\npdf_z = pdf(5 * z)\n\ndensity *= pdf_z\n\na = x + y\nb = 2 * y\nc = a - b + z\n\nnorm = np.sqrt(a.var() + b.var())\na /= norm\nb /= norm\n\n\n###############################################################################\n# Plot the figures\ndef plot_figs(fig_num, elev, azim):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)\n\n    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)\n    Y = np.c_[a, b, c]\n\n    # Using SciPy's SVD, this would be:\n    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)\n\n    pca = PCA(n_components=3)\n    pca.fit(Y)\n    pca_score = pca.explained_variance_ratio_\n    V = pca.components_\n\n    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score / pca_score.min()\n\n    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T\n    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]\n    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]\n    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]\n    x_pca_plane.shape = (2, 2)\n    y_pca_plane.shape = (2, 2)\n    z_pca_plane.shape = (2, 2)\n    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n\nelev = -40\nazim = -80\nplot_figs(1, elev, azim)\n\nelev = 30\nazim = 20\nplot_figs(2, elev, azim)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html
PCA example with Iris Data-set	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_pca_iris_001.png]]	<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfrom sklearn import decomposition\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nfig = plt.figure(1, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\npca = decomposition.PCA(n_components=3)\npca.fit(X)\nX = pca.transform(X)\n\nfor name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral)\n\nx_surf = [X[:, 0].min(), X[:, 0].max(),\n          X[:, 0].min(), X[:, 0].max()]\ny_surf = [X[:, 0].max(), X[:, 0].max(),\n          X[:, 0].min(), X[:, 0].min()]\nx_surf = np.array(x_surf)\ny_surf = np.array(y_surf)\nv0 = pca.transform(pca.components_[[0]])\nv0 /= v0[-1]\nv1 = pca.transform(pca.components_[[1]])\nv1 /= v1[-1]\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html
Model selection with Probabilistic PCA and Factor Analysis (FA)	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort\n#          Denis A. Engemann\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.covariance import ShrunkCovariance, LedoitWolf\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.grid_search import GridSearchCV\n\n###############################################################################\n# Create the data\n\nn_samples, n_features, rank = 1000, 50, 10\nsigma = 1.\nrng = np.random.RandomState(42)\nU, _, _ = linalg.svd(rng.randn(n_features, n_features))\nX = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n\n# Adding homoscedastic noise\nX_homo = X + sigma * rng.randn(n_samples, n_features)\n\n# Adding heteroscedastic noise\nsigmas = sigma * rng.rand(n_features) + sigma / 2.\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas\n\n###############################################################################\n# Fit the models\n\nn_components = np.arange(0, n_features, 5)  # options for n_components\n\n\ndef compute_scores(X):\n    pca = PCA()\n    fa = FactorAnalysis()\n\n    pca_scores, fa_scores = [], []\n    for n in n_components:\n        pca.n_components = n\n        fa.n_components = n\n        pca_scores.append(np.mean(cross_val_score(pca, X)))\n        fa_scores.append(np.mean(cross_val_score(fa, X)))\n\n    return pca_scores, fa_scores\n\n\ndef shrunk_cov_score(X):\n    shrinkages = np.logspace(-2, 0, 30)\n    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n\n\ndef lw_score(X):\n    return np.mean(cross_val_score(LedoitWolf(), X))\n\n\nfor X, title in [(X_homo, 'Homoscedastic Noise'),\n                 (X_hetero, 'Heteroscedastic Noise')]:\n    pca_scores, fa_scores = compute_scores(X)\n    n_components_pca = n_components[np.argmax(pca_scores)]\n    n_components_fa = n_components[np.argmax(fa_scores)]\n\n    pca = PCA(n_components='mle')\n    pca.fit(X)\n    n_components_pca_mle = pca.n_components_\n\n    print("best n_components by PCA CV = %d" % n_components_pca)\n    print("best n_components by FactorAnalysis CV = %d" % n_components_fa)\n    print("best n_components by PCA MLE = %d" % n_components_pca_mle)\n\n    plt.figure()\n    plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n    plt.plot(n_components, fa_scores, 'r', label='FA scores')\n    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n    plt.axvline(n_components_pca, color='b',\n                label='PCA CV: %d' % n_components_pca, linestyle='--')\n    plt.axvline(n_components_fa, color='r',\n                label='FactorAnalysis CV: %d' % n_components_fa, linestyle='--')\n    plt.axvline(n_components_pca_mle, color='k',\n                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n\n    # compare with other covariance estimators\n    plt.axhline(shrunk_cov_score(X), color='violet',\n                label='Shrunk Covariance MLE', linestyle='-.')\n    plt.axhline(lw_score(X), color='orange',\n                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n\n    plt.xlabel('nb of components')\n    plt.ylabel('CV scores')\n    plt.legend(loc='lower right')\n    plt.title(title)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html
Comparison of LDA and PCA 2D projection of Iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n# Percentage of variance explained for each components\nprint('explained variance ratio (first two components): %s'\n      % str(pca.explained_variance_ratio_))\n\nplt.figure()\nfor c, i, target_name in zip("rgb", [0, 1, 2], target_names):\n    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=target_name)\nplt.legend()\nplt.title('PCA of IRIS dataset')\n\nplt.figure()\nfor c, i, target_name in zip("rgb", [0, 1, 2], target_names):\n    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], c=c, label=target_name)\nplt.legend()\nplt.title('LDA of IRIS dataset')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html
Test with permutations the significance of a classification score	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_permutation_test_for_classification_001.png]]	<br><pre><code># Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import StratifiedKFold, permutation_test_score\nfrom sklearn import datasets\n\n\n##############################################################################\n# Loading a dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nn_classes = np.unique(y).size\n\n# Some noisy data not correlated\nrandom = np.random.RandomState(seed=0)\nE = random.normal(size=(len(X), 2200))\n\n# Add noisy data to the informative features for make the task harder\nX = np.c_[X, E]\n\nsvm = SVC(kernel='linear')\ncv = StratifiedKFold(y, 2)\n\nscore, permutation_scores, pvalue = permutation_test_score(\n    svm, X, y, scoring="accuracy", cv=cv, n_permutations=100, n_jobs=1)\n\nprint("Classification score %s (pvalue : %s)" % (score, pvalue))\n\n###############################################################################\n# View histogram of permutation scores\nplt.hist(permutation_scores, 20, label='Permutation scores')\nylim = plt.ylim()\n# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n#plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n#          color='g', linewidth=3, label='Classification Score'\n#          ' (pvalue %s)' % pvalue)\n#plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n#          color='k', linewidth=3, label='Luck')\nplt.plot(2 * [score], ylim, '--g', linewidth=3,\n         label='Classification Score'\n         ' (pvalue %s)' % pvalue)\nplt.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')\n\nplt.ylim(ylim)\nplt.legend()\nplt.xlabel('Score')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_permutation_test_for_classification.html
Polynomial interpolation	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_polynomial_interpolation_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Mathieu Blondel\n#         Jake Vanderplas\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    """ function to approximate by polynomial interpolation"""\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\nplt.plot(x_plot, f(x_plot), label="ground truth")\nplt.scatter(x, y, label="training points")\n\nfor degree in [3, 4, 5]:\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, y)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, label="degree %d" % degree)\n\nplt.legend(loc='lower left')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html
Precision-Recall	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_precision_recall_001.png]]	<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=random_state)\n\n# Run classifier\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute Precision-Recall and plot curve\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        y_score[:, i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n\n# Compute micro-average ROC curve and ROC area\nprecision["micro"], recall["micro"], _ = precision_recall_curve(y_test.ravel(),\n    y_score.ravel())\naverage_precision["micro"] = average_precision_score(y_test, y_score,\n                                                     average="micro")\n\n# Plot Precision-Recall curve\nplt.clf()\nplt.plot(recall[0], precision[0], label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0]))\nplt.legend(loc="lower left")\nplt.show()\n\n# Plot Precision-Recall curve for each class\nplt.clf()\nplt.plot(recall["micro"], precision["micro"],\n         label='micro-average Precision-recall curve (area = {0:0.2f})'\n               ''.format(average_precision["micro"]))\nfor i in range(n_classes):\n    plt.plot(recall[i], precision[i],\n             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n                   ''.format(i, average_precision[i]))\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Extension of Precision-Recall curve to multi-class')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
Prediction Latency	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom collections import defaultdict\n\nimport time\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import scoreatpercentile\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.ensemble.forest import RandomForestRegressor\nfrom sklearn.linear_model.ridge import Ridge\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.svm.classes import SVR\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return '__file__' in globals()\n\n\ndef atomic_benchmark_estimator(estimator, X_test, verbose=False):\n    """Measure runtime prediction of each instance."""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_instances, dtype=np.float)\n    for i in range(n_instances):\n        instance = X_test[[i], :]\n        start = time.time()\n        estimator.predict(instance)\n        runtimes[i] = time.time() - start\n    if verbose:\n        print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\n    """Measure runtime prediction of the whole input."""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_bulk_repeats, dtype=np.float)\n    for i in range(n_bulk_repeats):\n        start = time.time()\n        estimator.predict(X_test)\n        runtimes[i] = time.time() - start\n    runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))\n    if verbose:\n        print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\n    """\n    Measure runtimes of prediction in both atomic and bulk mode.\n\n    Parameters\n    ----------\n    estimator : already trained estimator supporting `predict()`\n    X_test : test input\n    n_bulk_repeats : how many times to repeat when evaluating bulk mode\n\n    Returns\n    -------\n    atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the\n    runtimes in seconds.\n\n    """\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats,\n                                             verbose)\n    return atomic_runtimes, bulk_runtimes\n\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\n    """Generate a regression dataset with the given parameters."""\n    if verbose:\n        print("generating dataset...")\n    X, y, coef = make_regression(n_samples=n_train + n_test,\n                                 n_features=n_features, noise=noise, coef=True)\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    idx = np.arange(n_train)\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X_train = X_train[idx]\n    y_train = y_train[idx]\n\n    std = X_train.std(axis=0)\n    mean = X_train.mean(axis=0)\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n\n    std = y_train.std(axis=0)\n    mean = y_train.mean(axis=0)\n    y_train = (y_train - mean) / std\n    y_test = (y_test - mean) / std\n\n    gc.collect()\n    if verbose:\n        print("ok")\n    return X_train, y_train, X_test, y_test\n\n\ndef boxplot_runtimes(runtimes, pred_type, configuration):\n    """\n    Plot a new `Figure` with boxplots of prediction runtimes.\n\n    Parameters\n    ----------\n    runtimes : list of `np.array` of latencies in micro-seconds\n    cls_names : list of estimator class names that generated the runtimes\n    pred_type : 'bulk' or 'atomic'\n\n    """\n\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    bp = plt.boxplot(runtimes, )\n\n    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],\n                                  estimator_conf['complexity_computer'](\n                                      estimator_conf['instance']),\n                                  estimator_conf['complexity_label']) for\n                 estimator_conf in configuration['estimators']]\n    plt.setp(ax1, xticklabels=cls_infos)\n    plt.setp(bp['boxes'], color='black')\n    plt.setp(bp['whiskers'], color='black')\n    plt.setp(bp['fliers'], color='red', marker='+')\n\n    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n                   alpha=0.5)\n\n    ax1.set_axisbelow(True)\n    ax1.set_title('Prediction Time per Instance - %s, %d feats.' % (\n        pred_type.capitalize(),\n        configuration['n_features']))\n    ax1.set_ylabel('Prediction Time (us)')\n\n    plt.show()\n\n\ndef benchmark(configuration):\n    """Run the whole benchmark."""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration['n_train'], configuration['n_test'],\n        configuration['n_features'])\n\n    stats = {}\n    for estimator_conf in configuration['estimators']:\n        print("Benchmarking", estimator_conf['instance'])\n        estimator_conf['instance'].fit(X_train, y_train)\n        gc.collect()\n        a, b = benchmark_estimator(estimator_conf['instance'], X_test)\n        stats[estimator_conf['name']] = {'atomic': a, 'bulk': b}\n\n    cls_names = [estimator_conf['name'] for estimator_conf in configuration[\n        'estimators']]\n    runtimes = [1e6 * stats[clf_name]['atomic'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, 'atomic', configuration)\n    runtimes = [1e6 * stats[clf_name]['bulk'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, 'bulk (%d)' % configuration['n_test'],\n                     configuration)\n\n\ndef n_feature_influence(estimators, n_train, n_test, n_features, percentile):\n    """\n    Estimate influence of the number of features on prediction time.\n\n    Parameters\n    ----------\n\n    estimators : dict of (name (str), estimator) to benchmark\n    n_train : nber of training instances (int)\n    n_test : nber of testing instances (int)\n    n_features : list of feature-space dimensionality to test (int)\n    percentile : percentile at which to measure the speed (int [0-100])\n\n    Returns:\n    --------\n\n    percentiles : dict(estimator_name,\n                       dict(n_features, percentile_perf_in_us))\n\n    """\n    percentiles = defaultdict(defaultdict)\n    for n in n_features:\n        print("benchmarking with %d features" % n)\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\n        for cls_name, estimator in estimators.items():\n            estimator.fit(X_train, y_train)\n            gc.collect()\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\n            percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,\n                                                               percentile)\n    return percentiles\n\n\ndef plot_n_features_influence(percentiles, percentile):\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    colors = ['r', 'g', 'b']\n    for i, cls_name in enumerate(percentiles.keys()):\n        x = np.array(sorted([n for n in percentiles[cls_name].keys()]))\n        y = np.array([percentiles[cls_name][n] for n in x])\n        plt.plot(x, y, color=colors[i], )\n    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n                   alpha=0.5)\n    ax1.set_axisbelow(True)\n    ax1.set_title('Evolution of Prediction Time with #Features')\n    ax1.set_xlabel('#Features')\n    ax1.set_ylabel('Prediction Time at %d%%-ile (us)' % percentile)\n    plt.show()\n\n\ndef benchmark_throughputs(configuration, duration_secs=0.1):\n    """benchmark throughput for different estimators."""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration['n_train'], configuration['n_test'],\n        configuration['n_features'])\n    throughputs = dict()\n    for estimator_config in configuration['estimators']:\n        estimator_config['instance'].fit(X_train, y_train)\n        start_time = time.time()\n        n_predictions = 0\n        while (time.time() - start_time) < duration_secs:\n            estimator_config['instance'].predict(X_test[[0]])\n            n_predictions += 1\n        throughputs[estimator_config['name']] = n_predictions / duration_secs\n    return throughputs\n\n\ndef plot_benchmark_throughput(throughputs, configuration):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = ['r', 'g', 'b']\n    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],\n                                  estimator_conf['complexity_computer'](\n                                      estimator_conf['instance']),\n                                  estimator_conf['complexity_label']) for\n                 estimator_conf in configuration['estimators']]\n    cls_values = [throughputs[estimator_conf['name']] for estimator_conf in\n                  configuration['estimators']]\n    plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)\n    ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))\n    ax.set_xticklabels(cls_infos, fontsize=10)\n    ymax = max(cls_values) * 1.2\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel('Throughput (predictions/sec)')\n    ax.set_title('Prediction Throughput for different estimators (%d '\n                 'features)' % configuration['n_features'])\n    plt.show()\n\n\n###############################################################################\n# main code\n\nstart_time = time.time()\n\n# benchmark bulk/atomic prediction speed for various regressors\nconfiguration = {\n    'n_train': int(1e3),\n    'n_test': int(1e2),\n    'n_features': int(1e2),\n    'estimators': [\n        {'name': 'Linear Model',\n         'instance': SGDRegressor(penalty='elasticnet', alpha=0.01,\n                                  l1_ratio=0.25, fit_intercept=True),\n         'complexity_label': 'non-zero coefficients',\n         'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},\n        {'name': 'RandomForest',\n         'instance': RandomForestRegressor(),\n         'complexity_label': 'estimators',\n         'complexity_computer': lambda clf: clf.n_estimators},\n        {'name': 'SVR',\n         'instance': SVR(kernel='rbf'),\n         'complexity_label': 'support vectors',\n         'complexity_computer': lambda clf: len(clf.support_vectors_)},\n    ]\n}\nbenchmark(configuration)\n\n# benchmark n_features influence on prediction speed\npercentile = 90\npercentiles = n_feature_influence({'ridge': Ridge()},\n                                  configuration['n_train'],\n                                  configuration['n_test'],\n                                  [100, 250, 500], percentile)\nplot_n_features_influence(percentiles, percentile)\n\n# benchmark throughput\nthroughputs = benchmark_throughputs(configuration)\nplot_benchmark_throughput(throughputs, configuration)\n\nstop_time = time.time()\nprint("example run in %.2fs" % (stop_time - start_time))</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html
Plot randomly generated classification dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_random_dataset_001.png]]	<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_gaussian_quantiles\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n\nplt.subplot(321)\nplt.title("One informative feature, one cluster per class", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(322)\nplt.title("Two informative features, one cluster per class", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(323)\nplt.title("Two informative features, two clusters per class", fontsize='small')\nX2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\nplt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)\n\n\nplt.subplot(324)\nplt.title("Multi-class, two informative features, one cluster",\n          fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(325)\nplt.title("Three blobs", fontsize='small')\nX1, Y1 = make_blobs(n_features=2, centers=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(326)\nplt.title("Gaussian divided into three quantiles", fontsize='small')\nX1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html
Hashing feature transformation using Totally Random Trees	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_random_forest_embedding_001.png]]	<br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_circles\nfrom sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import BernoulliNB\n\n# make a synthetic dataset\nX, y = make_circles(factor=0.5, random_state=0, noise=0.05)\n\n# use RandomTreesEmbedding to transform data\nhasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)\nX_transformed = hasher.fit_transform(X)\n\n# Visualize result using PCA\npca = TruncatedSVD(n_components=2)\nX_reduced = pca.fit_transform(X_transformed)\n\n# Learn a Naive Bayes classifier on the transformed data\nnb = BernoulliNB()\nnb.fit(X_transformed, y)\n\n\n# Learn an ExtraTreesClassifier for comparison\ntrees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)\ntrees.fit(X, y)\n\n\n# scatter plot of original and reduced data\nfig = plt.figure(figsize=(9, 8))\n\nax = plt.subplot(221)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_title("Original Data (2d)")\nax.set_xticks(())\nax.set_yticks(())\n\nax = plt.subplot(222)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50)\nax.set_title("PCA reduction (2d) of transformed data (%dd)" %\n             X_transformed.shape[1])\nax.set_xticks(())\nax.set_yticks(())\n\n# Plot the decision in original space. For that, we will assign a color to each\n# point in the mesh [x_min, m_max] x [y_min, y_max].\nh = .01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# transform grid using RandomTreesEmbedding\ntransformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\n\nax = plt.subplot(223)\nax.set_title("Naive Bayes on Transformed data")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\n# transform grid using ExtraTreesClassifier\ny_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\nax = plt.subplot(224)\nax.set_title("ExtraTrees predictions")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html
Plot randomly generated multilabel dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_random_multilabel_dataset_001.png]]	<br><pre><code>from __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\n\nprint(__doc__)\n\nCOLORS = np.array(['!',\n                   '#FF3333',  # red\n                   '#0198E1',  # blue\n                   '#BF5FFF',  # purple\n                   '#FCD116',  # yellow\n                   '#FF7216',  # orange\n                   '#4DBD33',  # green\n                   '#87421F'   # brown\n                   ])\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = np.random.randint(2 ** 10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,\n                                   n_classes=n_classes, n_labels=n_labels,\n                                   length=length, allow_unlabeled=False,\n                                   return_distributions=True,\n                                   random_state=RANDOM_SEED)\n\n    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]\n                                                    ).sum(axis=1)),\n               marker='.')\n    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,\n               marker='*', linewidth=.5, edgecolor='black',\n               s=20 + 1500 * p_c ** 2,\n               color=COLORS.take([1, 2, 4]))\n    ax.set_xlabel('Feature 0 count')\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))\nplt.subplots_adjust(bottom=.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title('n_labels=1, length=50')\nax1.set_ylabel('Feature 1 count')\n\nplot_2d(ax2, n_labels=3)\nax2.set_title('n_labels=3, length=50')\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\nplt.show()\n\nprint('The data was generated from (random_state=%d):' % RANDOM_SEED)\nprint('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\t')\nfor k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):\n    print('%s\t%0.2f\t%0.2f\t%0.2f' % (k, p, p_w[0], p_w[1]))</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_random_multilabel_dataset.html
Robust linear model estimation using RANSAC	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ransac_001.png]]	<br><pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import linear_model, datasets\n\n\nn_samples = 1000\nn_outliers = 50\n\n\nX, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n                                      n_informative=1, noise=10,\n                                      coef=True, random_state=0)\n\n# Add outlier data\nnp.random.seed(0)\nX[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\ny[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n\n# Fit line using all data\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)\n\n# Robustly fit linear model with RANSAC algorithm\nmodel_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())\nmodel_ransac.fit(X, y)\ninlier_mask = model_ransac.inlier_mask_\noutlier_mask = np.logical_not(inlier_mask)\n\n# Predict data of estimated models\nline_X = np.arange(-5, 5)\nline_y = model.predict(line_X[:, np.newaxis])\nline_y_ransac = model_ransac.predict(line_X[:, np.newaxis])\n\n# Compare estimated coefficients\nprint("Estimated coefficients (true, normal, RANSAC):")\nprint(coef, model.coef_, model_ransac.estimator_.coef_)\n\nplt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers')\nplt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers')\nplt.plot(line_X, line_y, '-k', label='Linear regressor')\nplt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor')\nplt.legend(loc='lower right')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html
RBF SVM parameters	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.grid_search import GridSearchCV\n\n\n# Utility function to move the midpoint of a colormap to be around\n# the values of interest.\n\nclass MidpointNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\n##############################################################################\n# Load and prepare data set\n#\n# dataset for grid search\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dataset for decision function visualization: we only keep the first two\n# features in X and sub-sample the dataset to keep only 2 classes and\n# make it a binary classification problem.\n\nX_2d = X[:, :2]\nX_2d = X_2d[y > 0]\ny_2d = y[y > 0]\ny_2d -= 1\n\n# It is usually a good idea to scale the data for SVM training.\n# We are cheating a bit in this example in scaling all of the data,\n# instead of fitting the transformation on the training set and\n# just applying it on the test set.\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_2d = scaler.fit_transform(X_2d)\n\n##############################################################################\n# Train classifiers\n#\n# For an initial search, a logarithmic grid with basis\n# 10 is often helpful. Using a basis of 2, a finer\n# tuning can be achieved but at a much higher cost.\n\nC_range = np.logspace(-2, 10, 13)\ngamma_range = np.logspace(-9, 3, 13)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ncv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\ngrid.fit(X, y)\n\nprint("The best parameters are %s with a score of %0.2f"\n      % (grid.best_params_, grid.best_score_))\n\n# Now we need to fit a classifier for all parameters in the 2d version\n# (we use a smaller set of parameters here because it takes a while to train)\n\nC_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = SVC(C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))\n\n##############################################################################\n# visualization\n#\n# draw visualization of parameter effects\n\nplt.figure(figsize=(8, 6))\nxx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\nfor (k, (C, gamma, clf)) in enumerate(classifiers):\n    # evaluate decision function in a grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # visualize decision function for these parameters\n    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n    plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)),\n              size='medium')\n\n    # visualize parameter's effect on decision function\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis('tight')\n\n# plot the scores of the grid\n# grid_scores_ contains parameter settings and scores\n# We extract just the scores\nscores = [x[1] for x in grid.grid_scores_]\nscores = np.array(scores).reshape(len(C_range), len(gamma_range))\n\n# Draw heatmap of the validation accuracy as a function of gamma and C\n#\n# The score are encoded as colors with the hot colormap which varies from dark\n# red to bright yellow. As the most interesting scores are all located in the\n# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n# as to make it easier to visualize the small variations of score values in the\n# interesting range while not brutally collapsing all the low score values to\n# the same color.\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\nplt.xlabel('gamma')\nplt.ylabel('C')\nplt.colorbar()\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title('Validation accuracy')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html
Restricted Boltzmann Machine features for digit classification	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_rbm_logistic_classification_001.png]]	<br><pre><code>from __future__ import print_function\n\nprint(__doc__)\n\n# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.ndimage import convolve\nfrom sklearn import linear_model, datasets, metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.pipeline import Pipeline\n\n\n###############################################################################\n# Setting up\n\ndef nudge_dataset(X, Y):\n    """\n    This produces a dataset 5 times bigger than the original one,\n    by moving the 8x8 images in X around by 1px to left, right, down, up\n    """\n    direction_vectors = [\n        [[0, 1, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [1, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 1],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 1, 0]]]\n\n    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n                                  weights=w).ravel()\n    X = np.concatenate([X] +\n                       [np.apply_along_axis(shift, 1, X, vector)\n                        for vector in direction_vectors])\n    Y = np.concatenate([Y for _ in range(5)], axis=0)\n    return X, Y\n\n# Load Data\ndigits = datasets.load_digits()\nX = np.asarray(digits.data, 'float32')\nX, Y = nudge_dataset(X, digits.target)\nX = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n                                                    test_size=0.2,\n                                                    random_state=0)\n\n# Models we will use\nlogistic = linear_model.LogisticRegression()\nrbm = BernoulliRBM(random_state=0, verbose=True)\n\nclassifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n\n###############################################################################\n# Training\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 20\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000.0\n\n# Training RBM-Logistic Pipeline\nclassifier.fit(X_train, Y_train)\n\n# Training Logistic regression\nlogistic_classifier = linear_model.LogisticRegression(C=100.0)\nlogistic_classifier.fit(X_train, Y_train)\n\n###############################################################################\n# Evaluation\n\nprint()\nprint("Logistic regression using RBM features:\n%s\n" % (\n    metrics.classification_report(\n        Y_test,\n        classifier.predict(X_test))))\n\nprint("Logistic regression using raw pixel features:\n%s\n" % (\n    metrics.classification_report(\n        Y_test,\n        logistic_classifier.predict(X_test))))\n\n###############################################################################\n# Plotting\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(rbm.components_):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle('100 components extracted by RBM', fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html
Nearest Neighbors regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#\n# License: BSD 3 clause (C) INRIA\n\n\n###############################################################################\n# Generate sample data\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\n\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\nT = np.linspace(0, 5, 500)[:, np.newaxis]\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - np.random.rand(8))\n\n###############################################################################\n# Fit regression model\nn_neighbors = 5\n\nfor i, weights in enumerate(['uniform', 'distance']):\n    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n    y_ = knn.fit(X, y).predict(T)\n\n    plt.subplot(2, 1, i + 1)\n    plt.scatter(X, y, c='k', label='data')\n    plt.plot(T, y_, c='g', label='prediction')\n    plt.axis('tight')\n    plt.legend()\n    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,\n                                                                weights))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html
Recursive feature elimination	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_rfe_digits_001.png]]	<br><pre><code>print(__doc__)\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = load_digits()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = SVC(kernel="linear", C=1)\nrfe = RFE(estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\nplt.matshow(ranking)\nplt.colorbar()\nplt.title("Ranking of pixels with RFE")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html
Recursive feature elimination with cross-validation	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_rfe_with_cross_validation_001.png]]	<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n                           n_redundant=2, n_repeated=0, n_classes=8,\n                           n_clusters_per_class=1, random_state=0)\n\n# Create the RFE object and compute a cross-validated score.\nsvc = SVC(kernel="linear")\n# The "accuracy" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2),\n              scoring='accuracy')\nrfecv.fit(X, y)\n\nprint("Optimal number of features : %d" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel("Number of features selected")\nplt.ylabel("Cross validation score (nb of correct classifications)")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html
Plot Ridge coefficients as a function of the regularization	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_ridge_path_001.png]]	<br><pre><code># Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# X is the 10x10 Hilbert matrix\nX = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\ny = np.ones(10)\n\n###############################################################################\n# Compute paths\n\nn_alphas = 200\nalphas = np.logspace(-10, -2, n_alphas)\nclf = linear_model.Ridge(fit_intercept=False)\n\ncoefs = []\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X, y)\n    coefs.append(clf.coef_)\n\n###############################################################################\n# Display results\n\nax = plt.gca()\nax.set_color_cycle(['b', 'r', 'g', 'c', 'k', 'y', 'm'])\n\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html
Robust linear estimator fitting	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn import linear_model, metrics\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nnp.random.seed(42)\n\nX = np.random.normal(size=400)\ny = np.sin(X)\n# Make sure that it X is 2D\nX = X[:, np.newaxis]\n\nX_test = np.random.normal(size=200)\ny_test = np.sin(X_test)\nX_test = X_test[:, np.newaxis]\n\ny_errors = y.copy()\ny_errors[::3] = 3\n\nX_errors = X.copy()\nX_errors[::3] = 3\n\ny_errors_large = y.copy()\ny_errors_large[::3] = 10\n\nX_errors_large = X.copy()\nX_errors_large[::3] = 10\n\nestimators = [('OLS', linear_model.LinearRegression()),\n              ('Theil-Sen', linear_model.TheilSenRegressor(random_state=42)),\n              ('RANSAC', linear_model.RANSACRegressor(random_state=42)), ]\n\nx_plot = np.linspace(X.min(), X.max())\n\nfor title, this_X, this_y in [\n        ('Modeling errors only', X, y),\n        ('Corrupt X, small deviants', X_errors, y),\n        ('Corrupt y, small deviants', X, y_errors),\n        ('Corrupt X, large deviants', X_errors_large, y),\n        ('Corrupt y, large deviants', X, y_errors_large)]:\n    plt.figure(figsize=(5, 4))\n    plt.plot(this_X[:, 0], this_y, 'k+')\n\n    for name, estimator in estimators:\n        model = make_pipeline(PolynomialFeatures(3), estimator)\n        model.fit(this_X, this_y)\n        mse = metrics.mean_squared_error(model.predict(X_test), y_test)\n        y_plot = model.predict(x_plot[:, np.newaxis])\n        plt.plot(x_plot, y_plot,\n                 label='%s: error = %.3f' % (name, mse))\n\n    plt.legend(loc='best', frameon=False,\n               title='Error: mean absolute deviation\n to non corrupt data')\n    plt.xlim(-4, 10.2)\n    plt.ylim(-2, 10.2)\n    plt.title(title)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_robust_fit.html
Robust Scaling on Toy Data	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_robust_scaling_001.png]]	<br><pre><code>from __future__ import print_function\nprint(__doc__)\n\n\n# Code source: Thomas Unterthiner\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# Create training and test data\nnp.random.seed(42)\nn_datapoints = 100\nCov = [[0.9, 0.0], [0.0, 20.0]]\nmu1 = [100.0, -3.0]\nmu2 = [101.0, -3.0]\nX1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)\nX2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)\nY_train = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])\nX_train = np.vstack([X1, X2])\n\nX1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)\nX2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)\nY_test = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])\nX_test = np.vstack([X1, X2])\n\nX_train[0, 0] = -1000  # a fairly large outlier\n\n\n# Scale data\nstandard_scaler = StandardScaler()\nXtr_s = standard_scaler.fit_transform(X_train)\nXte_s = standard_scaler.transform(X_test)\n\nrobust_scaler = RobustScaler()\nXtr_r = robust_scaler.fit_transform(X_train)\nXte_r = robust_scaler.fit_transform(X_test)\n\n\n# Plot data\nfig, ax = plt.subplots(1, 3, figsize=(12, 4))\nax[0].scatter(X_train[:, 0], X_train[:, 1],\n              color=np.where(Y_train > 0, 'r', 'b'))\nax[1].scatter(Xtr_s[:, 0], Xtr_s[:, 1], color=np.where(Y_train > 0, 'r', 'b'))\nax[2].scatter(Xtr_r[:, 0], Xtr_r[:, 1], color=np.where(Y_train > 0, 'r', 'b'))\nax[0].set_title("Unscaled data")\nax[1].set_title("After standard scaling (zoomed in)")\nax[2].set_title("After robust scaling (zoomed in)")\n# for the scaled data, we zoom in to the data center (outlier can't be seen!)\nfor a in ax[1:]:\n    a.set_xlim(-3, 3)\n    a.set_ylim(-3, 3)\nplt.tight_layout()\nplt.show()\n\n\n# Classify using k-NN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(Xtr_s, Y_train)\nacc_s = knn.score(Xte_s, Y_test)\nprint("Testset accuracy using standard scaler: %.3f" % acc_s)\nknn.fit(Xtr_r, Y_train)\nacc_r = knn.score(Xte_r, Y_test)\nprint("Testset accuracy using robust scaler:   %.3f" % acc_r)</code></pre>	http://scikit-learn.org/stable/auto_examples/preprocessing/plot_robust_scaling.html
Robust vs Empirical covariance estimate	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_robust_vs_empirical_covariance_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\n# example settings\nn_samples = 80\nn_features = 5\nrepeat = 10\n\nrange_n_outliers = np.concatenate(\n    (np.linspace(0, n_samples / 8, 5),\n     np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1]))\n\n# definition of arrays to store results\nerr_loc_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_cov_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_pure = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_pure = np.zeros((range_n_outliers.size, repeat))\n\n# computation\nfor i, n_outliers in enumerate(range_n_outliers):\n    for j in range(repeat):\n\n        rng = np.random.RandomState(i * j)\n\n        # generate data\n        X = rng.randn(n_samples, n_features)\n        # add some outliers\n        outliers_index = rng.permutation(n_samples)[:n_outliers]\n        outliers_offset = 10. * \\n            (np.random.randint(2, size=(n_outliers, n_features)) - 0.5)\n        X[outliers_index] += outliers_offset\n        inliers_mask = np.ones(n_samples).astype(bool)\n        inliers_mask[outliers_index] = False\n\n        # fit a Minimum Covariance Determinant (MCD) robust estimator to data\n        mcd = MinCovDet().fit(X)\n        # compare raw robust estimates with the true location and covariance\n        err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2)\n        err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))\n\n        # compare estimators learned from the full data set with true\n        # parameters\n        err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)\n        err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm(\n            np.eye(n_features))\n\n        # compare with an empirical covariance learned from a pure data set\n        # (i.e. "perfect" mcd)\n        pure_X = X[inliers_mask]\n        pure_location = pure_X.mean(0)\n        pure_emp_cov = EmpiricalCovariance().fit(pure_X)\n        err_loc_emp_pure[i, j] = np.sum(pure_location ** 2)\n        err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))\n\n# Display results\nfont_prop = matplotlib.font_manager.FontProperties(size=11)\nplt.subplot(2, 1, 1)\nplt.errorbar(range_n_outliers, err_loc_mcd.mean(1),\n             yerr=err_loc_mcd.std(1) / np.sqrt(repeat),\n             label="Robust location", color='m')\nplt.errorbar(range_n_outliers, err_loc_emp_full.mean(1),\n             yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),\n             label="Full data set mean", color='green')\nplt.errorbar(range_n_outliers, err_loc_emp_pure.mean(1),\n             yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),\n             label="Pure data set mean", color='black')\nplt.title("Influence of outliers on the location estimation")\nplt.ylabel(r"Error ($||\mu - \hat{\mu}||_2^2$)")\nplt.legend(loc="upper left", prop=font_prop)\n\nplt.subplot(2, 1, 2)\nx_size = range_n_outliers.size\nplt.errorbar(range_n_outliers, err_cov_mcd.mean(1),\n             yerr=err_cov_mcd.std(1),\n             label="Robust covariance (mcd)", color='m')\nplt.errorbar(range_n_outliers[:(x_size / 5 + 1)],\n             err_cov_emp_full.mean(1)[:(x_size / 5 + 1)],\n             yerr=err_cov_emp_full.std(1)[:(x_size / 5 + 1)],\n             label="Full data set empirical covariance", color='green')\nplt.plot(range_n_outliers[(x_size / 5):(x_size / 2 - 1)],\n         err_cov_emp_full.mean(1)[(x_size / 5):(x_size / 2 - 1)], color='green',\n         ls='--')\nplt.errorbar(range_n_outliers, err_cov_emp_pure.mean(1),\n             yerr=err_cov_emp_pure.std(1),\n             label="Pure data set empirical covariance", color='black')\nplt.title("Influence of outliers on the covariance estimation")\nplt.xlabel("Amount of contamination (%)")\nplt.ylabel("RMSE")\nplt.legend(loc="upper center", prop=font_prop)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_robust_vs_empirical_covariance.html
Receiver Operating Characteristic (ROC)	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features to make the problem harder\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=0)\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc["micro"] = auc(fpr["micro"], tpr["micro"])\n\n\n##############################################################################\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc="lower right")\nplt.show()\n\n\n##############################################################################\n# Plot ROC curves for the multiclass problem\n\n# Compute macro-average ROC curve and ROC area\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr["macro"] = all_fpr\ntpr["macro"] = mean_tpr\nroc_auc["macro"] = auc(fpr["macro"], tpr["macro"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr["micro"], tpr["micro"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc["micro"]),\n         linewidth=2)\n\nplt.plot(fpr["macro"], tpr["macro"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc["macro"]),\n         linewidth=2)\n\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
Receiver Operating Characteristic (ROC) with cross validation	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_roc_crossval_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom scipy import interp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.cross_validation import StratifiedKFold\n\n###############################################################################\n# Data IO and generation\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX, y = X[y != 2], y[y != 2]\nn_samples, n_features = X.shape\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n###############################################################################\n# Classification and ROC analysis\n\n# Run classifier with cross-validation and plot ROC curves\ncv = StratifiedKFold(y, n_folds=6)\nclassifier = svm.SVC(kernel='linear', probability=True,\n                     random_state=random_state)\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\nall_tpr = []\n\nfor i, (train, test) in enumerate(cv):\n    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n\nplt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n\nmean_tpr /= len(cv)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, 'k--',\n         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html
Spectral clustering for image segmentation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#           Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\n\n###############################################################################\nl = 100\nx, y = np.indices((l, l))\n\ncenter1 = (28, 24)\ncenter2 = (40, 50)\ncenter3 = (67, 58)\ncenter4 = (24, 70)\n\nradius1, radius2, radius3, radius4 = 16, 14, 15, 14\n\ncircle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\ncircle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\ncircle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2\ncircle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2\n\n###############################################################################\n# 4 circles\nimg = circle1 + circle2 + circle3 + circle4\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(img, mask=mask)\n\n# Take a decreasing function of the gradient: we take it weakly\n# dependent from the gradient the segmentation is close to a voronoi\ngraph.data = np.exp(-graph.data / graph.data.std())\n\n# Force the solver to be arpack, since amg is numerically\n# unstable on this example\nlabels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')\nlabel_im = -np.ones(mask.shape)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\n###############################################################################\n# 2 circles\nimg = circle1 + circle2\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\ngraph = image.img_to_graph(img, mask=mask)\ngraph.data = np.exp(-graph.data / graph.data.std())\n\nlabels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')\nlabel_im = -np.ones(mask.shape)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html
Feature selection using SelectFromModel and LassoCV	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_select_from_model_boston_001.png]]	<br><pre><code># Author: Manoj Kumar <mks542@nyu.edu>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\n# Load the boston dataset.\nboston = load_boston()\nX, y = boston['data'], boston['target']\n\n# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\nclf = LassoCV()\n\n# Set a minimum threshold of 0.25\nsfm = SelectFromModel(clf, threshold=0.25)\nsfm.fit(X, y)\nn_features = sfm.transform(X).shape[1]\n\n# Reset the threshold till the number of features equals two.\n# Note that the attribute can be set directly instead of repeatedly\n# fitting the metatransformer.\nwhile n_features > 2:\n    sfm.threshold += 0.1\n    X_transform = sfm.transform(X)\n    n_features = X_transform.shape[1]\n\n# Plot the selected two features from X.\nplt.title(\n    "Features selected from Boston using SelectFromModel with "\n    "threshold %0.3f." % sfm.threshold)\nfeature1 = X_transform[:, 0]\nfeature2 = X_transform[:, 1] \nplt.plot(feature1, feature2, 'r.')\nplt.xlabel("Feature number 1")\nplt.ylabel("Feature number 2")\nplt.ylim([np.min(feature2), np.max(feature2)])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html
SVM: Maximum margin separating hyperplane	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_separating_hyperplane_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# fit the model\nclf = svm.SVC(kernel='linear')\nclf.fit(X, Y)\n\n# get the separating hyperplane\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - (clf.intercept_[0]) / w[1]\n\n# plot the parallels to the separating hyperplane that pass through the\n# support vectors\nb = clf.support_vectors_[0]\nyy_down = a * xx + (b[1] - a * b[0])\nb = clf.support_vectors_[-1]\nyy_up = a * xx + (b[1] - a * b[0])\n\n# plot the line, the points, and the nearest vectors to the plane\nplt.plot(xx, yy, 'k-')\nplt.plot(xx, yy_down, 'k--')\nplt.plot(xx, yy_up, 'k--')\n\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=80, facecolors='none')\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html
SVM: Separating hyperplane for unbalanced classes	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_separating_hyperplane_unbalanced_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n#from sklearn.linear_model import SGDClassifier\n\n# we create 40 separable points\nrng = np.random.RandomState(0)\nn_samples_1 = 1000\nn_samples_2 = 100\nX = np.r_[1.5 * rng.randn(n_samples_1, 2),\n          0.5 * rng.randn(n_samples_2, 2) + [2, 2]]\ny = [0] * (n_samples_1) + [1] * (n_samples_2)\n\n# fit the model and get the separating hyperplane\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(X, y)\n\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - clf.intercept_[0] / w[1]\n\n\n# get the separating hyperplane using weighted classes\nwclf = svm.SVC(kernel='linear', class_weight={1: 10})\nwclf.fit(X, y)\n\nww = wclf.coef_[0]\nwa = -ww[0] / ww[1]\nwyy = wa * xx - wclf.intercept_[0] / ww[1]\n\n# plot separating hyperplanes and samples\nh0 = plt.plot(xx, yy, 'k-', label='no weights')\nh1 = plt.plot(xx, wyy, 'k--', label='with weights')\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.legend()\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html
Comparing various online solvers	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_comparison_001.png]]	<br><pre><code># Author: Rob Zinkov <rob at zinkov dot com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import SGDClassifier, Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\nrounds = 20\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\n\nclassifiers = [\n    ("SGD", SGDClassifier()),\n    ("ASGD", SGDClassifier(average=True)),\n    ("Perceptron", Perceptron()),\n    ("Passive-Aggressive I", PassiveAggressiveClassifier(loss='hinge',\n                                                         C=1.0)),\n    ("Passive-Aggressive II", PassiveAggressiveClassifier(loss='squared_hinge',\n                                                          C=1.0)),\n    ("SAG", LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / X.shape[0]))\n]\n\nxx = 1. - np.array(heldout)\n\nfor name, clf in classifiers:\n    print("training %s" % name)\n    rng = np.random.RandomState(42)\n    yy = []\n    for i in heldout:\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = \\n                train_test_split(X, y, test_size=i, random_state=rng)\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            yy_.append(1 - np.mean(y_pred == y_test))\n        yy.append(np.mean(yy_))\n    plt.plot(xx, yy, label=name)\n\nplt.legend(loc="upper right")\nplt.xlabel("Proportion train")\nplt.ylabel("Test Error Rate")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_comparison.html
Plot multi-class SGD on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_iris_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\ncolors = "bry"\n\n# shuffle\nidx = np.arange(X.shape[0])\nnp.random.seed(13)\nnp.random.shuffle(idx)\nX = X[idx]\ny = y[idx]\n\n# standardize\nmean = X.mean(axis=0)\nstd = X.std(axis=0)\nX = (X - mean) / std\n\nh = .02  # step size in the mesh\n\nclf = SGDClassifier(alpha=0.001, n_iter=100).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis('tight')\n\n# Plot also the training points\nfor i, color in zip(clf.classes_, colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                cmap=plt.cm.Paired)\nplt.title("Decision surface of multi-class SGD")\nplt.axis('tight')\n\n# Plot the three one-against-all classifiers\nxmin, xmax = plt.xlim()\nymin, ymax = plt.ylim()\ncoef = clf.coef_\nintercept = clf.intercept_\n\n\ndef plot_hyperplane(c, color):\n    def line(x0):\n        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n\n    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n             ls="--", color=color)\n\nfor i, color in zip(clf.classes_, colors):\n    plot_hyperplane(i, color)\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html
SGD: convex loss functions	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_loss_functions_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n    loss[z >= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], 'k-',\n         label="Zero-one loss")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0), 'g-',\n         label="Hinge loss")\nplt.plot(xx, -np.minimum(xx, 0), 'm-',\n         label="Perceptron loss")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), 'r-',\n         label="Log loss")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, 'b-',\n         label="Squared hinge loss")\nplt.plot(xx, modified_huber_loss(xx, 1), 'y--',\n         label="Modified Huber loss")\nplt.ylim((0, 8))\nplt.legend(loc="upper right")\nplt.xlabel(r"Decision function $f(x)$")\nplt.ylabel("$L(y, f(x))$")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html
SGD: Penalties	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_penalties_001.png]]	<br><pre><code>from __future__ import division\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef l1(xs):\n    return np.array([np.sqrt((1 - np.sqrt(x ** 2.0)) ** 2.0) for x in xs])\n\n\ndef l2(xs):\n    return np.array([np.sqrt(1.0 - x ** 2.0) for x in xs])\n\n\ndef el(xs, z):\n    return np.array([(2 - 2 * x - 2 * z + 4 * x * z -\n                      (4 * z ** 2\n                       - 8 * x * z ** 2\n                       + 8 * x ** 2 * z ** 2\n                       - 16 * x ** 2 * z ** 3\n                       + 8 * x * z ** 3 + 4 * x ** 2 * z ** 4) ** (1. / 2)\n                      - 2 * x * z ** 2) / (2 - 4 * z) for x in xs])\n\n\ndef cross(ext):\n    plt.plot([-ext, ext], [0, 0], "k-")\n    plt.plot([0, 0], [-ext, ext], "k-")\n\nxs = np.linspace(0, 1, 100)\n\nalpha = 0.501  # 0.5 division throuh zero\n\ncross(1.2)\n\nplt.plot(xs, l1(xs), "r-", label="L1")\nplt.plot(xs, -1.0 * l1(xs), "r-")\nplt.plot(-1 * xs, l1(xs), "r-")\nplt.plot(-1 * xs, -1.0 * l1(xs), "r-")\n\nplt.plot(xs, l2(xs), "b-", label="L2")\nplt.plot(xs, -1.0 * l2(xs), "b-")\nplt.plot(-1 * xs, l2(xs), "b-")\nplt.plot(-1 * xs, -1.0 * l2(xs), "b-")\n\nplt.plot(xs, el(xs, alpha), "y-", label="Elastic Net")\nplt.plot(xs, -1.0 * el(xs, alpha), "y-")\nplt.plot(-1 * xs, el(xs, alpha), "y-")\nplt.plot(-1 * xs, -1.0 * el(xs, alpha), "y-")\n\nplt.xlabel(r"$w_0$")\nplt.ylabel(r"$w_1$")\nplt.legend()\n\nplt.axis("equal")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html
SGD: Maximum margin separating hyperplane	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_separating_hyperplane_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets.samples_generator import make_blobs\n\n# we create 50 separable points\nX, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = SGDClassifier(loss="hinge", alpha=0.01, n_iter=200, fit_intercept=True)\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = np.linspace(-1, 5, 10)\nyy = np.linspace(-1, 5, 10)\n\nX1, X2 = np.meshgrid(xx, yy)\nZ = np.empty(X1.shape)\nfor (i, j), val in np.ndenumerate(X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_separating_hyperplane.html
SGD: Weighted samples	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sgd_weighted_samples_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight = 100 * np.abs(np.random.randn(20))\n# and assign a bigger weight to the last 10 samples\nsample_weight[:10] *= 10\n\n# plot the weighted data points\nxx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\nplt.figure()\nplt.scatter(X[:, 0], X[:, 1], c=y, s=sample_weight, alpha=0.9,\n            cmap=plt.cm.bone)\n\n## fit the unweighted model\nclf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)\nclf.fit(X, y)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nno_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n\n## fit the weighted model\nclf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)\nclf.fit(X, y, sample_weight=sample_weight)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsamples_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['dashed'])\n\nplt.legend([no_weights.collections[0], samples_weights.collections[0]],\n           ["no weights", "with weights"], loc="lower left")\n\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_weighted_samples.html
Sparse coding with a precomputed dictionary	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_sparse_coding_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pylab as pl\n\nfrom sklearn.decomposition import SparseCoder\n\n\ndef ricker_function(resolution, center, width):\n    """Discrete sub-sampled Ricker (Mexican hat) wavelet"""\n    x = np.linspace(0, resolution - 1, resolution)\n    x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n         * (1 - ((x - center) ** 2 / width ** 2))\n         * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n    return x\n\n\ndef ricker_matrix(width, resolution, n_components):\n    """Dictionary of Ricker (Mexican hat) wavelets"""\n    centers = np.linspace(0, resolution - 1, n_components)\n    D = np.empty((n_components, resolution))\n    for i, center in enumerate(centers):\n        D[i] = ricker_function(resolution, center, width)\n    D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n    return D\n\n\nresolution = 1024\nsubsampling = 3  # subsampling factor\nwidth = 100\nn_components = resolution / subsampling\n\n# Compute a wavelet dictionary\nD_fixed = ricker_matrix(width=width, resolution=resolution,\n                        n_components=n_components)\nD_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,\n                                    n_components=np.floor(n_components / 5))\n                for w in (10, 50, 100, 500, 1000))]\n\n# Generate a signal\ny = np.linspace(0, resolution - 1, resolution)\nfirst_quarter = y < resolution / 4\ny[first_quarter] = 3.\ny[np.logical_not(first_quarter)] = -1.\n\n# List the different sparse coding methods in the following format:\n# (title, transform_algorithm, transform_alpha, transform_n_nozero_coefs)\nestimators = [('OMP', 'omp', None, 15), ('Lasso', 'lasso_cd', 2, None), ]\n\npl.figure(figsize=(13, 6))\nfor subplot, (D, title) in enumerate(zip((D_fixed, D_multi),\n                                         ('fixed width', 'multiple widths'))):\n    pl.subplot(1, 2, subplot + 1)\n    pl.title('Sparse coding against %s dictionary' % title)\n    pl.plot(y, ls='dotted', label='Original signal')\n    # Do a wavelet approximation\n    for title, algo, alpha, n_nonzero in estimators:\n        coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,\n                            transform_alpha=alpha, transform_algorithm=algo)\n        x = coder.transform(y.reshape(1, -1))\n        density = len(np.flatnonzero(x))\n        x = np.ravel(np.dot(x, D))\n        squared_error = np.sum((y - x) ** 2)\n        pl.plot(x, label='%s: %s nonzero coefs,\n%.2f error'\n                % (title, density, squared_error))\n\n    # Soft thresholding debiasing\n    coder = SparseCoder(dictionary=D, transform_algorithm='threshold',\n                        transform_alpha=20)\n    x = coder.transform(y.reshape(1, -1))\n    _, idx = np.where(x != 0)\n    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y)\n    x = np.ravel(np.dot(x, D))\n    squared_error = np.sum((y - x) ** 2)\n    pl.plot(x,\n            label='Thresholding w/ debiasing:\n%d nonzero coefs, %.2f error' %\n            (len(idx), squared_error))\n    pl.axis('tight')\n    pl.legend()\npl.subplots_adjust(.04, .07, .97, .90, .09, .2)\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_sparse_coding.html
Sparse inverse covariance estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n# author: Gael Varoquaux <gael.varoquaux@inria.fr>\n# License: BSD 3 clause\n# Copyright: INRIA\n\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import make_sparse_spd_matrix\nfrom sklearn.covariance import GraphLassoCV, ledoit_wolf\nimport matplotlib.pyplot as plt\n\n##############################################################################\n# Generate the data\nn_samples = 60\nn_features = 20\n\nprng = np.random.RandomState(1)\nprec = make_sparse_spd_matrix(n_features, alpha=.98,\n                              smallest_coef=.4,\n                              largest_coef=.7,\n                              random_state=prng)\ncov = linalg.inv(prec)\nd = np.sqrt(np.diag(cov))\ncov /= d\ncov /= d[:, np.newaxis]\nprec *= d\nprec *= d[:, np.newaxis]\nX = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\n##############################################################################\n# Estimate the covariance\nemp_cov = np.dot(X.T, X) / n_samples\n\nmodel = GraphLassoCV()\nmodel.fit(X)\ncov_ = model.covariance_\nprec_ = model.precision_\n\nlw_cov_, _ = ledoit_wolf(X)\nlw_prec_ = linalg.inv(lw_cov_)\n\n##############################################################################\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.subplots_adjust(left=0.02, right=0.98)\n\n# plot the covariances\ncovs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),\n        ('GraphLasso', cov_), ('True', cov)]\nvmax = cov_.max()\nfor i, (name, this_cov) in enumerate(covs):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s covariance' % name)\n\n\n# plot the precisions\nprecs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),\n         ('GraphLasso', prec_), ('True', prec)]\nvmax = .9 * prec_.max()\nfor i, (name, this_prec) in enumerate(precs):\n    ax = plt.subplot(2, 4, i + 5)\n    plt.imshow(np.ma.masked_equal(this_prec, 0),\n               interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s precision' % name)\n    ax.set_axis_bgcolor('.7')\n\n# plot the model selection metric\nplt.figure(figsize=(4, 3))\nplt.axes([.2, .15, .75, .7])\nplt.plot(model.cv_alphas_, np.mean(model.grid_scores, axis=1), 'o-')\nplt.axvline(model.alpha_, color='.5')\nplt.title('Model selection')\nplt.ylabel('Cross-validation score')\nplt.xlabel('alpha')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html
Sparse recovery: feature selection for sparse linear models	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort and Gael Varoquaux\n# License: BSD 3 clause\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import linalg\n\nfrom sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n                                  LassoLarsCV)\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.utils.extmath import pinvh\nfrom sklearn.utils import ConvergenceWarning\n\n\ndef mutual_incoherence(X_relevant, X_irelevant):\n    """Mutual incoherence, as defined by formula (26a) of [Wainwright2006].\n    """\n    projector = np.dot(np.dot(X_irelevant.T, X_relevant),\n                       pinvh(np.dot(X_relevant.T, X_relevant)))\n    return np.max(np.abs(projector).sum(axis=1))\n\n\nfor conditioning in (1, 1e-4):\n    ###########################################################################\n    # Simulate regression data with a correlated design\n    n_features = 501\n    n_relevant_features = 3\n    noise_level = .2\n    coef_min = .2\n    # The Donoho-Tanner phase transition is around n_samples=25: below we\n    # will completely fail to recover in the well-conditioned case\n    n_samples = 25\n    block_size = n_relevant_features\n\n    rng = np.random.RandomState(42)\n\n    # The coefficients of our model\n    coef = np.zeros(n_features)\n    coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)\n\n    # The correlation of our design: variables correlated by blocs of 3\n    corr = np.zeros((n_features, n_features))\n    for i in range(0, n_features, block_size):\n        corr[i:i + block_size, i:i + block_size] = 1 - conditioning\n    corr.flat[::n_features + 1] = 1\n    corr = linalg.cholesky(corr)\n\n    # Our design\n    X = rng.normal(size=(n_samples, n_features))\n    X = np.dot(X, corr)\n    # Keep [Wainwright2006] (26c) constant\n    X[:n_relevant_features] /= np.abs(\n        linalg.svdvals(X[:n_relevant_features])).max()\n    X = StandardScaler().fit_transform(X.copy())\n\n    # The output variable\n    y = np.dot(X, coef)\n    y /= np.std(y)\n    # We scale the added noise as a function of the average correlation\n    # between the design and the output variable\n    y += noise_level * rng.normal(size=n_samples)\n    mi = mutual_incoherence(X[:, :n_relevant_features],\n                            X[:, n_relevant_features:])\n\n    ###########################################################################\n    # Plot stability selection path, using a high eps for early stopping\n    # of the path, to save computation time\n    alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42,\n                                                   eps=0.05)\n\n    plt.figure()\n    # We plot the path as a function of alpha/alpha_max to the power 1/3: the\n    # power 1/3 scales the path less brutally than the log, and enables to\n    # see the progression along the path\n    hg = plt.plot(alpha_grid[1:] ** .333, scores_path[coef != 0].T[1:], 'r')\n    hb = plt.plot(alpha_grid[1:] ** .333, scores_path[coef == 0].T[1:], 'k')\n    ymin, ymax = plt.ylim()\n    plt.xlabel(r'$(\alpha / \alpha_{max})^{1/3}$')\n    plt.ylabel('Stability score: proportion of times selected')\n    plt.title('Stability Scores Path - Mutual incoherence: %.1f' % mi)\n    plt.axis('tight')\n    plt.legend((hg[0], hb[0]), ('relevant features', 'irrelevant features'),\n               loc='best')\n\n    ###########################################################################\n    # Plot the estimated stability scores for a given alpha\n\n    # Use 6-fold cross-validation rather than the default 3-fold: it leads to\n    # a better choice of alpha:\n    # Stop the user warnings outputs- they are not necessary for the example\n    # as it is specifically set up to be challenging.\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n        warnings.simplefilter('ignore', ConvergenceWarning)\n        lars_cv = LassoLarsCV(cv=6).fit(X, y)\n\n    # Run the RandomizedLasso: we use a paths going down to .1*alpha_max\n    # to avoid exploring the regime in which very noisy variables enter\n    # the model\n    alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6)\n    clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y)\n    trees = ExtraTreesRegressor(100).fit(X, y)\n    # Compare with F-score\n    F, _ = f_regression(X, y)\n\n    plt.figure()\n    for name, score in [('F-test', F),\n                        ('Stability selection', clf.scores_),\n                        ('Lasso coefs', np.abs(lars_cv.coef_)),\n                        ('Trees', trees.feature_importances_),\n                        ]:\n        precision, recall, thresholds = precision_recall_curve(coef != 0,\n                                                               score)\n        plt.semilogy(np.maximum(score / np.max(score), 1e-4),\n                     label="%s. AUC: %.3f" % (name, auc(recall, precision)))\n\n    plt.plot(np.where(coef != 0)[0], [2e-4] * n_relevant_features, 'mo',\n             label="Ground truth")\n    plt.xlabel("Features")\n    plt.ylabel("Score")\n    # Plot only the 100 first coefficients\n    plt.xlim(0, 100)\n    plt.legend(loc='best')\n    plt.title('Feature selection scores - Mutual incoherence: %.1f'\n              % mi)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_recovery.html
Species distribution modeling	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_species_distribution_modeling_001.png]]	<br><pre><code># Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Jake Vanderplas <vanderplas@astro.washington.edu>\n#\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets.base import Bunch\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn.datasets.species_distributions import construct_grids\nfrom sklearn import svm, metrics\n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\nprint(__doc__)\n\n\ndef create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n    """Create a bunch with information about a particular organism\n\n    This will use the test/train record arrays to extract the\n    data specific to the given species name.\n    """\n    bunch = Bunch(name=' '.join(species_name.split("_")[:2]))\n    species_name = species_name.encode('ascii')\n    points = dict(test=test, train=train)\n\n    for label, pts in points.items():\n        # choose points associated with the desired species\n        pts = pts[pts['species'] == species_name]\n        bunch['pts_%s' % label] = pts\n\n        # determine coverage values for each of the training & testing points\n        ix = np.searchsorted(xgrid, pts['dd long'])\n        iy = np.searchsorted(ygrid, pts['dd lat'])\n        bunch['cov_%s' % label] = coverages[:, -iy, ix].T\n\n    return bunch\n\n\ndef plot_species_distribution(species=("bradypus_variegatus_0",\n                                       "microryzomys_minutus_0")):\n    """\n    Plot the species distribution.\n    """\n    if len(species) > 2:\n        print("Note: when more than two species are provided,"\n              " only the first two will be used")\n\n    t0 = time()\n\n    # Load the compressed data\n    data = fetch_species_distributions()\n\n    # Set up the data grid\n    xgrid, ygrid = construct_grids(data)\n\n    # The grid in x,y coordinates\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n\n    # create a bunch for each species\n    BV_bunch = create_species_bunch(species[0],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n    MM_bunch = create_species_bunch(species[1],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n\n    # background points (grid coordinates) for evaluation\n    np.random.seed(13)\n    background_points = np.c_[np.random.randint(low=0, high=data.Ny,\n                                                size=10000),\n                              np.random.randint(low=0, high=data.Nx,\n                                                size=10000)].T\n\n    # We'll make use of the fact that coverages[6] has measurements at all\n    # land points.  This will help us decide between land and water.\n    land_reference = data.coverages[6]\n\n    # Fit, predict, and plot for each species.\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print("_" * 80)\n        print("Modeling distribution of species '%s'" % species.name)\n\n        # Standardize features\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n\n        # Fit OneClassSVM\n        print(" - fit OneClassSVM ... ", end='')\n        clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.5)\n        clf.fit(train_cover_std)\n        print("done.")\n\n        # Plot map of South America\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print(" - plot coastlines using basemap")\n            m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n                        urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                        urcrnrlon=X.max(), resolution='c')\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(" - plot coastlines from coverage")\n            plt.contour(X, Y, land_reference,\n                        levels=[-9999], colors="k",\n                        linestyles="solid")\n            plt.xticks([])\n            plt.yticks([])\n\n        print(" - predict species distribution")\n\n        # Predict species distribution using the training data\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n\n        # We'll predict only for the land points.\n        idx = np.where(land_reference > -9999)\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n\n        pred = clf.decision_function((coverages_land - mean) / std)[:, 0]\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n\n        # plot contours of the prediction\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format='%.2f')\n\n        # scatter training/testing points\n        plt.scatter(species.pts_train['dd long'], species.pts_train['dd lat'],\n                    s=2 ** 2, c='black',\n                    marker='^', label='train')\n        plt.scatter(species.pts_test['dd long'], species.pts_test['dd lat'],\n                    s=2 ** 2, c='black',\n                    marker='x', label='test')\n        plt.legend()\n        plt.title(species.name)\n        plt.axis('equal')\n\n        # Compute AUC with regards to background points\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean)\n                                          / std)[:, 0]\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, "AUC: %.3f" % roc_auc, ha="right")\n        print("\n Area under the ROC curve : %f" % roc_auc)\n\n    print("\ntime elapsed: %.2fs" % (time() - t0))\n\n\nplot_species_distribution()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_species_distribution_modeling.html
Kernel Density Estimate of Species Distributions	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_species_kde_001.png]]	<br><pre><code># Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn.datasets.species_distributions import construct_grids\nfrom sklearn.neighbors import KernelDensity\n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\n# Get matrices/arrays of species IDs and locations\ndata = fetch_species_distributions()\nspecies_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n\nXtrain = np.vstack([data['train']['dd lat'],\n                    data['train']['dd long']]).T\nytrain = np.array([d.decode('ascii').startswith('micro')\n                  for d in data['train']['species']], dtype='int')\nXtrain *= np.pi / 180.  # Convert lat/long to radians\n\n# Set up the data grid for the contour plot\nxgrid, ygrid = construct_grids(data)\nX, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\nland_reference = data.coverages[6][::5, ::5]\nland_mask = (land_reference > -9999).ravel()\n\nxy = np.vstack([Y.ravel(), X.ravel()]).T\nxy = xy[land_mask]\nxy *= np.pi / 180.\n\n# Plot map of South America with distributions of each species\nfig = plt.figure()\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n\nfor i in range(2):\n    plt.subplot(1, 2, i + 1)\n\n    # construct a kernel density estimate of the distribution\n    print(" - computing KDE in spherical coordinates")\n    kde = KernelDensity(bandwidth=0.04, metric='haversine',\n                        kernel='gaussian', algorithm='ball_tree')\n    kde.fit(Xtrain[ytrain == i])\n\n    # evaluate only on the land: -9999 indicates ocean\n    Z = -9999 + np.zeros(land_mask.shape[0])\n    Z[land_mask] = np.exp(kde.score_samples(xy))\n    Z = Z.reshape(X.shape)\n\n    # plot contours of the density\n    levels = np.linspace(0, Z.max(), 25)\n    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n\n    if basemap:\n        print(" - plot coastlines using basemap")\n        m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                    urcrnrlon=X.max(), resolution='c')\n        m.drawcoastlines()\n        m.drawcountries()\n    else:\n        print(" - plot coastlines from coverage")\n        plt.contour(X, Y, land_reference,\n                    levels=[-9999], colors="k",\n                    linestyles="solid")\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.title(species_names[i])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html
A demo of the Spectral Biclustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_checkerboard\nfrom sklearn.datasets import samples_generator as sg\nfrom sklearn.cluster.bicluster import SpectralBiclustering\nfrom sklearn.metrics import consensus_score\n\nn_clusters = (4, 3)\ndata, rows, columns = make_checkerboard(\n    shape=(300, 300), n_clusters=n_clusters, noise=10,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Original dataset")\n\ndata, row_idx, col_idx = sg._shuffle(data, random_state=0)\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Shuffled dataset")\n\nmodel = SpectralBiclustering(n_clusters=n_clusters, method='log',\n                             random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint("consensus score: {:.1f}".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title("After biclustering; rearranged to show biclusters")\n\nplt.matshow(np.outer(np.sort(model.row_labels_) + 1,\n                     np.sort(model.column_labels_) + 1),\n            cmap=plt.cm.Blues)\nplt.title("Checkerboard structure of rearranged data")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html
A demo of the Spectral Co-Clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_biclusters\nfrom sklearn.datasets import samples_generator as sg\nfrom sklearn.cluster.bicluster import SpectralCoclustering\nfrom sklearn.metrics import consensus_score\n\ndata, rows, columns = make_biclusters(\n    shape=(300, 300), n_clusters=5, noise=5,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Original dataset")\n\ndata, row_idx, col_idx = sg._shuffle(data, random_state=0)\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Shuffled dataset")\n\nmodel = SpectralCoclustering(n_clusters=5, random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint("consensus score: {:.3f}".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title("After biclustering; rearranged to show biclusters")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_coclustering.html
Visualizing the stock market structure	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_stock_market_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD 3 clause\n\nimport datetime\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    from matplotlib.finance import quotes_historical_yahoo\nexcept ImportError:\n    from matplotlib.finance import quotes_historical_yahoo_ochl as quotes_historical_yahoo\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import cluster, covariance, manifold\n\n###############################################################################\n# Retrieve the data from Internet\n\n# Choose a time period reasonnably calm (not too long ago so that we get\n# high-tech firms, and before the 2008 crash)\nd1 = datetime.datetime(2003, 1, 1)\nd2 = datetime.datetime(2008, 1, 1)\n\n# kraft symbol has now changed from KFT to MDLZ in yahoo\nsymbol_dict = {\n    'TOT': 'Total',\n    'XOM': 'Exxon',\n    'CVX': 'Chevron',\n    'COP': 'ConocoPhillips',\n    'VLO': 'Valero Energy',\n    'MSFT': 'Microsoft',\n    'IBM': 'IBM',\n    'TWX': 'Time Warner',\n    'CMCSA': 'Comcast',\n    'CVC': 'Cablevision',\n    'YHOO': 'Yahoo',\n    'DELL': 'Dell',\n    'HPQ': 'HP',\n    'AMZN': 'Amazon',\n    'TM': 'Toyota',\n    'CAJ': 'Canon',\n    'MTU': 'Mitsubishi',\n    'SNE': 'Sony',\n    'F': 'Ford',\n    'HMC': 'Honda',\n    'NAV': 'Navistar',\n    'NOC': 'Northrop Grumman',\n    'BA': 'Boeing',\n    'KO': 'Coca Cola',\n    'MMM': '3M',\n    'MCD': 'Mc Donalds',\n    'PEP': 'Pepsi',\n    'MDLZ': 'Kraft Foods',\n    'K': 'Kellogg',\n    'UN': 'Unilever',\n    'MAR': 'Marriott',\n    'PG': 'Procter Gamble',\n    'CL': 'Colgate-Palmolive',\n    'GE': 'General Electrics',\n    'WFC': 'Wells Fargo',\n    'JPM': 'JPMorgan Chase',\n    'AIG': 'AIG',\n    'AXP': 'American express',\n    'BAC': 'Bank of America',\n    'GS': 'Goldman Sachs',\n    'AAPL': 'Apple',\n    'SAP': 'SAP',\n    'CSCO': 'Cisco',\n    'TXN': 'Texas instruments',\n    'XRX': 'Xerox',\n    'LMT': 'Lookheed Martin',\n    'WMT': 'Wal-Mart',\n    'WBA': 'Walgreen',\n    'HD': 'Home Depot',\n    'GSK': 'GlaxoSmithKline',\n    'PFE': 'Pfizer',\n    'SNY': 'Sanofi-Aventis',\n    'NVS': 'Novartis',\n    'KMB': 'Kimberly-Clark',\n    'R': 'Ryder',\n    'GD': 'General Dynamics',\n    'RTN': 'Raytheon',\n    'CVS': 'CVS',\n    'CAT': 'Caterpillar',\n    'DD': 'DuPont de Nemours'}\n\nsymbols, names = np.array(list(symbol_dict.items())).T\n\nquotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)\n          for symbol in symbols]\n\nopen = np.array([q.open for q in quotes]).astype(np.float)\nclose = np.array([q.close for q in quotes]).astype(np.float)\n\n# The daily variations of the quotes are what carry most information\nvariation = close - open\n\n###############################################################################\n# Learn a graphical structure from the correlations\nedge_model = covariance.GraphLassoCV()\n\n# standardize the time series: using correlations rather than covariance\n# is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)\n\n###############################################################################\n# Cluster using affinity propagation\n\n_, labels = cluster.affinity_propagation(edge_model.covariance_)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))\n\n###############################################################################\n# Find a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\n# We use a dense eigen_solver to achieve reproducibility (arpack is\n# initiated with random vectors that we don't control). In addition, we\n# use a large number of neighbors to capture the large-scale structure.\nnode_position_model = manifold.LocallyLinearEmbedding(\n    n_components=2, eigen_solver='dense', n_neighbors=6)\n\nembedding = node_position_model.fit_transform(X.T).T\n\n###############################################################################\n# Visualization\nplt.figure(1, facecolor='w', figsize=(10, 8))\nplt.clf()\nax = plt.axes([0., 0., 1., 1.])\nplt.axis('off')\n\n# Display a graph of the partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / np.sqrt(np.diag(partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, np.newaxis]\nnon_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n\n# Plot the nodes using the coordinates of our embedding\nplt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n            cmap=plt.cm.spectral)\n\n# Plot the edges\nstart_idx, end_idx = np.where(non_zero)\n#a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[embedding[:, start], embedding[:, stop]]\n            for start, stop in zip(start_idx, end_idx)]\nvalues = np.abs(partial_correlations[non_zero])\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, .7 * values.max()))\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(\n        zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[np.argmin(np.abs(dy))]\n    this_dy = dy[np.argmin(np.abs(dx))]\n    if this_dx > 0:\n        horizontalalignment = 'left'\n        x = x + .002\n    else:\n        horizontalalignment = 'right'\n        x = x - .002\n    if this_dy > 0:\n        verticalalignment = 'bottom'\n        y = y + .002\n    else:\n        verticalalignment = 'top'\n        y = y - .002\n    plt.text(x, y, name, size=10,\n             horizontalalignment=horizontalalignment,\n             verticalalignment=verticalalignment,\n             bbox=dict(facecolor='w',\n                       edgecolor=plt.cm.spectral(label / float(n_labels)),\n                       alpha=.6))\n\nplt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n         embedding[0].max() + .10 * embedding[0].ptp(),)\nplt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n         embedding[1].max() + .03 * embedding[1].ptp())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html
SVM-Anova: SVM with univariate feature selection	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_svm_anova_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets, feature_selection, cross_validation\nfrom sklearn.pipeline import Pipeline\n\n###############################################################################\n# Import some data to play with\ndigits = datasets.load_digits()\ny = digits.target\n# Throw away data, to be in the curse of dimension settings\ny = y[:200]\nX = digits.data[:200]\nn_samples = len(y)\nX = X.reshape((n_samples, -1))\n# add 200 non-informative features\nX = np.hstack((X, 2 * np.random.random((n_samples, 200))))\n\n###############################################################################\n# Create a feature-selection transform and an instance of SVM that we\n# combine together to have an full-blown estimator\n\ntransform = feature_selection.SelectPercentile(feature_selection.f_classif)\n\nclf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0))])\n\n###############################################################################\n# Plot the cross-validation score as a function of percentile of features\nscore_means = list()\nscore_stds = list()\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n\nfor percentile in percentiles:\n    clf.set_params(anova__percentile=percentile)\n    # Compute cross-validation score using all CPUs\n    this_scores = cross_validation.cross_val_score(clf, X, y, n_jobs=1)\n    score_means.append(this_scores.mean())\n    score_stds.append(this_scores.std())\n\nplt.errorbar(percentiles, score_means, np.array(score_stds))\n\nplt.title(\n    'Performance of the SVM-Anova varying the percentile of features selected')\nplt.xlabel('Percentile')\nplt.ylabel('Prediction rate')\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html
SVM-Kernels	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\n# Our dataset and targets\nX = np.c_[(.4, -.7),\n          (-1.5, -1),\n          (-1.4, -.9),\n          (-1.3, -1.2),\n          (-1.1, -.2),\n          (-1.2, -.4),\n          (-.5, 1.2),\n          (-1.5, 2.1),\n          (1, 1),\n          # --\n          (1.3, .8),\n          (1.2, .5),\n          (.2, -2),\n          (.5, -2.4),\n          (.2, -2.3),\n          (0, -2.7),\n          (1.3, 2.1)].T\nY = [0] * 8 + [1] * 8\n\n# figure number\nfignum = 1\n\n# fit the model\nfor kernel in ('linear', 'poly', 'rbf'):\n    clf = svm.SVC(kernel=kernel, gamma=2)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10)\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n\n    plt.axis('tight')\n    x_min = -3\n    x_max = 3\n    y_min = -3\n    y_max = 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html
SVM Margins Example	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in (('unreg', 1), ('reg', 0.05)):\n\n    clf = svm.SVC(kernel='linear', C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors\n    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n    yy_down = yy + a * margin\n    yy_up = yy - a * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.plot(xx, yy, 'k-')\n    plt.plot(xx, yy_down, 'k--')\n    plt.plot(xx, yy_up, 'k--')\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10)\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n\n    plt.axis('tight')\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html
Non-linear SVM	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_svm_nonlinear_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 500),\n                     np.linspace(-3, 3, 500))\nnp.random.seed(0)\nX = np.random.randn(300, 2)\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n\n# fit the model\nclf = svm.NuSVC()\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',\n           origin='lower', cmap=plt.cm.PuOr_r)\ncontours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n                       linetypes='--')\nplt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)\nplt.xticks(())\nplt.yticks(())\nplt.axis([-3, 3, -3, 3])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html
Support Vector Regression (SVR) using linear and non-linear kernels	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_svm_regression_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\n\n###############################################################################\n# Generate sample data\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\n\n###############################################################################\n# Add noise to targets\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n###############################################################################\n# Fit regression model\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\nsvr_lin = SVR(kernel='linear', C=1e3)\nsvr_poly = SVR(kernel='poly', C=1e3, degree=2)\ny_rbf = svr_rbf.fit(X, y).predict(X)\ny_lin = svr_lin.fit(X, y).predict(X)\ny_poly = svr_poly.fit(X, y).predict(X)\n\n###############################################################################\n# look at the results\nplt.scatter(X, y, c='k', label='data')\nplt.hold('on')\nplt.plot(X, y_rbf, c='g', label='RBF model')\nplt.plot(X, y_lin, c='r', label='Linear model')\nplt.plot(X, y_poly, c='b', label='Polynomial model')\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('Support Vector Regression')\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html
Scaling the regularization parameter for SVCs	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.utils import check_random_state\nfrom sklearn import datasets\n\n\nrnd = check_random_state(1)\n\n# set up dataset\nn_samples = 100\nn_features = 300\n\n# l1 data (only 5 informative features)\nX_1, y_1 = datasets.make_classification(n_samples=n_samples,\n                                        n_features=n_features, n_informative=5,\n                                        random_state=1)\n\n# l2 data: non sparse, but less features\ny_2 = np.sign(.5 - rnd.rand(n_samples))\nX_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]\nX_2 += 5 * rnd.randn(n_samples, n_features / 5)\n\nclf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,\n                       tol=1e-3),\n             np.logspace(-2.3, -1.3, 10), X_1, y_1),\n            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,\n                       tol=1e-4),\n             np.logspace(-4.5, -2, 10), X_2, y_2)]\n\ncolors = ['b', 'g', 'r', 'c']\n\nfor fignum, (clf, cs, X, y) in enumerate(clf_sets):\n    # set up the plot for each regressor\n    plt.figure(fignum, figsize=(9, 10))\n\n    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):\n        param_grid = dict(C=cs)\n        # To get nice curve, we need a large number of iterations to\n        # reduce the variance\n        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,\n                            cv=ShuffleSplit(n=n_samples, train_size=train_size,\n                                            n_iter=250, random_state=1))\n        grid.fit(X, y)\n        scores = [x[1] for x in grid.grid_scores_]\n\n        scales = [(1, 'No scaling'),\n                  ((n_samples * train_size), '1/n_samples'),\n                  ]\n\n        for subplotnum, (scaler, name) in enumerate(scales):\n            plt.subplot(2, 1, subplotnum + 1)\n            plt.xlabel('C')\n            plt.ylabel('CV Score')\n            grid_cs = cs * float(scaler)  # scale the C's\n            plt.semilogx(grid_cs, scores, label="fraction %.2f" %\n                         train_size)\n            plt.title('scaling=%s, penalty=%s, loss=%s' %\n                      (name, clf.penalty, clf.loss))\n\n    plt.legend(loc="best")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html
Swiss Roll reduction with LLE	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_swissroll_001.png]]	<br><pre><code># Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause (C) INRIA 2011\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\n# This import is needed to modify the way figure behaves\nfrom mpl_toolkits.mplot3d import Axes3D\nAxes3D\n\n#----------------------------------------------------------------------\n# Locally linear embedding of the swiss roll\n\nfrom sklearn import manifold, datasets\nX, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)\n\nprint("Computing LLE embedding")\nX_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nprint("Done. Reconstruction error: %g" % err)\n\n#----------------------------------------------------------------------\n# Plot result\n\nfig = plt.figure()\ntry:\n    # compatibility matplotlib < 1.0\n    ax = fig.add_subplot(211, projection='3d')\n    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\nexcept:\n    ax = fig.add_subplot(211)\n    ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)\n\nax.set_title("Original data")\nax = fig.add_subplot(212)\nax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.axis('tight')\nplt.xticks([]), plt.yticks([])\nplt.title('Projected data')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html
Theil-Sen Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor\nfrom sklearn.linear_model import RANSACRegressor\n\nprint(__doc__)\n\nestimators = [('OLS', LinearRegression()),\n              ('Theil-Sen', TheilSenRegressor(random_state=42)),\n              ('RANSAC', RANSACRegressor(random_state=42)), ]\n\n##############################################################################\n# Outliers only in the y direction\n\nnp.random.seed(0)\nn_samples = 200\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nw = 3.\nc = 2.\nnoise = 0.1 * np.random.randn(n_samples)\ny = w * x + c + noise\n# 10% outliers\ny[-20:] += -20 * x[-20:]\nX = x[:, np.newaxis]\n\nplt.plot(x, y, 'k+', mew=2, ms=8)\nline_x = np.array([-3, 3])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred,\n             label='%s (fit time: %.2fs)' % (name, elapsed_time))\n\nplt.axis('tight')\nplt.legend(loc='upper left')\n\n\n##############################################################################\n# Outliers in the X direction\n\nnp.random.seed(0)\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nnoise = 0.1 * np.random.randn(n_samples)\ny = 3 * x + 2 + noise\n# 10% outliers\nx[-20:] = 9.9\ny[-20:] += 22\nX = x[:, np.newaxis]\n\nplt.figure()\nplt.plot(x, y, 'k+', mew=2, ms=8)\n\nline_x = np.array([-3, 10])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred,\n             label='%s (fit time: %.2fs)' % (name, elapsed_time))\n\nplt.axis('tight')\nplt.legend(loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html
Compressive sensing: tomography reconstruction with L1 prior (Lasso)	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_tomography_l1_reconstruction_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy import ndimage\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ndef _weights(x, dx=1, orig=0):\n    x = np.ravel(x)\n    floor_x = np.floor((x - orig) / dx)\n    alpha = (x - orig - floor_x * dx) / dx\n    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n\n\ndef _generate_center_coordinates(l_x):\n    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n    center = l_x / 2.\n    X += 0.5 - center\n    Y += 0.5 - center\n    return X, Y\n\n\ndef build_projection_operator(l_x, n_dir):\n    """ Compute the tomography design matrix.\n\n    Parameters\n    ----------\n\n    l_x : int\n        linear size of image array\n\n    n_dir : int\n        number of angles at which projections are acquired.\n\n    Returns\n    -------\n    p : sparse matrix of shape (n_dir l_x, l_x**2)\n    """\n    X, Y = _generate_center_coordinates(l_x)\n    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n    data_inds, weights, camera_inds = [], [], []\n    data_unravel_indices = np.arange(l_x ** 2)\n    data_unravel_indices = np.hstack((data_unravel_indices,\n                                      data_unravel_indices))\n    for i, angle in enumerate(angles):\n        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n        inds, w = _weights(Xrot, dx=1, orig=X.min())\n        mask = np.logical_and(inds >= 0, inds < l_x)\n        weights += list(w[mask])\n        camera_inds += list(inds[mask] + i * l_x)\n        data_inds += list(data_unravel_indices[mask])\n    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n    return proj_operator\n\n\ndef generate_synthetic_data():\n    """ Synthetic binary data """\n    rs = np.random.RandomState(0)\n    n_pts = 36.\n    x, y = np.ogrid[0:l, 0:l]\n    mask_outer = (x - l / 2) ** 2 + (y - l / 2) ** 2 < (l / 2) ** 2\n    mask = np.zeros((l, l))\n    points = l * rs.rand(2, n_pts)\n    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n    res = np.logical_and(mask > mask.mean(), mask_outer)\n    return res - ndimage.binary_erosion(res)\n\n\n# Generate synthetic images, and projections\nl = 128\nproj_operator = build_projection_operator(l, l / 7.)\ndata = generate_synthetic_data()\nproj = proj_operator * data.ravel()[:, np.newaxis]\nproj += 0.15 * np.random.randn(*proj.shape)\n\n# Reconstruction with L2 (Ridge) penalization\nrgr_ridge = Ridge(alpha=0.2)\nrgr_ridge.fit(proj_operator, proj.ravel())\nrec_l2 = rgr_ridge.coef_.reshape(l, l)\n\n# Reconstruction with L1 (Lasso) penalization\n# the best value of alpha was determined using cross validation\n# with LassoCV\nrgr_lasso = Lasso(alpha=0.001)\nrgr_lasso.fit(proj_operator, proj.ravel())\nrec_l1 = rgr_lasso.coef_.reshape(l, l)\n\nplt.figure(figsize=(8, 3.3))\nplt.subplot(131)\nplt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\nplt.axis('off')\nplt.title('original image')\nplt.subplot(132)\nplt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\nplt.title('L2 penalization')\nplt.axis('off')\nplt.subplot(133)\nplt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\nplt.title('L1 penalization')\nplt.axis('off')\n\nplt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n                    right=1)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html
Train error vs Test error	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_train_error_vs_test_error_001.png]]	<br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn import linear_model\n\n###############################################################################\n# Generate sample data\nn_samples_train, n_samples_test, n_features = 75, 150, 500\nnp.random.seed(0)\ncoef = np.random.randn(n_features)\ncoef[50:] = 0.0  # only the top 10 features are impacting the model\nX = np.random.randn(n_samples_train + n_samples_test, n_features)\ny = np.dot(X, coef)\n\n# Split train and test data\nX_train, X_test = X[:n_samples_train], X[n_samples_train:]\ny_train, y_test = y[:n_samples_train], y[n_samples_train:]\n\n###############################################################################\n# Compute train and test errors\nalphas = np.logspace(-5, 1, 60)\nenet = linear_model.ElasticNet(l1_ratio=0.7)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = np.argmax(test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint("Optimal regularization parameter : %s" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_\n\n###############################################################################\n# Plot results functions\n\nimport matplotlib.pyplot as plt\nplt.subplot(2, 1, 1)\nplt.semilogx(alphas, train_errors, label='Train')\nplt.semilogx(alphas, test_errors, label='Test')\nplt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n           linewidth=3, label='Optimum on test')\nplt.legend(loc='lower left')\nplt.ylim([0, 1.2])\nplt.xlabel('Regularization parameter')\nplt.ylabel('Performance')\n\n# Show estimated coef_ vs true coef\nplt.subplot(2, 1, 2)\nplt.plot(coef, label='True coef')\nplt.plot(coef_, label='Estimated coef')\nplt.legend()\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html
Decision Tree Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_tree_regression_001.png]]	<br><pre><code>print(__doc__)\n\n# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c="k", label="data")\nplt.plot(X_test, y_1, c="g", label="max_depth=2", linewidth=2)\nplt.plot(X_test, y_2, c="r", label="max_depth=5", linewidth=2)\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html
Multi-output Decision Tree Regression	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_tree_regression_multioutput_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\ny[::5, :] += (0.5 - rng.rand(20, 2))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_3 = DecisionTreeRegressor(max_depth=8)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\nregr_3.fit(X, y)\n\n# Predict\nX_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_3.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(y[:, 0], y[:, 1], c="k", label="data")\nplt.scatter(y_1[:, 0], y_1[:, 1], c="g", label="max_depth=2")\nplt.scatter(y_2[:, 0], y_2[:, 1], c="r", label="max_depth=5")\nplt.scatter(y_3[:, 0], y_3[:, 1], c="b", label="max_depth=8")\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Multi-output Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html
Underfitting vs. Overfitting	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_underfitting_overfitting_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import cross_validation\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\ntrue_fun = lambda X: np.cos(1.5 * np.pi * X)\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([("polynomial_features", polynomial_features),\n                         ("linear_regression", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_validation.cross_val_score(pipeline,\n        X[:, np.newaxis], y, scoring="mean_squared_error", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")\n    plt.plot(X_test, true_fun(X_test), label="True function")\n    plt.scatter(X, y, label="Samples")\n    plt.xlabel("x")\n    plt.ylabel("y")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc="best")\n    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html
Plotting Validation Curves	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_validation_curve_001.png]]	<br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.learning_curve import validation_curve\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    SVC(), X, y, param_name="gamma", param_range=param_range,\n    cv=10, scoring="accuracy", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title("Validation Curve with SVM")\nplt.xlabel("$\gamma$")\nplt.ylabel("Score")\nplt.ylim(0.0, 1.1)\nplt.semilogx(param_range, train_scores_mean, label="Training score", color="r")\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2, color="r")\nplt.semilogx(param_range, test_scores_mean, label="Cross-validation score",\n             color="g")\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color="g")\nplt.legend(loc="best")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html
Plot the decision boundaries of a VotingClassifier	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_voting_decision_regions_001.png]]	<br><pre><code>print(__doc__)\n\nfrom itertools import product\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),\n                                    ('svc', clf3)],\n                        voting='soft', weights=[2, 1, 2])\n\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\neclf.fit(X, y)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [clf1, clf2, clf3, eclf],\n                        ['Decision Tree (depth=4)', 'KNN (k=7)',\n                         'Kernel SVM', 'Soft Voting']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html
Plot class probabilities calculated by the VotingClassifier	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_voting_probas_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(random_state=123)\nclf2 = RandomForestClassifier(random_state=123)\nclf3 = GaussianNB()\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                        voting='soft',\n                        weights=[1, 1, 5])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')\n\n# plot annotations\nplt.axvline(2.8, color='k', linestyle='dashed')\nax.set_xticks(ind + width)\nax.set_xticklabels(['LogisticRegression\nweight 1',\n                    'GaussianNB\nweight 1',\n                    'RandomForestClassifier\nweight 5',\n                    'VotingClassifier\n(average probabilities)'],\n                   rotation=40,\n                   ha='right')\nplt.ylim([0, 1])\nplt.title('Class probabilities for sample 1 by different classifiers')\nplt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html
Hierarchical clustering: structured vs unstructured ward	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Authors : Vincent Michel, 2010\n#           Alexandre Gramfort, 2010\n#           Gael Varoquaux, 2010\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets.samples_generator import make_swiss_roll\n\n###############################################################################\n# Generate data (swiss roll dataset)\nn_samples = 1500\nnoise = 0.05\nX, _ = make_swiss_roll(n_samples, noise)\n# Make it thinner\nX[:, 1] *= .5\n\n###############################################################################\n# Compute clustering\nprint("Compute unstructured hierarchical clustering...")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint("Elapsed time: %.2fs" % elapsed_time)\nprint("Number of points: %i" % label.size)\n\n###############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n              'o', color=plt.cm.jet(np.float(l) / np.max(label + 1)))\nplt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)\n\n\n###############################################################################\n# Define the structure A of the data. Here a 10 nearest neighbors\nfrom sklearn.neighbors import kneighbors_graph\nconnectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n\n###############################################################################\n# Compute clustering\nprint("Compute structured hierarchical clustering...")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, connectivity=connectivity,\n                               linkage='ward').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint("Elapsed time: %.2fs" % elapsed_time)\nprint("Number of points: %i" % label.size)\n\n###############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n              'o', color=plt.cm.jet(float(l) / np.max(label + 1)))\nplt.title('With connectivity constraints (time %.2fs)' % elapsed_time)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html
SVM: Weighted samples	A							http://scikit-learn.org/stable/auto_examples/index.html		[[Image:http://scikit-learn.org/stable/_images/plot_weighted_samples_001.png]]	<br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\ndef plot_decision_function(classifier, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(X[:, 0], X[:, 1], c=Y, s=100 * sample_weight, alpha=0.9,\n                 cmap=plt.cm.bone)\n\n    axis.axis('off')\n    axis.set_title(title)\n\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\nY = [1] * 10 + [-1] * 10\nsample_weight_last_ten = abs(np.random.randn(len(X)))\nsample_weight_constant = np.ones(len(X))\n# and bigger weights to some outliers\nsample_weight_last_ten[15:] *= 5\nsample_weight_last_ten[9] *= 15\n\n# for reference, first fit without class weights\n\n# fit the model\nclf_weights = svm.SVC()\nclf_weights.fit(X, Y, sample_weight=sample_weight_last_ten)\n\nclf_no_weights = svm.SVC()\nclf_no_weights.fit(X, Y)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nplot_decision_function(clf_no_weights, sample_weight_constant, axes[0],\n                       "Constant weights")\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1],\n                       "Modified weights")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html
Comparing randomized search and grid search for hyperparameter estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>print(__doc__)\n\nimport numpy as np\n\nfrom time import time\nfrom operator import itemgetter\nfrom scipy.stats import randint as sp_randint\n\nfrom sklearn.grid_search import GridSearchCV, RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\n\n# get some data\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# build a classifier\nclf = RandomForestClassifier(n_estimators=20)\n\n\n# Utility function to report best scores\ndef report(grid_scores, n_top=3):\n    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n    for i, score in enumerate(top_scores):\n        print("Model with rank: {0}".format(i + 1))\n        print("Mean validation score: {0:.3f} (std: {1:.3f})".format(\n              score.mean_validation_score,\n              np.std(score.cv_validation_scores)))\n        print("Parameters: {0}".format(score.parameters))\n        print("")\n\n\n# specify parameters and distributions to sample from\nparam_dist = {"max_depth": [3, None],\n              "max_features": sp_randint(1, 11),\n              "min_samples_split": sp_randint(1, 11),\n              "min_samples_leaf": sp_randint(1, 11),\n              "bootstrap": [True, False],\n              "criterion": ["gini", "entropy"]}\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n\nstart = time()\nrandom_search.fit(X, y)\nprint("RandomizedSearchCV took %.2f seconds for %d candidates"\n      " parameter settings." % ((time() - start), n_iter_search))\nreport(random_search.grid_scores_)\n\n# use a full grid over all parameters\nparam_grid = {"max_depth": [3, None],\n              "max_features": [1, 3, 10],\n              "min_samples_split": [1, 3, 10],\n              "min_samples_leaf": [1, 3, 10],\n              "bootstrap": [True, False],\n              "criterion": ["gini", "entropy"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\nstart = time()\ngrid_search.fit(X, y)\n\nprint("GridSearchCV took %.2f seconds for %d candidate parameter settings."\n      % (time() - start, len(grid_search.grid_scores_)))\nreport(grid_search.grid_scores_)</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html
Libsvm GUI	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code>from __future__ import division, print_function\n\nprint(__doc__)\n\n# Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib\nmatplotlib.use('TkAgg')\n\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nfrom matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg\nfrom matplotlib.figure import Figure\nfrom matplotlib.contour import ContourSet\n\nimport Tkinter as Tk\nimport sys\nimport numpy as np\n\nfrom sklearn import svm\nfrom sklearn.datasets import dump_svmlight_file\nfrom sklearn.externals.six.moves import xrange\n\ny_min, y_max = -50, 50\nx_min, x_max = -50, 50\n\n\nclass Model(object):\n    """The Model which hold the data. It implements the\n    observable in the observer pattern and notifies the\n    registered observers on change event.\n    """\n\n    def __init__(self):\n        self.observers = []\n        self.surface = None\n        self.data = []\n        self.cls = None\n        self.surface_type = 0\n\n    def changed(self, event):\n        """Notify the observers. """\n        for observer in self.observers:\n            observer.update(event, self)\n\n    def add_observer(self, observer):\n        """Register an observer. """\n        self.observers.append(observer)\n\n    def set_surface(self, surface):\n        self.surface = surface\n\n    def dump_svmlight_file(self, file):\n        data = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)\n\n\nclass Controller(object):\n    def __init__(self, model):\n        self.model = model\n        self.kernel = Tk.IntVar()\n        self.surface_type = Tk.IntVar()\n        # Whether or not a model has been fitted\n        self.fitted = False\n\n    def fit(self):\n        print("fit the model")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: "linear", 1: "rbf", 2: "poly"}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print("Accuracy:", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed("surface")\n\n    def decision_surface(self, cls):\n        delta = 1\n        x = np.arange(x_min, x_max + delta, delta)\n        y = np.arange(y_min, y_max + delta, delta)\n        X1, X2 = np.meshgrid(x, y)\n        Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z\n\n    def clear_data(self):\n        self.model.data = []\n        self.fitted = False\n        self.model.changed("clear")\n\n    def add_example(self, x, y, label):\n        self.model.data.append((x, y, label))\n        self.model.changed("example_added")\n\n        # update decision surface if already fitted.\n        self.refit()\n\n    def refit(self):\n        """Refit the model if already fitted. """\n        if self.fitted:\n            self.fit()\n\n\nclass View(object):\n    """Test docstring. """\n    def __init__(self, root, controller):\n        f = Figure()\n        ax = f.add_subplot(111)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlim((x_min, x_max))\n        ax.set_ylim((y_min, y_max))\n        canvas = FigureCanvasTkAgg(f, master=root)\n        canvas.show()\n        canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas.mpl_connect('button_press_event', self.onclick)\n        toolbar = NavigationToolbar2TkAgg(canvas, root)\n        toolbar.update()\n        self.controllbar = ControllBar(root, controller)\n        self.f = f\n        self.ax = ax\n        self.canvas = canvas\n        self.controller = controller\n        self.contours = []\n        self.c_labels = None\n        self.plot_kernels()\n\n    def plot_kernels(self):\n        self.ax.text(-50, -60, "Linear: $u^T v$")\n        self.ax.text(-20, -60, "RBF: $\exp (-\gamma \| u-v \|^2)$")\n        self.ax.text(10, -60, "Poly: $(\gamma \, u^T v + r)^d$")\n\n    def onclick(self, event):\n        if event.xdata and event.ydata:\n            if event.button == 1:\n                self.controller.add_example(event.xdata, event.ydata, 1)\n            elif event.button == 3:\n                self.controller.add_example(event.xdata, event.ydata, -1)\n\n    def update_example(self, model, idx):\n        x, y, l = model.data[idx]\n        if l == 1:\n            color = 'w'\n        elif l == -1:\n            color = 'k'\n        self.ax.plot([x], [y], "%so" % color, scalex=0.0, scaley=0.0)\n\n    def update(self, event, model):\n        if event == "examples_loaded":\n            for i in xrange(len(model.data)):\n                self.update_example(model, i)\n\n        if event == "example_added":\n            self.update_example(model, -1)\n\n        if event == "clear":\n            self.ax.clear()\n            self.ax.set_xticks([])\n            self.ax.set_yticks([])\n            self.contours = []\n            self.c_labels = None\n            self.plot_kernels()\n\n        if event == "surface":\n            self.remove_surface()\n            self.plot_support_vectors(model.clf.support_vectors_)\n            self.plot_decision_surface(model.surface, model.surface_type)\n\n        self.canvas.draw()\n\n    def remove_surface(self):\n        """Remove old decision surface."""\n        if len(self.contours) > 0:\n            for contour in self.contours:\n                if isinstance(contour, ContourSet):\n                    for lineset in contour.collections:\n                        lineset.remove()\n                else:\n                    contour.remove()\n            self.contours = []\n\n    def plot_support_vectors(self, support_vectors):\n        """Plot the support vectors by placing circles over the\n        corresponding data points and adds the circle collection\n        to the contours list."""\n        cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],\n                             s=80, edgecolors="k", facecolors="none")\n        self.contours.append(cs)\n\n    def plot_decision_surface(self, surface, type):\n        X1, X2, Z = surface\n        if type == 0:\n            levels = [-1.0, 0.0, 1.0]\n            linestyles = ['dashed', 'solid', 'dashed']\n            colors = 'k'\n            self.contours.append(self.ax.contour(X1, X2, Z, levels,\n                                                 colors=colors,\n                                                 linestyles=linestyles))\n        elif type == 1:\n            self.contours.append(self.ax.contourf(X1, X2, Z, 10,\n                                                  cmap=matplotlib.cm.bone,\n                                                  origin='lower', alpha=0.85))\n            self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors='k',\n                                                 linestyles=['solid']))\n        else:\n            raise ValueError("surface type unknown")\n\n\nclass ControllBar(object):\n    def __init__(self, root, controller):\n        fm = Tk.Frame(root)\n        kernel_group = Tk.Frame(fm)\n        Tk.Radiobutton(kernel_group, text="Linear", variable=controller.kernel,\n                       value=0, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text="RBF", variable=controller.kernel,\n                       value=1, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text="Poly", variable=controller.kernel,\n                       value=2, command=controller.refit).pack(anchor=Tk.W)\n        kernel_group.pack(side=Tk.LEFT)\n\n        valbox = Tk.Frame(fm)\n        controller.complexity = Tk.StringVar()\n        controller.complexity.set("1.0")\n        c = Tk.Frame(valbox)\n        Tk.Label(c, text="C:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(\n            side=Tk.LEFT)\n        c.pack()\n\n        controller.gamma = Tk.StringVar()\n        controller.gamma.set("0.01")\n        g = Tk.Frame(valbox)\n        Tk.Label(g, text="gamma:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)\n        g.pack()\n\n        controller.degree = Tk.StringVar()\n        controller.degree.set("3")\n        d = Tk.Frame(valbox)\n        Tk.Label(d, text="degree:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)\n        d.pack()\n\n        controller.coef0 = Tk.StringVar()\n        controller.coef0.set("0")\n        r = Tk.Frame(valbox)\n        Tk.Label(r, text="coef0:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)\n        r.pack()\n        valbox.pack(side=Tk.LEFT)\n\n        cmap_group = Tk.Frame(fm)\n        Tk.Radiobutton(cmap_group, text="Hyperplanes",\n                       variable=controller.surface_type, value=0,\n                       command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(cmap_group, text="Surface",\n                       variable=controller.surface_type, value=1,\n                       command=controller.refit).pack(anchor=Tk.W)\n\n        cmap_group.pack(side=Tk.LEFT)\n\n        train_button = Tk.Button(fm, text='Fit', width=5,\n                                 command=controller.fit)\n        train_button.pack()\n        fm.pack(side=Tk.LEFT)\n        Tk.Button(fm, text='Clear', width=5,\n                  command=controller.clear_data).pack(side=Tk.LEFT)\n\n\ndef get_parser():\n    from optparse import OptionParser\n    op = OptionParser()\n    op.add_option("--output",\n                  action="store", type="str", dest="output",\n                  help="Path where to dump data.")\n    return op\n\n\ndef main(argv):\n    op = get_parser()\n    opts, args = op.parse_args(argv[1:])\n    root = Tk.Tk()\n    model = Model()\n    controller = Controller(model)\n    root.wm_title("Scikit-learn Libsvm GUI")\n    view = View(root, controller)\n    model.add_observer(view)\n    Tk.mainloop()\n\n    if opts.output:\n        model.dump_svmlight_file(opts.output)\n\nif __name__ == "__main__":\n    main(sys.argv)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/svm_gui.html
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom time import time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\n\nn_samples = 2000\nn_features = 1000\nn_topics = 10\nn_top_words = 20\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print("Topic #%d:" % topic_idx)\n        print(" ".join([feature_names[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()\n\n\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n# to filter out useless terms early on: the posts are stripped of headers,\n# footers and quoted replies, and common English words, words occurring in\n# only one document or in at least 95% of the documents are removed.\n\nprint("Loading dataset...")\nt0 = time()\ndataset = fetch_20newsgroups(shuffle=True, random_state=1,\n                             remove=('headers', 'footers', 'quotes'))\ndata_samples = dataset.data\nprint("done in %0.3fs." % (time() - t0))\n\n# Use tf-idf features for NMF.\nprint("Extracting tf-idf features for NMF...")\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n                                   stop_words='english')\nt0 = time()\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\nprint("done in %0.3fs." % (time() - t0))\n\n# Use tf (raw term count) features for LDA.\nprint("Extracting tf features for LDA...")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n                                stop_words='english')\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint("done in %0.3fs." % (time() - t0))\n\n# Fit the NMF model\nprint("Fitting the NMF model with tf-idf features,"\n      "n_samples=%d and n_features=%d..."\n      % (n_samples, n_features))\nt0 = time()\nnmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\nexit()\nprint("done in %0.3fs." % (time() - t0))\n\nprint("\nTopics in NMF model:")\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\n\nprint("Fitting LDA models with tf features, n_samples=%d and n_features=%d..."\n      % (n_samples, n_features))\nlda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n                                learning_method='online', learning_offset=50.,\n                                random_state=0)\nt0 = time()\nlda.fit(tf)\nprint("done in %0.3fs." % (time() - t0))\n\nprint("\nTopics in LDA model:")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html
Wikipedia principal eigenvector	A							http://scikit-learn.org/stable/auto_examples/index.html			<br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom bz2 import BZ2File\nimport os\nfrom datetime import datetime\nfrom pprint import pprint\nfrom time import time\n\nimport numpy as np\n\nfrom scipy import sparse\n\nfrom sklearn.decomposition import randomized_svd\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.externals.six.moves.urllib.request import urlopen\nfrom sklearn.externals.six import iteritems\n\n\nprint(__doc__)\n\n###############################################################################\n# Where to download the data, if not already on disk\nredirects_url = "http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2"\nredirects_filename = redirects_url.rsplit("/", 1)[1]\n\npage_links_url = "http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2"\npage_links_filename = page_links_url.rsplit("/", 1)[1]\n\nresources = [\n    (redirects_url, redirects_filename),\n    (page_links_url, page_links_filename),\n]\n\nfor url, filename in resources:\n    if not os.path.exists(filename):\n        print("Downloading data from '%s', please wait..." % url)\n        opener = urlopen(url)\n        open(filename, 'wb').write(opener.read())\n        print()\n\n\n###############################################################################\n# Loading the redirect files\n\nmemory = Memory(cachedir=".")\n\n\ndef index(redirects, index_map, k):\n    """Find the index of an article name after redirect resolution"""\n    k = redirects.get(k, k)\n    return index_map.setdefault(k, len(index_map))\n\n\nDBPEDIA_RESOURCE_PREFIX_LEN = len("http://dbpedia.org/resource/")\nSHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\n\n\ndef short_name(nt_uri):\n    """Remove the < and > URI markers and the common URI prefix"""\n    return nt_uri[SHORTNAME_SLICE]\n\n\ndef get_redirects(redirects_filename):\n    """Parse the redirections and build a transitively closed map out of it"""\n    redirects = {}\n    print("Parsing the NT redirect file")\n    for l, line in enumerate(BZ2File(redirects_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print("ignoring malformed line: " + line)\n            continue\n        redirects[short_name(split[0])] = short_name(split[2])\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n    # compute the transitive closure\n    print("Computing the transitive closure of the redirect relation")\n    for l, source in enumerate(redirects.keys()):\n        transitive_target = None\n        target = redirects[source]\n        seen = set([source])\n        while True:\n            transitive_target = target\n            target = redirects.get(target)\n            if target is None or target in seen:\n                break\n            seen.add(target)\n        redirects[source] = transitive_target\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n    return redirects\n\n\n# disabling joblib as the pickling of large dicts seems much too slow\n#@memory.cache\ndef get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n    """Extract the adjacency graph as a scipy sparse matrix\n\n    Redirects are resolved first.\n\n    Returns X, the scipy sparse adjacency matrix, redirects as python\n    dict from article names to article names and index_map a python dict\n    from article names to python int (article indexes).\n    """\n\n    print("Computing the redirect map")\n    redirects = get_redirects(redirects_filename)\n\n    print("Computing the integer index map")\n    index_map = dict()\n    links = list()\n    for l, line in enumerate(BZ2File(page_links_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print("ignoring malformed line: " + line)\n            continue\n        i = index(redirects, index_map, short_name(split[0]))\n        j = index(redirects, index_map, short_name(split[2]))\n        links.append((i, j))\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n        if limit is not None and l >= limit - 1:\n            break\n\n    print("Computing the adjacency matrix")\n    X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\n    for i, j in links:\n        X[i, j] = 1.0\n    del links\n    print("Converting to CSR representation")\n    X = X.tocsr()\n    print("CSR conversion done")\n    return X, redirects, index_map\n\n\n# stop after 5M links to make it possible to work in RAM\nX, redirects, index_map = get_adjacency_matrix(\n    redirects_filename, page_links_filename, limit=5000000)\nnames = dict((i, name) for name, i in iteritems(index_map))\n\nprint("Computing the principal singular vectors using randomized_svd")\nt0 = time()\nU, s, V = randomized_svd(X, 5, n_iter=3)\nprint("done in %0.3fs" % (time() - t0))\n\n# print the names of the wikipedia related strongest compenents of the the\n# principal singular vector which should be similar to the highest eigenvector\nprint("Top wikipedia pages according to principal singular vectors")\npprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\npprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])\n\n\ndef centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\n    """Power iteration computation of the principal eigenvector\n\n    This method is also known as Google PageRank and the implementation\n    is based on the one from the NetworkX project (BSD licensed too)\n    with copyrights by:\n\n      Aric Hagberg <hagberg@lanl.gov>\n      Dan Schult <dschult@colgate.edu>\n      Pieter Swart <swart@lanl.gov>\n    """\n    n = X.shape[0]\n    X = X.copy()\n    incoming_counts = np.asarray(X.sum(axis=1)).ravel()\n\n    print("Normalizing the graph")\n    for i in incoming_counts.nonzero()[0]:\n        X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\n    dangle = np.asarray(np.where(X.sum(axis=1) == 0, 1.0 / n, 0)).ravel()\n\n    scores = np.ones(n, dtype=np.float32) / n  # initial guess\n    for i in range(max_iter):\n        print("power iteration #%d" % i)\n        prev_scores = scores\n        scores = (alpha * (scores * X + np.dot(dangle, prev_scores))\n                  + (1 - alpha) * prev_scores.sum() / n)\n        # check convergence: normalized l_inf norm\n        scores_max = np.abs(scores).max()\n        if scores_max == 0.0:\n            scores_max = 1.0\n        err = np.abs(scores - prev_scores).max() / scores_max\n        print("error: %0.6f" % err)\n        if err < n * tol:\n            return scores\n\n    return scores\n\nprint("Computing principal eigenvector score using a power iteration method")\nt0 = time()\nscores = centrality_scores(X, max_iter=100, tol=1e-10)\nprint("done in %0.3fs" % (time() - t0))\npprint([names[i] for i in np.abs(scores).argsort()[-10:]])</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html
